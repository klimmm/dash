================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2024-10-15T07:39:00.096Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
data_processing.py
data_utils.py
process_dataframe.py
process_market_metrics.py
process_ranks.py
steps.md
table_data.py

================================================================
Repository Files
================================================================

================
File: data_processing.py
================
#data_processing.py

import pandas as pd
import numpy as np
import os
import re
from datetime import datetime, timedelta
from collections import OrderedDict

from typing import List, Tuple, Optional, Dict, OrderedDict, Any, Union, Set
from dataclasses import dataclass
from memory_profiler import profile
import logging

from logging_config import get_logger, custom_profile
logger = get_logger(__name__)


from data_process.data_utils import save_df_to_csv, log_dataframe_info, category_structure, insurer_name_map

from constants.filter_options import MARKET_METRIC_OPTIONS, METRICS, base_metrics, calculated_metrics, calculated_ratios
from constants.mapping import map_insurer


from data_process.process_dataframe import get_processed_dataframe
from data_process.process_ranks import process_for_ranks
from data_process.process_market_metrics import process_market_metrics, add_market_share_rows, add_averages_and_ratios, add_growth_rows_long




# Configure logger
from logging_config import get_logger
logger = get_logger(__name__)


metrics = list(MARKET_METRIC_OPTIONS.keys())

@custom_profile

def get_preprocessed_df(
    df: pd.DataFrame, 
    primary_y_metrics: List[str], 
    secondary_y_metrics: List[str],
    selected_linemains: List[str],
    premium_loss_selection: List[str],
    period_type: str,
    x_column: str,
    series_column: str,
    group_column: str,
    start_quarter: str,
    end_quarter: str,    
    num_periods: int,
    is_groups_stacked: bool,
    is_series_stacked: bool,
    group_by_series: bool,    
    main_insurer: str,
    top_n_list: List[int],
    
    top_n_insurers: Optional[List[str]] = None,
    comparison_insurers: Optional[List[str]] = None,
    benchmark: Optional[List[str]] = None,
    number_of_insurers: Optional[List[int]] = None,    
    reinsurance_form: Optional[List[str]] = None,
    reinsurance_geography: Optional[List[str]] = None,
    reinsurance_type: Optional[List[str]] = None,

) -> pd.DataFrame:


    logger.debug(f"x_column: {x_column}")         
    logger.debug(f"x_column: {x_column}")         
    logger.debug(f"x_column: {x_column}")         
                  
    
    selected_insurers = [main_insurer] + (comparison_insurers or []) + (benchmark or [])

    logger.debug(f"type(df): {type(df)}")    
    save_df_to_csv(df, f"before_process_chart_data.csv")

    chart_columns = [x_column, series_column, group_column]
    selected_metrics = (primary_y_metrics or []) + (secondary_y_metrics or []) 
    extended_metrics = (selected_metrics or []) + ['total_premiums', 'total_losses'] 
    main_metric = extended_metrics[0]

    if 'insurer' in chart_columns and main_insurer != 'total': 

        df = add_total_top_bench_rows(df, main_insurer, top_n_insurers, selected_insurers, end_quarter, top_n_list)

    else:
        logger.debug(f"column insurer not in requested chart_columns, proceeding without adding benchmark insurers.")   
    duplicates = df[df.duplicated()]        
    logger.debug(f"dublicates: {duplicates}")   

    

    
    if 'linemain' in chart_columns:      
        logger.debug(f" column linemain is in requested chart_columns, filtering out all_lines.")                 
        df = df[df['linemain'] != 'all_lines']
    
    else:
        logger.debug(f"column linemain not in requested chart_columns, keeping only all_lines")                 
        df = df[df['linemain'] == 'all_lines']
    
    logger.debug(f"dublicates: {df[df.duplicated()]}")   
    logger.debug(f"Unique lines df after filtering for line column: {df['linemain'].unique().tolist()}")
    logger.debug(f"Unique values df after filtering for line column: {df['value'].unique().tolist()}")
   

    if any(metric.endswith(("market_share", "market_share_q_to_q_change")) for metric in selected_metrics):
        logger.debug(f"There are market_share metrics in selected metrics, calculating market share metrics.")         
        df = add_market_share_rows(df)
    else:        
        logger.debug(f"No market_share metrics in selected metrics, proceeding without calculating market share metrics.")
    logger.debug(f"dublicates: {df[df.duplicated()]}")   

        
    logger.debug(f"Unique metrics df after checking for market share metrics in selected metrics: {df['metric'].unique().tolist()}")
    logger.debug(f"Unique values df after checking for market share metrics in selected metrics: {df['value'].unique().tolist()}")
  
    logger.debug(f"dublicates: {df[df.duplicated()]}")   
    logger.debug(f"Unique insurer df after checking for market share metrics in selected metrics: {df['insurer'].unique().tolist()}")

            
    if set(selected_metrics) - base_metrics:
        logger.debug(f"There are selected metrics not in base metrics.")
        df = add_averages_and_ratios(df, selected_metrics)  
    else:        
        logger.debug(f"All selected metrics are in base metrics")

    logger.debug(f"Unique metrics df after averages_and_ratio: {df['metric'].unique().tolist()}")
    logger.debug(f"Unique value df after averages_and_ratios: {df['value'].unique().tolist()}")

    logger.debug(f"dublicates: {df[df.duplicated()]}")   

        
    if any(metric.endswith(("q_to_q_change")) for metric in selected_metrics):
        logger.debug(f"There are q_to_q_change metrics in selected metrics, calculating q_to_q change metrics.")                
        df = add_growth_rows_long(df, num_periods, period_type, num_periods)            
    else:        
         logger.debug(f"No q_to_q_change metrics in selected metrics, proceeding without calculating q_to_q change metrics.")  
    
    #df = df.sort_values(by='year_quarter', ascending=False)
 
    #df = df[df['year_quarter'].isin(df['year_quarter'].unique()[:num_periods])]

    # Extract years and get N most recent unique years
    unique_years = df['year_quarter'].dt.year.unique()
    years_to_keep = sorted(unique_years, reverse=True)[:num_periods]
    
    # Filter the DataFrame
    df = df[df['year_quarter'].dt.year.isin(years_to_keep)]    


    
    #current_date = datetime.now()
    #cutoff_date = current_date - timedelta(days=365 * num_periods)
    
    #df = df[df['year_quarter'] >= cutoff_date]

    
    if 'insurer' in chart_columns:      
        if selected_insurers:             
            logger.debug(f" column insurer is in requested chart_columns, filtering for selected_insurers.")                 
            df = df.loc[df['insurer'].isin(selected_insurers)]
        else:
            df = df[df['insurer'] == main_insurer if main_insurer in df['insurer'].unique() else 'total']
    else:
        logger.debug(f"column insurer not in requested chart_columns, proceed with filtering for main insurer.")                 
        logger.info(f"Unique insurer df after add aver: {df['insurer'].unique().tolist()}")
      
        if 'linemain' in chart_columns:
            df = df[df['insurer'] == main_insurer] if main_insurer in df['insurer'].unique() else df[df['insurer'] == 'total']
            
    
    duplicates = df[df.duplicated()]        
    logger.debug(f"dublicates: {duplicates}")   


    logger.debug(f"Unique value df after qchecking for linamain: {df['value'].unique().tolist()}")
    duplicates = df[df.duplicated()]        
    logger.debug(f"dublicates: {duplicates}")   

    
    if 'metric' in chart_columns:      
        logger.debug(f" column metric is in requested chart_columns, filtering for selected_metrics.")                 
        df = df.loc[df['metric'].isin(selected_metrics)]
    else:
        logger.debug(f" no main metric in unique insurer values, filtering for main_metric")   
        if main_metric in df['metric'].unique():
            logger.debug(f" no main metric in unique metric values, filtering for 'total'")   
            
            df = df[df['metric'] == main_metric]
        else:
            logger.warning(f" no main metric in unique insurer values, filtering for 'total_premiums")   
            
            df = df[df['metric'] == 'ceded_premiums']                
     
    logger.debug(f"Unique value df after qchecking for metric: {df['value'].unique().tolist()}")
    duplicates = df[df.duplicated()]        
    logger.debug(f"dublicates: {duplicates}")   

      
    if 'year_quarter' or 'year' or 'quarter' in chart_columns:      
        logger.debug(f" year_quarter is in requested chart_columns, no additional filtering requireds.")             
     
    else:
        logger.warning(f" year_quarter not in in requested chart_columns, keeping the most recent date only")   
        end_quarter = df['year_quarter'].max()
        df = df[df['year_quarter'] == end_quarter]      
    duplicates = df[df.duplicated()]        
    logger.debug(f"dublicates: {duplicates}")   

        
    logger.debug(f"Unique value df after qchecking for metric: {df['value'].unique().tolist()}")

    df = aggregate_based_on_chart_cols(df, x_column, series_column, group_column  
    )
    logger.debug(f"columns after percent calc by chart columns: {df.columns}")         

    logger.debug(f"Unique value df after aggregate_based_on_chart_cols: {df['value'].unique().tolist()}")
    
    chart_data = calculate_percent_column(
        df, x_column, series_column, group_column, is_groups_stacked, is_series_stacked, group_by_series
    )
    
    logger.debug(f"Unique value df after chart data: {df['value'].unique().tolist()}")
   
    logger.debug(f"columns after percent calc by chart columns: {chart_data.columns}")         
    save_df_to_csv(chart_data, f"chart_data_final.csv")

    
    return chart_data 




#@custom_profile

def aggregate_based_on_chart_cols(
    df: pd.DataFrame, 
    x_column: str, 
    series_column: str, 
    group_column: str,     
) -> pd.DataFrame:
    logger.debug(f"x_column: {x_column}")         
    logger.debug(f"series_column: {series_column}")        
    logger.debug(f"group_column: {group_column}")         
    logger.debug(f"columns before aggregating by chart columns: {df.columns}")       
    logger.debug(f"Unique values in year_quarter: {df['year_quarter'].unique().tolist()}")
    save_df_to_csv(df, f"before aggregatel.csv")
  
    df['quarter'] = df['year_quarter'].dt.quarter
    df['year'] = df['year_quarter'].dt.year
    logger.debug(f"unique quarter values before aggregating by chart columns: {df['quarter'].unique()}")
    logger.debug(f"unique year values before aggregating by chart columns: {df['year'].unique()}")

    logger.debug(f"columns before agg by chart columns: {df.columns}")         
    logger.debug(f"Unique values in in x_column: {df[x_column].unique().tolist()}")
    logger.debug(f"Unique values in in series_column: {df[series_column].unique().tolist()}")
    logger.debug(f"Unique values in in group_column: {df[group_column].unique().tolist()}")
    
    # Get all columns from the DataFrame

    group_columns = [col for col in df.columns if col in [x_column, series_column, group_column]]
    logger.debug(f"group_columns: {group_columns}")         
    
    df = df.groupby(group_columns)['value'].sum().reset_index()
   
    logger.debug(f"columns after_agg by chart columns: {df.columns}")         
    logger.debug(f"Unique values in in x_column after_agg: {df[x_column].unique().tolist()}")
    logger.debug(f"Unique values in in series_column after_agg: {df[series_column].unique().tolist()}")
    logger.debug(f"Unique values in in group_column after_agg: {df[group_column].unique().tolist()}") 
    #logger.debug(f"unique quarter values after aggregating by chart columns: {df['quarter'].unique()}")
    #logger.debug(f"unique year values after aggregating by chart columns: {df['year'].unique()}")

    save_df_to_csv(df, f"after aggregatel.csv")
    
    logger.debug(f"columns after aggregating by chart columns: {df.columns}")  
    return df

#@custom_profile

def calculate_percent_column(
    df: pd.DataFrame, 
    x_column: str, 
    series_column: str, 
    group_column: str,     
    is_groups_stacked: bool = False, 
    is_series_stacked: bool = False,
    group_by_series: bool = False    
) -> pd.DataFrame:

    logger.debug(f"x_column: {x_column}")         
    logger.debug(f"series_column: {series_column}")         
    logger.debug(f"group_column: {group_column}")         
    logger.debug(f"is_groups_stacked: {is_groups_stacked}")         
    logger.debug(f"is_series_stacked: {is_series_stacked}")         
    logger.debug(f"group_by_series: {group_by_series}")     

    logger.debug(f"columns before percent calc by chart columns: {df.columns}")         

    save_df_to_csv(df, f"before calculate_percent_column chart_data.csv")
    logger.debug(f"columns before calculating percent: {df.columns}")       

    if is_series_stacked and not is_groups_stacked:
        groupby_columns = [x_column, series_column]
    elif not is_series_stacked and is_groups_stacked:
        groupby_columns = [x_column, group_column]
    elif is_groups_stacked and is_series_stacked:
        groupby_columns = [x_column]
    
    else:  # not is_groups_stacked and not is_series_stacked
        groupby_columns = [x_column, group_column, series_column]
    
    df = df.copy()
    logger.debug(f"columns after percent calc by chart columns: {df.columns}")         

    # Calculate the total value for each group
    df['total_value'] = df.groupby(groupby_columns)['value'].transform('sum')
    logger.debug(f"columns after percent calc by chart columns: {df.columns}")         

    # Calculate the percent column
    df['percent'] = df['value'] / df['total_value']
    logger.debug(f"columns after percent calc by chart columns: {df.columns}")         

    # Drop the temporary total_value column
    df = df.drop(columns=['total_value'])
    logger.debug(f"columns after percent calc by chart columns: {df.columns}")         


    
    df = df.sort_values(['value'], ascending=[False]).reset_index(drop=True)

    logger.debug(f"columns after percent calc by chart columns: {df.columns}")         


    
    return df
 

#@custom_profile

def add_total_top_bench_rows(
    df: pd.DataFrame,
    main_insurer: str,
    top_n_insurers: List[str],
    selected_insurers: List[str],
    end_quarter: str,
    top_n_list: Optional[List[int]] = None
) -> pd.DataFrame:
    """
    Adds aggregate rows for 'Total', top-N largest 'value' entries within each group, and top-N benchmarks.

    Parameters:
    - df (pd.DataFrame): The original DataFrame containing data.
    - main_insurer (str): The main insurer to exclude during top-N calculation.
    - top_n_insurers (List[str]): A list of insurers for the final filtering.
    - selected_insurers (List[str]): Additional insurers for final filtering.
    - end_quarter (str): The specific quarter to filter when calculating top-N benchmarks.
    - top_n_list (List[int], optional): A list of integers specifying the top-N aggregates to add.

    Returns:
    - pd.DataFrame: The DataFrame augmented with 'Total', 'Top N', and 'Top N Benchmark' aggregate rows.
    """
    
    logger.info(f"top_n_insurers starting add_total_top_bench_rows'{top_n_insurers}")
    logger.info(f"main_insurer'{main_insurer}")

    if top_n_list is None:
        top_n_list = [5, 10, 20]

    # Create a copy to avoid modifying the original DataFrame
    df = df.copy()

    # Convert 'year_quarter' to datetime if it's not already
    if not pd.api.types.is_datetime64_any_dtype(df['year_quarter']):
        df['year_quarter'] = pd.to_datetime(df['year_quarter'], errors='coerce')
        if df['year_quarter'].isnull().any():
            logger.debug("Some 'year_quarter' entries could not be converted to datetime.")

    # Convert 'insurer' to string to maintain consistency
    df['insurer'] = df['insurer'].astype(str)

    # Convert end_quarter to datetime
    try:
        end_quarter_dt = pd.to_datetime(end_quarter)
    except Exception as e:
        logger.error(f"Error converting end_quarter '{end_quarter}' to datetime: {e}")
        raise

    # Identify identifier columns (all except 'insurer' and 'value')
    id_columns = [col for col in df.columns if col not in ['insurer', 'value']]

    # Calculate total rows
    df_total = df[df['insurer'] == 'total']
    logger.debug(f"df_total head'{df_total.head()}")
    df_others = df[df['insurer'] == 'others']

    top_rows = [f"top-{n}" for n in top_n_list]
    
    df = df[df['insurer'] != 'total']
    df = df[df['insurer'] != 'others']
    df = df[~df['insurer'].isin(top_rows)]
    
    
    
    top_rows_df = df[df['insurer'].isin(top_rows)]
  

    logger.debug(f"df_not total head'{df.head()}")

    # Store DataFrames for each Top-N and Top-N Benchmark
    df_top_n_list = []
    
    for n in top_n_list:
        # Sort by identifier columns and 'value' to get largest values first
        df_sorted = df.sort_values(by=id_columns + ['value'], ascending=False)

        # Select top N 'value' entries per group
        df_top_n = df_sorted.groupby(id_columns).head(n).copy()
        df_top_n_grouped = df_top_n.groupby(id_columns)['value'].sum().reset_index()
        df_top_n_grouped['insurer'] = f'top-{n}'
        df_top_n_list.append(df_top_n_grouped)
        logger.debug(f"Added top-{n} aggregate rows.")

        # Identify top-N insurers for 'direct_premiums' in the specified end_quarter
        df_benchmark = df[
            (df['insurer'] != main_insurer) &
            (df['year_quarter'] == end_quarter_dt) &
            (df['metric'] == 'direct_premiums')
        ]

        if df_benchmark.empty:
            logger.debug(f"No benchmark data found for end_quarter '{end_quarter}' and metric 'direct_premiums'. Skipping top-{n}-benchmark.")
            continue

        # Select top N insurers by value
        top_n_benchmark_insurers = df_benchmark.groupby('insurer')['value'].sum().nlargest(n).index.tolist()
        logger.debug(f"Top-{n} benchmark insurers: {top_n_benchmark_insurers}")

        # Filter the original DataFrame for these top-N benchmark insurers
        df_benchmark_top_n = df[df['insurer'].isin(top_n_benchmark_insurers)]

        if df_benchmark_top_n.empty:
            logger.debug(f"No data found for top-{n} benchmark insurers.")
            continue

        # Aggregate their values across all dimensions
        df_benchmark_aggregated = df_benchmark_top_n.groupby(id_columns)['value'].sum().reset_index()
        df_benchmark_aggregated['insurer'] = f'top-{n}-benchmark'
        df_top_n_list.append(df_benchmark_aggregated)
        logger.debug(f"Added top-{n}-benchmark aggregate rows.")

    # Concatenate original DataFrame with totals and aggregates
    result_df = pd.concat([df, df_total, df_others, top_rows_df] + df_top_n_list, ignore_index=True)
    logger.debug(f"result_df head'{result_df.head()}")
    
    
    logger.debug("Concatenated original DataFrame with total and aggregate rows.")

    # Perform sorting
    result_df = result_df.sort_values(by=id_columns + ['value'], ascending=False)
    logger.debug("Sorted sthe final DataFrame.")
    logger.debug(f"result_df unique insurers: {result_df['insurer'].unique()}")
    logger.debug(f"result_df selected_insurers: {selected_insurers}")
    logger.info(f"result_df main_insurer: {main_insurer}")
    logger.info(f"top_n_insurers: {top_n_insurers}")

    # Final filtering: combine top_n_insurers, main_insurer, and selected_insurers
    final_insurers_list = list(set(
        top_n_insurers + [main_insurer] + top_rows + selected_insurers +
        ['total'] + ['others'] +
        [f'top-{n}' for n in top_n_list] +
        [f'top-{n}-benchmark' for n in top_n_list]
    ))

    # Ensure that 'total', 'top-{n}', 'top-{n}-benchmark', 'main_insurer', and selected_insurers are in the final list
    result_df = result_df[result_df['insurer'].isin(final_insurers_list)]
    logger.debug("Filtered the final DataFrame based on the specified insurers.")

   
    return result_df

================
File: data_utils.py
================
# data_utils.py


import pandas as pd
from typing import List, Dict, Any, Tuple
import json
import re
import os
from logging_config import get_logger
from constants.translations import translate 
from constants.filter_options import METRICS, LINEMAIN_COL, INSURER_COL
from functools import lru_cache

logger = get_logger(__name__)

def load_json(file_path: str) -> Dict:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.debug(f"Successfully loaded {file_path}")
        return data
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        raise
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in file: {file_path}")
        raise

def save_df_to_csv(df, filename, max_rows=500):
    """
    Save a DataFrame to a CSV file with a maximum number of rows.
    
    Args:
        df (pd.DataFrame): The DataFrame to save
        filename (str): The name of the file to save to
        max_rows (int): The maximum number of rows to save (default: 500)
    """
    output_dir = 'intermediate_outputs'
    os.makedirs(output_dir, exist_ok=True)
    
    df_to_save = df.head(max_rows)
    full_path = os.path.join(output_dir, filename)
    df_to_save.to_csv(full_path, index=False)
    logger.debug(f"Saved {len(df_to_save)} rows to {full_path}")

def print_dataframe_info(df: pd.DataFrame, stage: str = ""):
    logger.debug(f"DataFrame Info - {stage}")
    logger.debug("Column Names:")
    for col in df.columns:
        logger.debug(f"- {col}")
    logger.debug("=" * 50)
    
    logger.debug("Data Types:")
    logger.debug(df.dtypes)
    logger.debug("=" * 50)
    
    for col in df.columns:
        unique_values = df[col].unique()
        logger.debug(f"{col}: {len(unique_values)} unique values")
        logger.debug(f"  Values: {unique_values}")
    logger.debug("=" * 50)

def log_dataframe_info(df: pd.DataFrame, step_name: str):
    logger.debug(f"--- {step_name} ---")
    logger.debug(f"DataFrame shape: {df.shape}")
    logger.debug(f"Columns: {df.columns.tolist()}")
    for column in df.columns:
        unique_values = df[column].nunique()
        logger.debug(f"Unique values in '{column}': {unique_values}")
        if unique_values < 10:
            logger.debug(f"Unique values: {df[column].unique().tolist()}")
    logger.debug(f"First 5 rows:\n{df.head().to_string()}")
    logger.debug("-------------------")


@lru_cache(maxsize=None)
def load_and_preprocess_data(file_path: str) -> pd.DataFrame:
    logger.debug(f"Starting to load and preprocess data from {file_path}")

    dtype_dict = {
    'datatypec': 'object',
    'linemain': 'object',
    'insurer': 'object',
    'value': 'float64'
    }
    
    try:
        df = pd.read_csv(file_path, dtype=dtype_dict, parse_dates=['year_quarter'])
        logger.debug(f"Successfully loaded {file_path}")
        return df
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        raise

    
    """Validate the dataframe has all required columns."""
    required_columns = [INSURER_COL, LINEMAIN_COL]  # Убедитесь, что эти константы импортированы
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        # Вместо вызова исключения, выводим предупреждение
        logger.warning(f"Missing columns: {', '.join(missing_columns)}")
        
        # Если отсутствует столбец LINEMAIN_COL, создаем его с временными данными
        if LINEMAIN_COL in missing_columns:
            df[LINEMAIN_COL] = 'Unknown'
            logger.warning(f"Created '{LINEMAIN_COL}' column with 'Unknown' values")
    
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'])
    
    df = df.rename(columns={'insurance_line': 'linemain', 'datatype': 'metric'})
    
    df['value'] = df['value'] / 1_000_000

    df['metric'] = df['metric'].fillna(0)
    df = df.sort_values('year_quarter', ascending=True)
    
    return df







insurer_data = load_json('./constants/insurer_map.json')
default_insurer_options = [{'label': item['name'], 'value': item['insurer']} for item in insurer_data]
insurer_name_map = {item['insurer']: item['name'] for item in insurer_data}



def create_additional_table_metrics(primary_y_metrics: List[str], secondary_y_metrics: List[str]):
    
    
    selected_metrics = (primary_y_metrics or[]) + (secondary_y_metrics or[])
    selected_table_metric = selected_metrics[0]        
    
    additional_table_metric_options = [
        {
            'label': translate(METRICS[metric]['label']),
            'value': metric
        }
        for metric in METRICS
        if metric != selected_table_metric
    ]
    logger.debug(f"Number of additional metrics: {len(additional_table_metric_options)}")
    logger.debug(f"Additional metrics: {additional_table_metric_options}")
    
    return additional_table_metric_options    

def create_year_quarter_options(df: pd.DataFrame) -> List[Dict[str, str]]:
    """
    Create year-quarter options for dropdown from a DataFrame.

    Args:
        df (pd.DataFrame): DataFrame containing a 'year_quarter' column.

    Returns:
        List[Dict[str, str]]: A list of dictionaries with 'label' and 'value' for each unique year-quarter.
    """
    
    year_quarter_options = [
        {'label': quarter.strftime('%YQ%q'), 'value': quarter.strftime('%YQ%q')}
        for quarter in df['year_quarter'].dt.to_period('Q').unique()
    ]
    
    translated_options = [
        {'label': translate(option['value']), 'value': option['value']}
        for option in year_quarter_options
    ]
    
    return year_quarter_options  # Add this line to return the list


category_structure = load_json('./constants/modified_insurance.json')

def map_insurer_code_to_name(df: pd.DataFrame) -> pd.DataFrame:
    insurer_map_dict = load_json('./constants/insurer_map.json')
    insurer_map = {item['insurer']: item['name'] for item in insurer_map_dict}
    df['insurer'] = df['insurer'].map(insurer_map).fillna(df['insurer'])
    return df

def map_line_code_to_name(df: pd.DataFrame) -> pd.DataFrame:
    line_map_dict = load_json('./constants/line_map.json')
    line_map = {item['code']: item['name'] for item in line_map_dict}   
    df['linemain'] = df['linemain'].map(line_map).fillna(df['linemain'])
    return df

def map_metric_code_to_name(df: pd.DataFrame) -> pd.DataFrame:
    datatype_map = load_json('./constants/datatype_map.json')

    df['metric'] = df['metric'].map(datatype_map).fillna(df['metric'])
    return df

def map_insurer(insurer_code: str) -> str:
    # Load the insurer mapping from the JSON file
    with open('./constants/insurer_map.json') as f:
        INSURER_MAPPING = {item['insurer']: item['name'] for item in json.load(f)}
    
    # Add manual values to the INSURER_MAPPING dictionary
    INSURER_MAPPING['Total Market'] = 'Всего по рынку'
    INSURER_MAPPING['Top 5 Concentration'] = 'Топ 5'
    INSURER_MAPPING['Top 10 Concentration'] = 'Топ 10'
    
    return INSURER_MAPPING.get(insurer_code, insurer_code)

def map_line(line_code):
    
    with open('./constants/modified_insurance.json', 'r', encoding='utf-8') as f:
        insurance_data = json.load(f)    
    
    LINE_MAPPING = {category: data['label'] for category, data in insurance_data.items()}

    if isinstance(line_code, list):
        return [LINE_MAPPING.get(code, code) for code in line_code]

    
    return LINE_MAPPING.get(line_code, line_code)





__all__ = ['load_csv', 'print_dataframe_info', 'create_insurer_options', 'create_additional_table_metrics', 'create_year_quarter_options', 'load_and_preprocess_data', 'load_json', 'map_insurer', 'map_line', 'map_insurer_code_to_name', 'map_line_code_to_name', 'map_metric_code_to_name', 'category_structure', 'default_insurer_options']

================
File: process_dataframe.py
================
import pandas as pd
import numpy as np
import os
import re
from datetime import datetime
from collections import OrderedDict

from typing import List, Tuple, Optional, Dict, OrderedDict, Any, Union, Set
from dataclasses import dataclass
from memory_profiler import profile
import logging


from data_process.data_utils import save_df_to_csv, log_dataframe_info, category_structure, insurer_name_map

from constants.filter_options import METRICS, base_metrics, calculated_metrics, calculated_ratios


from logging_config import setup_logging, set_debug_level, DebugLevels, get_logger, debug_log, intensive_debug_log, custom_profile

logger = get_logger(__name__)



@custom_profile
def get_processed_dataframe(
    df: pd.DataFrame, 
    premium_loss_selection: List[str],
    primary_y_metrics: List[str], 
    secondary_y_metrics: List[str],
    selected_linemains: List[str],
    period_type: str,   
    start_quarter: str,
    end_quarter: str,
    num_periods: int,
    quarter_value: int,
    reinsurance_form: Optional[Any] = None,
    reinsurance_geography: Optional[Any] = None,
    reinsurance_type: Optional[Any] = None  
) -> pd.DataFrame:
    
    logger.info("Started get_processed_dataframe function")
    #df = optimize_dtypes(df)
    
    logger.debug(f"Unique insurers df before process_dataframe: {df['insurer'].unique().tolist()}")
    logger.debug(f"uniqu values in year quarter before process_dataframe{df['year_quarter'].unique().tolist()}")
    selected_metrics = (primary_y_metrics or []) + (secondary_y_metrics or []) 
    extended_metrics = (selected_metrics or []) + ['total_premiums', 'total_losses'] 
    df = (
        df
        .pipe(
            filter_by_date_range_and_period_type, 
            period_type=period_type, 
            start_quarter=start_quarter, 
            end_quarter=end_quarter, 
            quarter_value=quarter_value, 
            num_periods=num_periods
        )
        .pipe(
            filter_by_life_nonlife_and_direct_inward, 
            premium_loss_selection=premium_loss_selection
        )
        .pipe(
            filter_reinsurance, 
            reinsurance_form=reinsurance_form, 
            reinsurance_geography=reinsurance_geography, 
            reinsurance_type=reinsurance_type
        )
        .pipe(
            filter_required_metrics, 
            selected_metrics=extended_metrics
        )
        .pipe(
            filter_by_insurance_lines, 
            selected_linemains=selected_linemains
        )
        .pipe(
            add_calculated_metrics, 
            selected_metrics=selected_metrics
        )
        .pipe(
            add_total_rows
        )
    )



    
    #df = filter_by_date_range_and_period_type(
    #        df, period_type, start_quarter, end_quarter, quarter_value, num_periods)       
    #df = filter_by_life_nonlife_and_direct_inward(df, premium_loss_selection)        
    #df = filter_reinsurance(df, reinsurance_form, reinsurance_geography, reinsurance_type)  
    #df = filter_required_metrics(df, extended_metrics) 
    #df = filter_by_insurance_lines(df, selected_linemains)
    #df = add_calculated_metrics(df, selected_metrics)
    #df = add_total_rows(df)
    
    
    save_df_to_csv(df, f"process_dataframe.csv")
    logger.debug(f"Unique insurers df after process_dataframe: {df['insurer'].unique().tolist()}")
    logger.info("Finished get_processed_dataframe function")
    
    return df    



#@custom_profile
def filter_by_date_range_and_period_type(
    df: pd.DataFrame,
    period_type: str,
    start_quarter: str,
    end_quarter: str,    
    quarter_value: int,
    num_periods: int,
) -> pd.DataFrame:

    logger.info("Started filter_by_date_range_and_period_type function")

    logger.debug(f"period_type: {period_type}")
    logger.debug(f"start_quarter: {start_quarter}")
    logger.debug(f"end_quarter: {end_quarter}")
    logger.debug(f"num_periods: {num_periods}")
    logger.info(f"df index: {df.index}")
    logger.info(f"df dtypes: {df.dtypes}")

    logger.debug(f"columns before: filter_by_date_range_and_period_type {df.head()}")
    if not pd.api.types.is_datetime64_any_dtype(df['year_quarter']):
        df['year_quarter'] = pd.to_datetime(df['year_quarter'], errors='coerce')
        if df['year_quarter'].isnull().any():
            logger.debug("Some 'year_quarter' entries could not be converted to datetime.")   

    
    
    
    
    end_quarter_date = pd.to_datetime(end_quarter)
    df = df[df['year_quarter'] <= end_quarter_date]
    logger.debug(f"uniqu values in year quarter before filter{df['year_quarter'].unique().tolist()}")
    logger.debug(f"end_quarter_datee {end_quarter_date}")
    
    unique_years = df['year_quarter'].dt.year.unique()
    years_to_keep = sorted(unique_years, reverse=True)[:num_periods]
    
    # Filter the DataFrame
    df = df[df['year_quarter'].dt.year.isin(years_to_keep)]       
    

    intensive_debug_log(logger,f"columns after: filter_by_end_quarter_date {df.head()}")
    
    start_quarter_date = pd.to_datetime(start_quarter)
    if period_type == 'cumulative_sum':
        df = df[df['year_quarter'] >= start_quarter_date]
        
    intensive_debug_log(logger,f"columns after: filter_by_start_quarter_date {df.head()}")

    grouping_cols  = [col for col in df.columns if col not in ['year_quarter', 'value']]
    df = df.sort_values(grouping_cols + ['year_quarter'], ascending=True)
    end_quarter_num = end_quarter_date.quarter


    if period_type == 'previous_quarter':  
        pass    
    
    elif period_type == 'same_q_last_year':

        df = df[df['year_quarter'].dt.quarter == end_quarter_num]
    
    elif period_type in ['same_q_last_year_ytd', 'previous_q_mat', 'same_q_last_year_mat', 'cumulative_sum']:
        
        #df = df.sort_values(grouping_cols + ['year_quarter'], ascending=True)
        
        if period_type == 'same_q_last_year_ytd':
            df = df[df['year_quarter'].dt.quarter <= quarter_value]
            df['year'] = df['year_quarter'].dt.year
            df['ytd_value'] = df.groupby(['year'] + grouping_cols)['value'].cumsum()
            df = df.reset_index(drop=True)
            df['value'] = df['ytd_value']
            df = df.drop(columns=['year', 'ytd_value'])
            df = df[df['year_quarter'].dt.quarter == quarter_value]

        elif period_type in ['previous_q_mat', 'same_q_last_year_mat']:
            df.set_index('year_quarter', inplace=True)
            df['last_4_quarters_sum'] = df.groupby(grouping_cols)['value'].rolling(window='365D', min_periods=1).sum().reset_index(level=grouping_cols, drop=True)
            df.reset_index(inplace=True)
            df['value'] = df['last_4_quarters_sum']
            df = df.drop(columns=['last_4_quarters_sum'])
            if period_type == 'same_q_last_year_mat':
                df = df[df['year_quarter'].dt.quarter == end_quarter_num]        

        elif period_type == 'cumulative_sum':
            df['cumsum'] = df.groupby(grouping_cols)['value'].cumsum()
            df['value'] = df['cumsum']
            df = df.drop(columns=['cumsum'])



    #df = df.sort_values(by='year_quarter', ascending=False)

    '''if num_periods is not None:
        if period_type == 'previous_quarter':  
            N = num_periods + 4 # will needed more quarters for y_on_y grothw calc
        else:
            N = num_periods + 1
            
        result = df[df['year_quarter'].isin(df['year_quarter'].unique()[:N])]

    else:        
        if period_type == 'previous_quarter':  
            start_quarter_date = pd.to_datetime(start_quarter) - pd.DateOffset(years=1)
        else:
            start_quarter_date = pd.to_datetime(start_quarter)
            
        
        result = df[df['year_quarter'] >= start_quarter_date]'''

    logger.info("Finished filter_by_date_range_and_period_type function")

    logger.debug(f"Unique quarters in df after filter_by_period_type: {df['year_quarter'].unique().tolist()}") 
    
    return df


#@custom_profile
def filter_by_life_nonlife_and_direct_inward(
    df: pd.DataFrame, 
    premium_loss_selection: Optional[List[str]] = None,
    line_types: Optional[List[str]] = None

) -> pd.DataFrame:
    logger.info("Starting filter_by_life_nonlife_and_direct_inward function")

    intensive_debug_log(logger,f"columns before: filter_by_life_nonlife_and_direct_inward {df.head()}")

    logger.debug(f"premium_loss_selection: {premium_loss_selection}")

    mask = pd.Series(True, index=df.index)
       
    # Filter by line_type if specified
    mask = pd.Series(True, index=df.index)
    if line_types and 'line_type' in df.columns:
        mask &= df['line_type'].isin(line_types)
    if premium_loss_selection and 'metric' in df.columns:
        exclude_metrics = []
        if 'direct' not in premium_loss_selection:
            exclude_metrics.extend(['direct_premiums', 'direct_losses'])
        if 'inward' not in premium_loss_selection:
            exclude_metrics.extend(['inward_premiums', 'inward_losses'])
        if exclude_metrics:
            mask &= ~df['metric'].isin(exclude_metrics)
    df = df.loc[mask].drop(columns='line_type', errors='ignore')
    
    
    logger.info("Finished filter_by_life_nonlife_and_direct_inward function")

    intensive_debug_log(logger,f"columns after: {df.head()}")
    
    return df

#@custom_profile
def filter_reinsurance(
    df: pd.DataFrame, 
    reinsurance_form: Optional[Any] = None,
    reinsurance_geography: Optional[Any] = None,
    reinsurance_type: Optional[Any] = None
) -> pd.DataFrame:
    """
    Filter the DataFrame based on reinsurance criteria.

    Args:
        df (pd.DataFrame): Input DataFrame to filter.
        reinsurance_form (Optional[Any]): Allowed reinsurance forms.
        reinsurance_geography (Optional[Any]): Allowed reinsurance geographies.
        reinsurance_type (Optional[Any]): Allowed reinsurance types.

    Returns:
        pd.DataFrame: Filtered DataFrame or original DataFrame if all parameters are None.
    """
    logger.info("Starting filter_reinsurance function")

    logger.debug(f"Input DataFrame shape: {df.shape}")
    logger.debug(f"reinsurance_form: {reinsurance_form}")
    logger.debug(f"reinsurance_geography: {reinsurance_geography}")
    logger.debug(f"reinsurance_type: {reinsurance_type}")
    intensive_debug_log(logger,f"columns before: filter_reinsurance {df.head()}")

    filter_criteria = []
    for col, values in [
        ('reinsurance_form', reinsurance_form),
        ('reinsurance_geography', reinsurance_geography),
        ('reinsurance_type', reinsurance_type)
    ]:
        if values is not None:
            if not isinstance(values, (list, tuple, pd.Series)):
                logger.warning(f"Invalid type for {col}: {type(values)}. Expected list-like object.")
                continue
            if len(values) == 0:
                logger.warning(f"Empty list provided for {col}. Skipping this filter.")
                continue
            if col not in df.columns:
                logger.warning(f"Column '{col}' not found in DataFrame. Skipping this filter.")
                continue
            filter_criteria.append((col, values))
        else:
            logger.debug(f"Skipping filter for '{col}'. Values is None.")

    if filter_criteria:
        for col, values in filter_criteria:
            logger.debug(f"Applying filter for '{col}' with values: {values}")
            df = df[df[col].isin(values)]
            logger.debug(f"DataFrame shape after filtering {col}: {df.shape}")
    else:
        logger.info("No valid filtering criteria. Returning original DataFrame.")
    
    logger.info("Finished filter_reinsurance function")
    save_df_to_csv(df, f"filter_reinsurance.csv")

    logger.debug(f" DataFrame shape after filter_reinsurance: {df.shape}")
    return df


#@custom_profile
def filter_required_metrics(df: pd.DataFrame, selected_metrics: List[str]) -> pd.DataFrame:
    logger = logging.getLogger(__name__)
    logger.info("Starting filter_required_metrics function")

    
    logger.info(f"Function filter_required_metrics called with selected metrics: {selected_metrics}")
    logger.info(f"uniqu values in metrics before filter{df['metric'].unique().tolist()}")
    logger.info(f"original columnsr{df.columns.tolist()}")
    
    original_columns = df.columns.tolist()

   
    relevant_metrics = set()
    for selected_metric in selected_metrics:
        logger.info(f"Processing selected metric: {selected_metric}")
        
        # Check for exact matches first
        if selected_metric in base_metrics:
            relevant_metrics.add(selected_metric)
            logger.info(f"Exact match found in base_metrics: {selected_metric}")
        elif selected_metric in calculated_metrics:
            relevant_metrics.update(calculated_metrics[selected_metric])
            logger.info(f"Exact match found in calculated_metrics: {selected_metric}. Adding base metrics: {calculated_metrics[selected_metric]}")
        elif selected_metric in calculated_ratios:
            relevant_metrics.update(calculated_ratios[selected_metric])
            logger.info(f"Exact match found in calculated_ratios: {selected_metric}. Adding base metrics: {calculated_ratios[selected_metric]}")
        
        # Check for substring matches
        for metric in base_metrics:
            if metric in selected_metric:
                relevant_metrics.add(metric)
                logger.info(f"Substring match found in base_metrics: {metric} for selected metric: {selected_metric}")
        
        for calc_metric, base_deps in calculated_metrics.items():
            if calc_metric in selected_metric:
                relevant_metrics.update(base_deps)
                logger.info(f"Substring match found in calculated_metrics: {calc_metric} for selected metric: {selected_metric}. Adding base metrics: {base_deps}")
        
        for ratio_metric, base_deps in calculated_ratios.items():
            if ratio_metric in selected_metric:
                relevant_metrics.update(base_deps)
                logger.info(f"Substring match found in calculated_ratios: {ratio_metric} for selected metric: {selected_metric}. Adding base metrics: {base_deps}")
    
    

    
    logger.info(f"Relevant metrics after processing: {relevant_metrics}")
    logger.info(f"uniqu values in metrics{df['metric'].unique().tolist()}")
    
    # Filter the DataFrame without copying if possible
    df = df.loc[df['metric'].isin(relevant_metrics)]
    df = df.reindex(columns=original_columns)
    logger.info("Finished filter_required_metrics function")
    save_df_to_csv(df, f"filter_required_metrics.csv")
    
    #logger.debug(f"Filtered DataFrame shape: {df.shape}")
    return df


#@custom_profile
def filter_by_insurance_lines(
    df: pd.DataFrame,
    selected_linemains: List[str]
) -> pd.DataFrame:
    """
    Filters and aggregates insurance data based on selected insurance lines.
    
    Args:
        df (pd.DataFrame): The input DataFrame containing insurance data.
        selected_linemains (List[str]): List of lines to filter.
    
    Returns:
        pd.DataFrame: The processed DataFrame.
    """
    logger.info("Starting filter_by_insurance_lines function")
    logger.debug(f"Input DataFrame shape: {df.shape}")

    logger.debug(f"selected_linemains: {selected_linemains}")
       
    # Define special categories and their subcategories
    parent_lines = {
        'страхование жизни': ['1', '2'],
        'имущество юр.лиц': ['4.1.7', '4.3'],
        'ж/д': ['4.1.2', '4.2.2'],
        'море, грузы': ['4.1.4', '4.1.5', '4.2.4'],
        'авиа': ['4.1.3', '4.2.3'],
        'спец. риски': ['4.1.3', '4.2.3', '4.1.4', '4.1.5', '4.2.4', '4.1.2', '4.2.2', '4.4'],
        'прочее': ['4.1.6', '4.2.1', '4.2.5', '4.2.6', '4.2.7', '4.2.8', '5', '7', '8'],
        'страхование нежизни': ['3.1', '3.2', '4.1.1', '6', '4.1.3', '4.2.3', '4.1.4', '4.1.5', '4.2.4', '4.1.2', '4.2.2', '4.4', '4.1.7', '4.3', '4.1.8', '4.1.6', '4.2.1', '4.2.5', '4.2.6', '4.2.7', '4.2.8', '5', '7', '8']
    }

    # Create a mapping from subcategories to their special categories, but only for selected special categories
    subline_to_parent = {
        sub: parent 
        for parent, subs in parent_lines.items() 
        for sub in subs 
        if parent in selected_linemains
    }

    # Create a set of all relevant linemains (selected + subcategories of selected special categories)
    relevant_lines = set(selected_linemains)
    for line in selected_linemains:
        if line in parent_lines:
            relevant_lines.update(parent_lines[line])

    original_columns = df.columns.tolist()
    
    # Filter the DataFrame to include only relevant linemains
    df_filtered = df[df['linemain'].isin(relevant_lines)].copy()

    # Rename sublines their parents' names, but only if the parent is selected
    df_filtered['linemain'] = df_filtered['linemain'].map(lambda x: subline_to_parent.get(x, x))

    # Perform the groupby aggregation
    index_columns = [col for col in df.columns if col not in ['linemain', 'value']]
    df = df_filtered.groupby(index_columns + ['linemain'])['value'].sum().reset_index()

    df_all_lines = df.groupby(index_columns)['value'].sum().reset_index()
    df_all_lines['linemain'] = 'all_lines'
    
    result_df = pd.concat([df, df_all_lines], ignore_index=True)
    result_df = result_df.reindex(columns=original_columns)
    logger.debug(f"Final DataFrame shape: {result_df.shape}")
    logger.info("Finished filter_by_insurance_lines function")
    logger.debug(f"Uunique quarters in df after filter_by_insurance_lines: {result_df['year_quarter'].unique().tolist()}")     
    save_df_to_csv(result_df, f"filter_by_insurance_lines.csv")
    
    return result_df


#@profile




def add_calculated_metrics(df: pd.DataFrame, selected_metrics: List[str]) -> pd.DataFrame:
    logger.info("Starting add_calculated_metrics function")
    logger.info(f"Input DataFrame shape: {df.shape}")
    logger.info(f"Selected metrics: {selected_metrics}")
    logger.info(f"Unique metrics in df before: {df['metric'].unique().tolist()}")
    logger.info(f"df index: {df.index}")
    logger.info(f"df dtypes: {df.dtypes}")
    # Identify grouping columns
    grouping_cols = [col for col in df.columns if col not in ['metric', 'value']]
    logger.info(f"Grouping columns: {grouping_cols}")

    relevant_calculated_metrics = set()
    for selected_metric in selected_metrics:
        logger.info(f"Processing selected metric: {selected_metric}")
    
        # Check if the selected metric starts with any calculated ratio
        # Check if the selected metric starts with any calculated ratio
        for calculated_metric in calculated_metrics:
            if selected_metric.startswith(calculated_metric):
                relevant_calculated_metrics.add(calculated_metric)  # Add ratio_metric to the set
                logger.debug(f"Selected metric {selected_metric} starts with calculated ratio {calculated_metric}. Adding ratio_metric: {calculated_metric}")
    
    logger.info(f"added  relevant_calculated_metrics: {list(relevant_calculated_metrics)}")
    
    selected_metrics = list(relevant_calculated_metrics) + selected_metrics
    
    selected_metrics = list(set(selected_metrics))
    
    logger.info(f"selected_metrics  after andding relevant calculated metrics: {list(selected_metrics)}")



    def calculate_for_group(group):
        metrics_dict = dict(zip(group['metric'], group['value']))
        new_rows = []
        logger.debug("Starting calculate_for_grous function")
        
        calculated_metrics = {
            'net_balance': lambda d: d.get('ceded_losses', 0) - d.get('ceded_premiums', 0),
            'total_premiums': lambda d: d.get('direct_premiums', 0) + d.get('inward_premiums', 0),
            'net_premiums': lambda d: d.get('direct_premiums', 0) + d.get('inward_premiums', 0) - d.get('ceded_premiums', 0),
            'total_losses': lambda d: d.get('direct_losses', 0) + d.get('inward_losses', 0),
            'net_losses': lambda d: d.get('direct_losses', 0) + d.get('inward_losses', 0) - d.get('ceded_losses', 0),
            'gross_result': lambda d: d.get('direct_premiums', 0) + d.get('inward_premiums', 0) - d.get('direct_losses', 0) - d.get('inward_losses', 0),
            'net_result': lambda d: (d.get('direct_premiums', 0) + d.get('inward_premiums', 0) - d.get('ceded_premiums', 0)) - 
                                    (d.get('direct_losses', 0) + d.get('inward_losses', 0) - d.get('ceded_losses', 0))
        }


        
        for metric, calculation in calculated_metrics.items():
            if metric in selected_metrics:
                logger.debug(f"Calculating {metric}")
                try:
                    result = calculation(metrics_dict)
                    
                    # Create a new row with all columns filled
                    new_row = {col: group[col].iloc[0] for col in grouping_cols}
                    new_row.update({'metric': metric, 'value': result})
                    new_rows.append(new_row)
                    
                    logger.debug(f"Calculated {metric} successfully")
                    
                    # Log if any required metrics were missing
                    missing = [key for key in calculation.__code__.co_varnames if key not in metrics_dict]
                    if missing:
                        logger.debug(f"Calculated {metric} with missing metrics treated as 0: {missing}")
                except Exception as e:
                    logger.error(f"Error calculating {metric}: {str(e)}")
        
        if new_rows:
            return pd.concat([group, pd.DataFrame(new_rows)], ignore_index=True)
        logger.info("Finishing calculate_for_grous function")
        
        return group

    # Apply calculations to each group
    result_df = df.groupby(grouping_cols).apply(calculate_for_group).reset_index(drop=True)
    
    logger.info(f"Resulting DataFrame shape: {result_df.shape}")
    logger.debug(f"Unique metrics in result_df: {result_df['metric'].unique().tolist()}")
    logger.debug("Finished add_calculated_metrics function")
    save_df_to_csv(result_df, f"calculate metricss.csv")
   
    return result_df

#@custom_profile
def add_total_rows(df: pd.DataFrame) -> pd.DataFrame:

    if 'total' in df['insurer'].unique():
        return df
   
    original_columns = df.columns.tolist()
    
    # Identify identifier columns (all except 'insurer' and 'value')
    id_columns = [col for col in df.columns if col not in ['insurer', 'value']]

    # Calculate total rows
    df_total = df.groupby(id_columns)['value'].sum().reset_index()
    df_total['insurer'] = 'total'
    result_df = pd.concat([df, df_total], ignore_index=True)
    result_df = result_df.reindex(columns=original_columns)
    save_df_to_csv(result_df, f"add_total_rows.csv")

    return result_df

================
File: process_market_metrics.py
================
import pandas as pd
import numpy as np
import os
import re
from datetime import datetime
from collections import OrderedDict

from typing import List, Tuple, Optional, Dict, OrderedDict, Any, Union, Set
from dataclasses import dataclass
from memory_profiler import profile
import logging

from data_process.data_utils import save_df_to_csv, log_dataframe_info, category_structure, insurer_name_map

from constants.filter_options import MARKET_METRIC_OPTIONS, METRICS, base_metrics, calculated_metrics, calculated_ratios, REINSURANCE_FIG_TYPES, INSURANCE_FIG_TYPES
from constants.mapping import map_insurer



from logging_config import setup_logging, set_debug_level, DebugLevels, get_logger, debug_log, intensive_debug_log, custom_profile

logger = get_logger(__name__)



#@custom_profile

def process_market_metrics(
    df: pd.DataFrame,
    primary_y_metrics: List[str],
    secondary_y_metrics: List[str],
    period_type: str,   
    num_periods: int = 2,
    num_periods_growth: int = 1
       
) -> pd.DataFrame:


    selected_metrics = (primary_y_metrics or []) + (secondary_y_metrics or []) 

    extended_metrics = (selected_metrics or []) + ['total_premiums', 'total_losses']  
    
    df = add_market_share_rows(df)
    df = add_averages_and_ratios(df, extended_metrics)
    df = add_growth_rows_long(df, num_periods, period_type, num_periods_growth)        

    return df




def add_market_share_rows(df):
    logger.debug("Starting add_market_share_rows function.")
    
    # Log unique 'metric' values before processing
    unique_metrics_before = df['metric'].unique()
    logger.debug(f"Unique 'metric' values BEFORE processing: {unique_metrics_before}")
    
    
    # Step 1: Define the group columns (exclude 'insurer' and 'value')
    group_cols = [col for col in df.columns if col not in ['insurer', 'value']]
    logger.debug(f"Grouping columns: {group_cols}")
    
    # Step 2: Extract total values for each group
    total_df = df[df['insurer'] == 'total'][group_cols + ['value']].rename(columns={'value': 'total_value'})
    logger.debug(f"Extracted total_df with {len(total_df)} rows.")
    
    # Step 3: Merge total values back to the original DataFrame
    df_merged = df.merge(total_df, on=group_cols, how='left')
    logger.debug(f"Merged DataFrame has {len(df_merged)} rows.")
    
    # Step 4: Calculate market share
    df_merged['market_share'] = df_merged['value'] / df_merged['total_value']
    
    # Optional: Handle cases where total_value is zero to avoid division by zero
    df_merged['market_share'] = df_merged['market_share'].fillna(0)
    logger.debug("Calculated 'market_share' for all applicable rows.")
    
    # Step 5: Prepare the market share DataFrame
    # Optionally exclude the 'total' insurer if desired
    # df_market_share = df_merged[df_merged['insurer'] != 'total'].copy()
    df_market_share = df_merged.copy()
    
    # Modify the 'metric' column to indicate market share
    df_market_share['metric'] = df_market_share['metric'] + '_market_share'
    logger.debug("Modified 'metric' column to include '_market_share'.")
    
    # Select the necessary columns without duplicating 'metric'
    market_share_cols = [col for col in group_cols if col != 'metric'] + ['metric', 'insurer', 'market_share']
    df_market_share_new = df_market_share[market_share_cols].rename(columns={'market_share': 'value'})
    logger.debug("Prepared the market share DataFrame.")
    
    # Step 6: Append the market share rows to the original DataFrame
    df_final = pd.concat([df, df_market_share_new], ignore_index=True)
    logger.debug(f"Appended market share rows. Original rows: {len(df)}, Market share rows: {len(df_market_share_new)}, Total rows after append: {len(df_final)}")
    
    # Optional: Sort the DataFrame for better readability
    df_final = df_final.sort_values(by=group_cols + ['insurer']).reset_index(drop=True)
    logger.debug("Sorted the final DataFrame.")
    
    # Log unique 'metric' values after processing
    unique_metrics_after = df_final['metric'].unique()
    logger.debug(f"Unique 'metric' values AFTER processing: {unique_metrics_after}")
    
    logger.debug("Completed add_market_share_rows function.")
    
    return df_final

def add_averages_and_ratios(
    df: pd.DataFrame,
    selected_metrics: List[str]
) -> pd.DataFrame:
    """
    Calculates new metric ratios based on selected_metrics and calculated_ratios,
    and transforms existing metrics if needed.
    
    Parameters:
    - df (pd.DataFrame): The input DataFrame with columns ['year_quarter', 'metric', 'linemain', 'insurer', 'value'].
    - selected_metrics (List[str]): A list of metric names for which ratios need to be calculated or transformed.
    
    Returns:
    - pd.DataFrame: The original DataFrame with additional rows for the calculated ratios and transformed metrics.
    """
    logger.info("Starting add_averages_and_ratios function")
    logger.debug(f"Input DataFrame shape: {df.shape}")
    logger.debug(f"Selected metrics: {selected_metrics}")

    # Define the formula mapping for each calculated ratio and metric transformation
    formula_mapping = {
        'average_sum_insured': lambda df: df['sums_end'] / df['contracts_end'] / 1_000_000,
        'average_new_sum_insured': lambda df: df['new_sums'] / df['new_contracts'] / 1_000_000,
        'average_new_premium': lambda df: df['direct_premiums'] / df['new_contracts'],
        'average_loss': lambda df: df['direct_losses'] / df['claims_settled'],
        'ceded_premiums_ratio': lambda df: df.get('ceded_premiums', 0) / df['total_premiums'],
        'ceded_losses_ratio': lambda df: df.get('ceded_losses', 0) / df['total_losses'],
        'ceded_losses_to_ceded_premiums_ratio': lambda df: df.get('ceded_losses', 0) / df.get('ceded_premiums', 1),  # Avoid division by zero
        'gross_loss_ratio': lambda df: (
            df.get('direct_losses', 0) + df.get('inward_losses', 0)
        ) / (
            df.get('direct_premiums', 0) + df.get('inward_premiums', 0)
        ),
        'net_loss_ratio': lambda df: (
            df.get('direct_losses', 0) + df.get('inward_losses', 0) - df.get('ceded_losses', 0)
        ) / (
            df.get('direct_premiums', 0) + df.get('inward_premiums', 0) - df.get('ceded_premiums', 0)
        ),
        'effect_on_loss_ratio': lambda df: (
            (df.get('direct_losses', 0) + df.get('inward_losses', 0)) / 
            (df.get('direct_premiums', 0) + df.get('inward_premiums', 0))
        ) - (
            (df.get('direct_losses', 0) + df.get('inward_losses', 0) - df.get('ceded_losses', 0)) / 
            (df.get('direct_premiums', 0) + df.get('inward_premiums', 0) - df.get('ceded_premiums', 0))
        ),
        'ceded_ratio_diff': lambda df: (
            (df.get('ceded_losses', 0) / df['total_losses']) - 
            (df.get('ceded_premiums', 0) / df['total_premiums'])
        ),
        # Transformation metrics
        'sums_end': lambda x: x / 1000,
        'new_sums': lambda x: x / 1000,
        'new_contracts': lambda x: x * 1000000,
        'contracts_end': lambda x: x * 1000,
        'claims_settled': lambda x: x * 1000,
        'claims_reported': lambda x: x * 1000,
    }

    logger.debug(f"Formula mapping keys: {list(formula_mapping.keys())}")
        
    logger.debug(f"Function called with selected_metrics: {selected_metrics}")
    logger.debug(f"Initial DataFrame shape: {df.shape}")    
    logger.debug(f"unique metric values: {df['metric'].unique()}")  

    
    relevant_metrics = set()
    for selected_metric in selected_metrics:
        logger.debug(f"Processing selected metric: {selected_metric}")
    
        # Check if the selected metric starts with any calculated ratio
        # Check if the selected metric starts with any calculated ratio
        for ratio_metric in calculated_ratios:
            if selected_metric.startswith(ratio_metric):
                relevant_metrics.add(ratio_metric)  # Add ratio_metric to the set
                logger.debug(f"Selected metric {selected_metric} starts with calculated ratio {ratio_metric}. Adding ratio_metric: {ratio_metric}")
    
    logger.debug(f"Relevant metrics after processing: {list(relevant_metrics)}")
    
    selected_metrics = list(relevant_metrics) + selected_metrics
    
    selected_metrics = list(set(selected_metrics))
   
    logger.debug(f"selected metrics after processing: {selected_metrics}")
    # Create a copy of the original DataFrame to avoid modifying the input
    new_df = df.copy()
    logger.debug(f"Created copy of DataFrame. New shape: {new_df.shape}")

    # Separate metrics that need transformation from those that need calculation
    transform_metrics = [
        'sums_end', 'new_sums', 'new_contracts', 
        'contracts_end', 'claims_settled', 'claims_reported'
    ]
    calc_metrics = [m for m in selected_metrics if m not in transform_metrics]
    logger.debug(f"Transform metrics: {transform_metrics}")
    logger.debug(f"Calculation metrics: {calc_metrics}")

    # Transform existing metrics
    for metric in transform_metrics:
        if metric in selected_metrics:
            mask = new_df['metric'] == metric
            new_df.loc[mask, 'value'] = new_df.loc[mask, 'value'].apply(formula_mapping[metric])
            logger.debug(f"Transformed metric: {metric}")

    # Define required metrics for each formula explicitly
    required_metrics = {
        'average_sum_insured': ['sums_end', 'contracts_end'],
        'average_new_sum_insured': ['new_sums', 'new_contracts'],
        'average_new_premium': ['direct_premiums', 'new_contracts'],
        'average_loss': ['direct_losses', 'claims_settled'],
        'ceded_premiums_ratio': ['ceded_premiums', 'total_premiums'],
        'ceded_losses_ratio': ['ceded_losses', 'total_losses'],
        'ceded_losses_to_ceded_premiums_ratio': ['ceded_losses', 'ceded_premiums'],
        'gross_loss_ratio': ['direct_losses', 'inward_losses', 'direct_premiums', 'inward_premiums'],
        'net_loss_ratio': ['direct_losses', 'inward_losses', 'ceded_losses', 'direct_premiums', 'inward_premiums', 'ceded_premiums'],
        'effect_on_loss_ratio': ['direct_losses', 'inward_losses', 'ceded_losses', 'direct_premiums', 'inward_premiums', 'ceded_premiums'],
        'ceded_ratio_diff': ['ceded_losses', 'total_losses', 'ceded_premiums', 'total_premiums'],
    }

    # Calculate new metrics
    if calc_metrics:
        logger.debug("Entering calculation block")
        index_columns = [col for col in df.columns if col not in ['metric', 'value']]
        logger.debug(f"Index columns: {index_columns}")
        
        # Prepare the data for calculation
        pivoted_data = new_df.pivot_table(
            values='value', 
            index=index_columns, 
            columns='metric',
            aggfunc='first'  # Assuming each combination is unique
        ).reset_index()
        logger.debug(f"Pivoted data shape: {pivoted_data.shape}")
        logger.debug(f"Pivoted data columns: {pivoted_data.columns}")

        # Iterate over each ratio to calculate
        for ratio in calc_metrics:
            logger.debug(f"Calculating ratio: {ratio}")
            logger.debug(f"Formula exists for {ratio}: {ratio in formula_mapping}")
            
            if ratio in formula_mapping:
                try:
                    # Identify the required metrics for the current ratio
                    metrics_needed = required_metrics.get(ratio, [])
                    logger.debug(f"Metrics needed for {ratio}: {metrics_needed}")
                    
                    # Ensure all required metrics are present, adding missing ones with default value 0
                    for metric in metrics_needed:
                        if metric not in pivoted_data.columns:
                            pivoted_data[metric] = 0
                            logger.debug(f"Added missing column {metric} with default value 0")
                    
                    # Optionally, fill NaN values in required metrics with 0
                    pivoted_data.fillna({metric: 0 for metric in metrics_needed}, inplace=True)
                    
                    # Apply the formula
                    pivoted_data[ratio] = formula_mapping[ratio](pivoted_data)
                    
                    logger.debug(f"Calculated values for {ratio}: {pivoted_data[ratio].head()}")
                    logger.debug(f"NaN count for calculated {ratio}: {pivoted_data[ratio].isna().sum()}")
                    
                    # Check if the ratio was successfully calculated
                    if pivoted_data[ratio].isna().all():
                        logger.warning(f"All calculated values for {ratio} are NaN. Skipping adding these rows.")
                        continue
                    
                    # Prepare the new rows
                    new_rows = pivoted_data.melt(
                        id_vars=index_columns,
                        value_vars=[ratio],
                        var_name='metric',
                        value_name='value'
                    )
                    logger.debug(f"New rows shape for {ratio}: {new_rows.shape}")

                    # Append the new rows to the DataFrame
                    new_df = pd.concat([new_df, new_rows], ignore_index=True)
                    logger.debug(f"Updated DataFrame shape after adding {ratio}: {new_df.shape}")
                except Exception as e:
                    logger.error(f"Error calculating {ratio}: {str(e)}")
                    continue  # Skip to the next ratio if there's an error

    logger.debug(f"Final DataFrame shape: {new_df.shape}")
    logger.debug(f"Final metrics: {new_df['metric'].unique()}")
    logger.info("Finished add_averages_and_ratios function")
    
    return new_df



def add_growth_rows_long(df: pd.DataFrame, num_periods: int = 2, period_type: str = 'previous_quarter',     num_periods_growth: int = 1) -> pd.DataFrame:
    logger.debug(f"Function add_growth_rows_long called with unique metrics: {df['metric'].unique()}")
    logger.debug(f"Function add_growth_rows_long called with year quarters: {df['year_quarter'].unique()}")

    if not pd.api.types.is_datetime64_any_dtype(df['year_quarter']):
        df['year_quarter'] = pd.to_datetime(df['year_quarter'], errors='coerce')
        if df['year_quarter'].isnull().any():
            logger.debug("Some 'year_quarter' entries could not be converted to datetime.")

    # Identify grouping columns (excluding 'year_quarter', 'metric', and 'value')
    grouping_columns = [col for col in df.columns if col not in ['year_quarter', 'metric', 'value']]
    logger.debug(f"group_column: {grouping_columns}")         

    if period_type == 'previous_quarter':
        # Extract quarter from datetime column
        df['quarter'] = df['year_quarter'].dt.quarter
        #df.to_csv('df.csv')
        grouping_columns_with_period = grouping_columns + ['quarter']
        logger.debug("Added 'quarter' column for 'previous_quarter' period type.")
        logger.debug(f"quarters_original: \n{df['year_quarter'].head()}")
        logger.debug(f"quarters_extracted: \n{df['quarter'].head()}")
    else:
        grouping_columns_with_period = grouping_columns


    logger.debug(f"group_column with period: {grouping_columns_with_period}")         
    
    # Sort the DataFrame by grouping columns and 'year_quarter'
    df_sorted = df.sort_values(by=grouping_columns_with_period + ['year_quarter']).copy()
    logger.info(f"Uunique year quarter df_sorted : {df_sorted['year_quarter'].unique().tolist()}")   

    # Identify metrics that end with 'market_share'
    is_market_share = df_sorted['metric'].str.endswith('market_share')

    # Define a threshold for near-zero values
    epsilon = 1e-8
    
    # Handle non-market_share metrics
    non_ms = ~is_market_share
    group_cols = grouping_columns_with_period + ['metric']
    logger.debug(f"ggroup_cols: {group_cols}")         
    
    # Calculate previous values within each group
    df_sorted_non_ms = df_sorted[non_ms].copy()
    df_sorted_non_ms['previous'] = df_sorted_non_ms.groupby(group_cols)['value'].shift(1)
    
    # Calculate percentage change with safe handling
    df_sorted_non_ms['pct_change'] = np.where(
        df_sorted_non_ms['previous'] > epsilon,  # Valid denominator
        (df_sorted_non_ms['value'] - df_sorted_non_ms['previous']) / df_sorted_non_ms['previous'],  # pct_change
        np.nan  # Default value when denominator is near zero or invalid
    )
    
    # Fill any remaining NaN values if necessary
    #df_sorted_non_ms['pct_change'] = df_sorted_non_ms['pct_change'].fillna(0)
    
    # Handle market_share metrics by calculating absolute difference
    df_sorted_ms = df_sorted[is_market_share].copy()
    df_sorted_ms['abs_diff'] = df_sorted_ms.groupby(group_cols)['value'].diff().fillna(0)


    # Initialize a new column for growth
    df_sorted['growth'] = 0

    # Assign calculated growth values appropriately
    df_sorted['growth'] = df_sorted['growth'].astype(float)  # Convert 'growth' to float if it's not already

    df_sorted.loc[non_ms, 'growth'] = df_sorted_non_ms['pct_change'].values
    df_sorted.loc[is_market_share, 'growth'] = df_sorted_ms['abs_diff'].values
    
    # Prepare the growth DataFrame
    growth_df = df_sorted.copy()
    growth_df['metric'] = growth_df['metric'] + '_q_to_q_change'
    growth_df['value'] = growth_df['growth']


    # Select relevant columns for growth DataFrame
    growth_df = growth_df.drop(columns=['growth'])
    original_df = df_sorted.drop(columns=['growth'])
    logger.debug(f" unique metrics in growth_df: {growth_df['metric'].unique()}")

    # Sort 'year_quarter' in descending order to identify the most recent periods
    original_df_sorted = original_df.sort_values(by='year_quarter', ascending=False).copy()
    logger.info(f"Uunique year quarter sorted:: {original_df_sorted['year_quarter'].unique().tolist()}")   
    logger.info(f"Uunique  metric sorted:: {original_df_sorted['metric'].unique().tolist()}")   

    growth_df_sorted = growth_df.sort_values(by='year_quarter', ascending=False).copy()
    logger.info(f"Uunique year quarter growth:: {growth_df_sorted['year_quarter'].unique().tolist()}")   
    
    # Get unique 'year_quarter' values sorted descending
    unique_quarters = original_df_sorted['year_quarter'].drop_duplicates().sort_values(ascending=False)
    logger.info(f"quarters_original: {unique_quarters}")    
    # Determine quarters for original_df and growth_df
    quarters_original = unique_quarters.iloc[:num_periods]
    logger.info(f"quarters_original: {quarters_original}")    
    
    
    quarters_growth = unique_quarters.iloc[:max(num_periods_growth, 1)]  # Ensure at least 1 period
    logger.info(f"quarters_growths: {quarters_growth}")    
    
    # Filter original_df for the specified number of periods
    original_df_filtered = original_df_sorted[original_df_sorted['year_quarter'].isin(quarters_original)].copy()
    
    # Filter growth_df for num_periods - 1 periods
    growth_df_filtered = growth_df_sorted[growth_df_sorted['year_quarter'].isin(quarters_growth)].copy()
    
    # Concatenate the original and growth DataFrames
    result_df = pd.concat([original_df_filtered, growth_df_filtered], ignore_index=True)

    if period_type == 'previous_quarter':
        result_df = result_df.drop(columns=['quarter'])
        logger.debug("Dropped 'quarter' column from the final result.")
    
    
    # Optionally, sort the result for better readability
    result_df = result_df.sort_values(by=grouping_columns + ['year_quarter', 'metric']).reset_index(drop=True)
    #result_df = result_df.sort_values(by='year_quarter', ascending=False)
 
    #result = result_df[result_df['year_quarter'].isin(result_df['year_quarter'].unique()[:num_periods])]
   
    #unique_quarters = result['year_quarter'].drop_duplicates().sort_values(ascending=False)
    #filtered_unique_quarters = unique_quarters.iloc[:num_periods]
    #result = result[result['year_quarter'].isin(filtered_unique_quarters)].copy()
    
    return result_df

================
File: process_ranks.py
================
import pandas as pd
import numpy as np
import os
import re
from datetime import datetime
from collections import OrderedDict

from typing import List, Tuple, Optional, Dict, OrderedDict, Any, Union, Set
from dataclasses import dataclass
from memory_profiler import profile
import logging

from data_process.data_utils import save_df_to_csv, log_dataframe_info
from constants.filter_options import METRICS
from constants.mapping import map_insurer



from logging_config import get_logger, custom_profile
logger = get_logger(__name__)



#@custom_profile

def process_for_ranks(
    df: pd.DataFrame, 
    main_insurer: str,
    benchmark: List[str],
    comparison_insurers: List[str],
    primary_y_metrics: List[str],    
    secondary_y_metrics: List[str],
    number_of_insurers: int,
    top_n_list = [5, 10, 20]    
) -> pd.DataFrame:

    selected_insurers = [main_insurer] + (comparison_insurers or []) + (benchmark or [])

    selected_metrics = (primary_y_metrics or []) + (secondary_y_metrics or []) 
    extended_metrics = (selected_metrics or []) + ['total_premiums', 'total_losses']
    
    df = add_top_n_rows(df, top_n_list)
    
    ranking_df = add_others_rows(
        df, selected_insurers, top_n_list)
    
    top_n_insurers, insurer_options, benchmark_options, compare_options = get_top_n_insurers_and_insurer_options(
            ranking_df, main_insurer, number_of_insurers, extended_metrics, top_n_list) 

    logger.debug(f"Unique insurers after Step 2: {df['insurer'].unique().tolist()}")
    #logger.debug(f"top_n_insurers {top_n_insurers}")
    #logger.debug(f"insurer_options {insurer_options}")
    #logger.debug(f"benchmark_options {benchmark_options}")
    #logger.debug(f"compare_options {compare_options}")
   
    save_df_to_csv(ranking_df, f"step_2_ranking_processed.csv")
  
    return ranking_df, top_n_insurers, insurer_options, benchmark_options, compare_options





def get_top_n_insurers_and_insurer_options(df: pd.DataFrame, selected_insurer: str, number_of_insurers: int, extended_metrics: List[str], top_n_list: Optional[List[int]] = None) -> List[str]:

    logger.debug(f"Starting get_top_n_insurers function with n={number_of_insurers} and metrics={extended_metrics}")
    logger.debug(f"Input DataFrame shape: {df.shape}")
    
    df = df[df['linemain'] == 'all_lines']
    all_insurers_original = set(df['insurer'].unique())
   
    # Get the most recent year_quarter and filter df
    end_quarter = df['year_quarter'].max()
    end_quarter_df = df[df['year_quarter'] == end_quarter]
    logger.debug(f"end_quarter: {end_quarter}")

    # Filter df to get insurer options sorted based on total premiums
    
    insurer_options_df = end_quarter_df[end_quarter_df['metric'] != 'total_premiums']
    insurer_options_df = insurer_options_df.sort_values(['value'], ascending=[False]).reset_index(drop=True)
   
    all_insurers_last_quarter = insurer_options_df['insurer'].unique().tolist()
    insurers_not_in_last_quarter = list(all_insurers_original - set(all_insurers_last_quarter))
    all_insurers = all_insurers_last_quarter + insurers_not_in_last_quarter

    #logger.debug(f"all_insurers: {all_insurers}")

    if top_n_list is None:
        top_n_list = [5, 10, 20]
    
    benchmark_insurers = [f"top-{n}-benchmark" for n in top_n_list]
    top_rows = [f"top-{n}" for n in top_n_list]
    
    
    df_top_insurers = df[df['insurer'].isin(top_rows)]
    df = df[~df['insurer'].isin(top_rows)]

    
    top_insurers_options = [{'label': map_insurer(insurer), 'value': insurer} 
                         for insurer in top_rows + ['total']]  
    
    insurer_options = [
        {'label': map_insurer(insurer), 'value': insurer} 
        for insurer in all_insurers if insurer not in benchmark_insurers and insurer != 'others'
    ]
    
    benchmark_options = [{'label': map_insurer(insurer), 'value': insurer} 
                         for insurer in benchmark_insurers + ['total'] + ['others']]
    
    compare_options = [{'label': map_insurer(insurer), 'value': insurer} 
                       for insurer in all_insurers if insurer not in benchmark_insurers and insurer != selected_insurer]

    ranking_metric = extended_metrics[0]
    logger.debug(f"Ranking insurers based on metric: {ranking_metric}")
    ranking_df = end_quarter_df[end_quarter_df['metric'] == ranking_metric]

    
    insurers_to_exclude = ['others'] + top_rows + benchmark_insurers + ['total']    
    
    ranking_df = ranking_df[~ranking_df['insurer'].isin(insurers_to_exclude)]
    
    ranking_df = ranking_df.sort_values(['value'], ascending=[False]).reset_index(drop=True)
    logger.info(f"ranking_df unique insurers: {ranking_df['insurer'].unique()}")

    # Get the top-n insurers
    top_n_insurers = ranking_df['insurer'].head(number_of_insurers).tolist()
    
    logger.info(f"Top {number_of_insurers} insurers: {top_n_insurers}")
    #logger.debug(f"Top {number_of_insurers} {ranking_metric} values: {sorted_df[ranking_metric].head(number_of_insurers).tolist()}")

    all_insurers_options = insurer_options + top_insurers_options 
    
    return top_n_insurers, all_insurers_options, benchmark_options, compare_options



def add_top_n_rows(df: pd.DataFrame, top_n_list: List[int]) -> pd.DataFrame:
    original_columns = df.columns.tolist()
    
    
    df_total = df[df['insurer'].isin(['total', 'others'])]
    df = df[~df['insurer'].isin(['total', 'others'])]
    
    
    if top_n_list is None:
        top_n_list = [5, 10, 20]   
    # Identify identifier columns (all except 'insurer' and 'value')
    id_columns = [col for col in df.columns if col not in ['insurer', 'value']]
    
    result_df = df.copy()
    
    for n in top_n_list:
        # Group by id_columns and get top N values for each group
        df_top_n = df.groupby(id_columns).apply(lambda x: x.nlargest(n, 'value'))
        
        # Reset index to flatten the result
        df_top_n = df_top_n.reset_index(drop=True)
        
        # Sum the top N values
        df_top_n_sum = df_top_n.groupby(id_columns)['value'].sum().reset_index()
        
        # Add the 'insurer' column with the appropriate label
        df_top_n_sum['insurer'] = f'top-{n}'
        
        # Concatenate with the result DataFrame
        result_df = pd.concat([result_df, df_top_n_sum], ignore_index=True)


    # Reorder columns to match original order
    result_df = result_df.reindex(columns=original_columns)

    df_final = pd.concat([result_df, df_total], ignore_index=True)
    save_df_to_csv(df_final, f"add_top_n_rows.csv")

    
    return df_final



def add_others_rows(df: pd.DataFrame, selected_insurers: List[str], top_n_list: List[int]) -> pd.DataFrame:

    # Remove 'others' from selected_insurers if present
    selected_insurers = [insurer for insurer in selected_insurers if insurer != 'others']

    # Check if 'top_10' is in selected_insurers
    top_10_selected = 'top-10' in selected_insurers
    
    top_rows = [f"top-{n}" for n in top_n_list]
    benchmark = [f"top-{n}-benchmark" for n in top_n_list]
    
    # Add 'total' to the list of insurers to be filtered out
    insurers_to_exclude = selected_insurers + top_rows + benchmark+ ['total']    

    selected_top_n = next((f'top-{n}' for n in top_n_list if f'top-{n}' in selected_insurers or f'top-{n}-benchmark' in selected_insurers), None)
    
    original_columns = df.columns.tolist()
    df_others = df[~df['insurer'].isin(insurers_to_exclude)]
    
    # Identify identifier columns (all except 'insurer' and 'value')
    id_columns = [col for col in df.columns if col not in ['insurer', 'value']]
    
    # Calculate others rows
    df_others = df_others.groupby(id_columns)['value'].sum().reset_index()
    
    # If a top-N is selected, subtract its values from others
    if selected_top_n:
        df_top_n = df[df['insurer'] == selected_top_n]
        df_others = df_others.merge(df_top_n[id_columns + ['value']], on=id_columns, how='left', suffixes=('', '_top_n'))
        df_others['value'] = df_others['value'] - df_others['value_top_n'].fillna(0)
        df_others = df_others.drop(columns=['value_top_n'])
    
    df_others['insurer'] = 'others'
    
    # Remove rows where 'value' is 0 or negative after subtraction
    df_others = df_others[df_others['value'] > 0]
    
    result_df = pd.concat([df, df_others], ignore_index=True)
    result_df = result_df.reindex(columns=original_columns)
    save_df_to_csv(result_df, f"add_others_rows.csv")
    
    return result_df

================
File: steps.md
================
process_dataframe.py

    df = filter_by_date_range_and_period_type(
            df, period_type, start_quarter, end_quarter, num_periods)       
    df = filter_by_life_nonlife_and_direct_inward(df, premium_loss_selection)        
    df = filter_reinsurance(df, reinsurance_form, reinsurance_geography, reinsurance_type)  
    df = filter_required_metrics(df, extended_metrics) 
    df = filter_by_insurance_lines(df, selected_linemains)
    df = add_calculated_metrics(df, selected_metrics)
    df = add_total_rows(df)

process_ranks.py

    df = df[df['linemain'] == 'all_lines']

    df = add_top_n_rows(df, top_n_list)
    
    ranking_df = add_others_rows(
        df, selected_insurers, top_n_list)
    
    top_n_insurers, insurer_options, benchmark_options, compare_options = get_top_n_insurers_and_insurer_options(
            ranking_df, main_insurer, number_of_insurers, selected_metrics, top_n_list) 


process_market_metrics.py



    extended_metrics = (selected_metrics or []) + ['total_premiums', 'total_losses']  
    
    df = add_market_share_rows(df)
    df = add_averages_and_ratios(df, extended_metrics)
    df = add_growth_rows_long(df, num_periods, period_type)

================
File: table_data.py
================
import pandas as pd
import logging
from typing import List
from data_process.data_utils import save_df_to_csv
from typing import List, Tuple, Optional, Dict, OrderedDict, Any, Union, Set
import numpy as np
from io import StringIO

import pandas as pd
import numpy as np
import os
from typing import List, Tuple, Optional, Dict, OrderedDict, Any, Union, Set
import logging
from dataclasses import dataclass
from data_process.data_utils import save_df_to_csv, log_dataframe_info, category_structure, insurer_name_map
from datetime import datetime
import re
from constants.filter_options import MARKET_METRIC_OPTIONS, METRICS, base_metrics, calculated_metrics, calculated_ratios, REINSURANCE_FIG_TYPES, INSURANCE_FIG_TYPES
from constants.mapping import map_insurer
from memory_profiler import profile

import json
import pandas as pd
from collections import OrderedDict




from logging_config import get_logger
logger = get_logger(__name__)

def create_table_data(
    df: pd.DataFrame,
    primary_y_metrics: List[str],
    secondary_y_metrics: List[str],
    top_n_insurers: List[str],
    number_of_insurers: int = 20,
    top_n_list: List[int] = [5, 10, 20]
) -> pd.DataFrame:

    selected_metrics = (primary_y_metrics or []) + (secondary_y_metrics or [])            

    table_df = table_data_filter(df, top_n_insurers, number_of_insurers, selected_metrics, top_n_list)
    table_df = table_data_pivot(table_df, selected_metrics)

    return table_df

def table_data_filter(
    df: pd.DataFrame,
    top_n_insurers: List[str],
    number_of_insurers: int,
    all_metrics: List[str],
    top_n_list: List[int] = [5, 10, 20]
) -> pd.DataFrame:
    """
    Processes the DataFrame to generate a pivot table with sorted metric columns.
    """
    logger.info(f"Top N insurers: {top_n_insurers}")
    logger.info(f"Number of insurers to select: {number_of_insurers}")
    save_df_to_csv(df, "16_1_table_inter.csv")  # Ensure this function handles paths correctly
    
    # Filter for 'all_lines'
    if 'all_lines' not in df['linemain'].values:
        group_columns = [col for col in df.columns if col not in ['linemain', 'value']]
        df = df.groupby(group_columns)['value'].sum().reset_index()
        df['linemain'] = 'all_lines'
    else:
        df = df[df['linemain'] == 'all_lines']    
    # Define summary rows
    save_df_to_csv(df, "16_2_table_inter.csv")  # Ensure this function handles paths correctly

    summary_rows = [f'top-{n}' for n in top_n_list] + ['total']
    logger.info(f"Summary rows: {summary_rows}")
    insurers_to_keep = top_n_insurers + summary_rows
    logger.info(f"insurers_to_keep: {insurers_to_keep}")
    
    # Create a list of metrics to keep
    metrics_to_keep = (
        all_metrics +
        [f"{m}_q_to_q_change" for m in all_metrics] +
        [f"{m}_market_share" for m in all_metrics] +
        [f"{m}_market_share_q_to_q_change" for m in all_metrics]
    )
    logger.debug(f"Metrics to keep: {metrics_to_keep}")
    
    # Filter the DataFrame to keep only the selected insurers and metrics
    df = df[df['metric'].isin(metrics_to_keep)].copy()
    df = df[df['insurer'].isin(insurers_to_keep)]
    
    save_df_to_csv(df, "16_3_table_inter.csv")  # Ensure this function handles paths correctly
    
    # Sort the grouped DataFrame by year_quarter (most recent first), then by the original insurer order
    df_sorted = df.sort_values(['year_quarter', 'metric'], ascending=[False, True])
    logger.debug("Sorted the grouped DataFrame by 'year_quarter' and 'metric'.")
    
    # Log the unique values present in the sorted DataFrame
    logger.info(f"Insurers present in df_grouped_sorted: {df_sorted['insurer'].unique()}")
    logger.info(f"Metrics present in df_grouped_sorted: {df_sorted['metric'].unique()}")
    logger.info(f"Year quarters present in df_grouped_sorted: {df_sorted['year_quarter'].unique()}")
        
    return df_sorted

    


def table_data_pivot(df: pd.DataFrame, all_metrics: List[str]) -> pd.DataFrame:
    # Step 1: Convert 'year_quarter' to datetime
    df['year_quarter'] = pd.to_datetime(df['year_quarter'])
    
    # Step 2: Format 'year_quarter' as 'YYYYQX' (e.g., '2024Q2')
    df['year_quarter'] = df['year_quarter'].dt.to_period('Q').astype(str)
    
    # Step 3: Define the order of attributes, including the base metric as ''
    attributes_order = ['', 'q_to_q_change', 'market_share', 'market_share_q_to_q_change']
    
    # Step 4: Function to extract base_metric and attribute from the 'metric' column
    def extract_metric_attribute(metric, base_metrics):
        for base in base_metrics:
            if metric.startswith(base):
                suffix = metric[len(base):]  # Extract the suffix after the base metric
                if suffix.startswith('_'):
                    suffix = suffix[1:]  # Remove leading underscore
                else:
                    suffix = ''  # No suffix
                return pd.Series([base, suffix])
        return pd.Series([metric, ''])  # If no base metric matches
    
    # Step 5: Apply the function to extract 'base_metric' and 'attribute'
    df[['base_metric', 'attribute']] = df.apply(
        lambda row: extract_metric_attribute(row['metric'], all_metrics), axis=1
    )
    
    # Step 6: Function to create unified column names
    def create_column_name(row):
        if row['attribute']:
            return f"{row['base_metric']}_{row['year_quarter']}_{row['attribute']}"
        else:
            return f"{row['base_metric']}_{row['year_quarter']}"
    
    # Step 7: Apply the function to create the 'column_name'
    df['column_name'] = df.apply(create_column_name, axis=1)
    
    # Step 8: Pivot the table to wide format
    pivot_df = df.pivot_table(
        index='insurer',
        columns='column_name',
        values='value',
        aggfunc='first'  # Assumes no duplicate entries per insurer and column_name
    ).reset_index()
    
    # Step 9: Flatten the MultiIndex columns if necessary
    pivot_df.columns = pivot_df.columns.get_level_values(0)
    
    # Step 10: Sort the years in descending order to have the latest year first (e.g., ['2024Q2', '2023Q2', ...])
    years_sorted = sorted(df['year_quarter'].unique(), reverse=True)
    
    # Step 11: Initialize the list of desired columns with 'insurer'
    desired_columns = ['insurer']
    
    # Step 12: Iterate through each metric and each attribute to append columns in the specified order
    for metric in all_metrics:
        for attr in attributes_order:
            for year in years_sorted:
                if attr == '':
                    column_name = f"{metric}_{year}"
                else:
                    column_name = f"{metric}_{year}_{attr}"
                desired_columns.append(column_name)
    
    # Step 13: Remove duplicates while preserving order
    desired_columns = list(OrderedDict.fromkeys(desired_columns))
    
    # Step 14: Add missing columns with NaN to ensure all desired columns are present
    for col in desired_columns:
        if col not in pivot_df.columns:
            pivot_df[col] = pd.NA
    
    # Step 15: Reorder the DataFrame columns to match the desired order
    df = pivot_df[desired_columns]
    
    # Step 16: Insert 'N' as the first column (row identifier)
    #pivot_df.insert(0, 'N', range(1, len(pivot_df) + 1))
    mask = ~((df.iloc[1:] == 0) | df.iloc[1:].isna()).all() & ~df.iloc[1:].isna().all()
    df = df.loc[:, mask]
    df = df.fillna('n/a')
    
    # Identify summary rows that start with 'top' or 'total'
    summary_prefixes = ('top', 'total')
    summary_df = df[df['insurer'].str.startswith(summary_prefixes)]
    main_df = df[~df['insurer'].str.startswith(summary_prefixes)]
    
    insurer_idx = df.columns.get_loc('insurer')
    sorting_col = df.columns[insurer_idx + 1]

    main_df = main_df.copy()
    summary_df = summary_df.copy()

    # Convert the sorting column to numeric, coercing errors to NaN
    main_df[sorting_col] = pd.to_numeric(main_df[sorting_col], errors='coerce')
    summary_df[sorting_col] = pd.to_numeric(summary_df[sorting_col], errors='coerce')
    
    # Sort main insurers in descending order
    sorted_main = main_df.sort_values(by=sorting_col, ascending=False)
    
    # Sort summary rows in descending order
    sorted_summary = summary_df.sort_values(by=sorting_col, ascending=True)
        
    # Step 6: Concatenate the Sorted DataFrames
    sorted_df = pd.concat([sorted_main, sorted_summary], ignore_index=True)
    
    # Step 7: Add the Sequential Numbering Column (`N`)
    main_length = len(sorted_main)
    number_sequence = list(range(1, main_length + 1))
    summary_length = len(sorted_summary)
    number_sequence += [np.nan] * summary_length
    sorted_df['N'] = number_sequence
    
    # Step 8: Finalize the DataFrame
    # Move the 'N' column to the end
    cols = list(sorted_df.columns)
    cols.remove('N')
    pivot_df = sorted_df[['N'] + cols]

        

    return pivot_df
