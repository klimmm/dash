This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-09T17:14:38.541Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Directory Structure
================================================================
app/
  callbacks/
    app_layout_callbacks.py
    filter_update_callbacks.py
    insurance_lines_callbacks.py
    period_filter.py
  components/
    dash_table.py
    insurance_lines_tree.py
  __init__.py
  app_layout.py
config/
  default_values.py
  logging_config.py
  main_config.py
constants/
  filter_options.py
  translations.py
data_process/
  data_utils.py
  process_filters.py
  table_data.py
app.py

================================================================
Files
================================================================

================
File: app/callbacks/app_layout_callbacks.py
================
# app_layout_callbacks.py

import dash
from dash import Input, Output, State
from config.logging_config import get_logger, track_callback, track_callback_end
logger = get_logger(__name__)


def setup_tab_state_callbacks(app: dash.Dash) -> None:
    """Setup callbacks for tab state and visibility."""

    @app.callback(
        [
            Output("data-table-content", "style"),
            Output("show-data-table", "data"),
            Output("data-table-tab", "className"),
        ],
        [Input("data-table-tab", "n_clicks")],
        prevent_initial_call=True
    )
    def toggle_visibility(data_table_clicks):
        """Toggle visibility of table content."""
        ctx = dash.callback_context
        start_time = track_callback('app.tab_state_callbacks', 'toggle_visibility', ctx)

        try:
            # Initial load - show table by default
            if not data_table_clicks:
                result = (
                    {"display": "block"},
                    True,
                    "tab-like-button active"
                )
                track_callback_end('app.tab_state_callbacks', 'toggle_visibility', start_time, result=result)
                return result

            # Toggle based on button click
            is_showing = True
            style = {"display": "block"}
            button_class = "tab-like-button active"

            result = (style, is_showing, button_class)
            track_callback_end('app.tab_state_callbacks', 'toggle_visibility', start_time, result=result)
            return result

        except Exception as e:
            logger.exception("Error in toggle_visibility")
            track_callback_end('app.tab_state_callbacks', 'toggle_visibility', start_time, error=str(e))
            raise

    @app.callback(
        Output("debug-collapse", "is_open"),
        Input("debug-toggle", "n_clicks"),
        State("debug-collapse", "is_open"),
    )
    def toggle_debug_collapse(n_clicks: int, is_open: bool) -> bool:
        """Toggle the debug log collapse."""
        ctx = dash.callback_context
        start_time = track_callback('app.tab_state_callbacks', 'toggle_debug_collapse', ctx)

        try:
            if n_clicks:
                result = not is_open
                track_callback_end('app.tab_state_callbacks', 'toggle_debug_collapse', start_time, result="not is_open")
                return result
            track_callback_end('app.tab_state_callbacks', 'toggle_debug_collapse', start_time, result="is_open")
            return is_open

        except Exception as e:
            logger.exception("Error in toggle_debug_collapse")
            track_callback_end('app.tab_state_callbacks', 'toggle_debug_collapse', start_time, error=str(e))
            raise



def setup_sidebar_callbacks(app: dash.Dash) -> None:
    """Setup callbacks for sidebar toggle functionality."""
    
    @app.callback(
        [
            Output("sidebar-filters", "className"),
            Output("sidebar-col", "className"),
            Output("main-content-col", "className"),
            Output("toggle-sidebar-button", "children")
        ],
        [Input("toggle-sidebar-button", "n_clicks")],
        [State("sidebar-filters", "className")]
    )
    def toggle_sidebar(n_clicks, current_class):
        """Toggle sidebar visibility and update button text."""
        if not n_clicks:
            # Initial state - now expanded
            return (
                "sidebar-filters expanded",  # Changed from 'collapsed' to 'expanded'
                "sidebar-col expanded",      # Added 'expanded'
                "main-col shifted",          # Added 'shifted'
                "Hide Filters"               # Changed from 'Show Filters' to 'Hide Filters'
            )
            
        # Toggle based on current state
        if "collapsed" in (current_class or ""):
            return (
                "sidebar-filters expanded",
                "sidebar-col expanded",
                "main-col shifted",
                "Hide Filters"
            )
        else:
            return (
                "sidebar-filters collapsed",
                "sidebar-col",
                "main-col",
                "Show Filters"
            )

    @app.callback(
        Output("sidebar-filters", "style"),
        [Input("navbar-toggler", "n_clicks")],
        [State("sidebar-filters", "style")]
    )
    def handle_mobile_sidebar(n_clicks, current_style):
        """Handle sidebar visibility on mobile."""
        if not n_clicks:
            # Initial state for mobile - visible
            return {"display": "block"}
            
        current_style = current_style or {}
        new_style = current_style.copy()
        
        # Toggle display on mobile
        if new_style.get("display") == "none":
            new_style["display"] = "block"
        else:
            new_style["display"] = "none"
            
        return new_style

================
File: app/callbacks/filter_update_callbacks.py
================
import json
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple
import dash
from dash import Dash, Input, Output, State
from dash.exceptions import PreventUpdate
from data_process.data_utils import category_structure_162, category_structure_158, get_categories_by_level
from config.logging_config import get_logger, track_callback, track_callback_end
from constants.filter_options import METRICS_OPTIONS
from config.default_values import DEFAULT_PRIMARY_METRICS, DEFAULT_PREMIUM_LOSS_TYPES
from app.app_layout import create_component

logger = get_logger(__name__)

# Simplified metric mappings
FORM_METRICS = {
    '0420158': {'total_premiums', 'total_losses', 'ceded_premiums', 'ceded_losses',
                'net_premiums', 'net_losses', 'ceded_premiums_ratio', 'ceded_losses_ratio'
               'ceded_losses_to_ceded_premiums_ratio', 'net_loss_ratio'
               },
    '0420162': {'total_premiums', 'total_losses', 'direct_premiums', 'direct_losses',
                'inward_premiums', 'inward_losses', 'ceded_premiums', 'ceded_losses',
                'net_premiums', 'net_losses', 'new_sums', 'sums_end', 'premiums_interm',
                'ceded_premiums_ratio', 'gross_loss_ratio', 'ceded_losses_ratio', 'new_contracts',
                'average_new_premium', 'direct_loss_ratio', 'premiums_interm_ratio', 'commissions_interm',
                'claims_settled', 'average_loss', 'inward_loss_ratio', 'ceded_losses_to_ceded_premiums_ratio',
                'net_loss_ratio', 'average_new_sum_insured', 'average_rate', 'average_sum_insured',
                'commissions_rate'
               }
}

PRIMARY_TO_SECONDARY_METRICS_MAP = {
    'total_premiums': {'total_losses', 'inward_premiums', 'direct_premiums', 
                      'ceded_premiums', 'net_premiums', 'ceded_premiums_ratio', 
                      'gross_loss_ratio'},
    'total_losses': {'direct_losses', 'inward_losses', 'ceded_losses', 
                    'net_losses', 'ceded_losses_ratio'},
    'direct_premiums': {'new_contracts', 'average_new_premium', 'direct_losses',
                       'direct_loss_ratio', 'premiums_interm', 
                       'premiums_interm_ratio', 'commissions_interm'},
    'direct_losses': {'claims_settled', 'average_loss'},
    'inward_premiums': {'inward_losses', 'inward_loss_ratio'},
    'ceded_premiums': {'ceded_losses', 'ceded_premiums_ratio', 'ceded_losses_ratio', 
                      'ceded_losses_to_ceded_premiums_ratio'},
    'ceded_losses': {'ceded_premiums', 'ceded_losses_ratio',
                    'ceded_losses_to_ceded_premiums_ratio'},
    'net_premiums': {'net_losses', 'net_loss_ratio'},
    'net_losses': {'net_premiums', 'net_loss_ratio'},
    'new_sums': {'new_contracts', 'average_new_sum_insured', 'average_rate'},
    'sums_end': {'contracts_end', 'average_sum_insured'},
    'premiums_interm': {'direct_premiums', 'commissions_interm', 'commissions_rate'}
}


@dataclass
class FilterState:
    primary_y_metric: List[str]
    secondary_y_metric: List[str]
    selected_metrics: List[str]
    premium_loss_checklist: List[str]
    selected_lines: List[str]
    show_data_table: bool = False
    clear_filters_btn: int = 0
    reporting_form: str = ''

    @staticmethod
    def normalize(value: Any) -> List[Any]:
        if not value:
            return []
        if isinstance(value, str):
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return [value]
        return [value] if not isinstance(value, list) else value


def get_premium_loss_state(metric: str, reporting_form: str) -> Tuple[bool, Optional[List[str]]]:
    if not metric:
        return True, ['direct']

    is_form_158 = reporting_form == '0420158'

    states = {
        ('total_premiums', 'total_losses'): 
            (True, ['direct', 'inward']) if is_form_158 else (False, None),
        ('ceded_premiums', 'ceded_losses', 'ceded_premiums_ratio',
         'ceded_losses_to_ceded_premiums_ratio', 'net_premiums', 'net_losses'):
            (True, ['direct', 'inward']),
        ('inward_premiums', 'inward_losses'): 
            (True, ['inward'])
    }

    for metrics, state in states.items():
        if metric in metrics:
            return state
    return True, ['direct']


def get_metric_options(reporting_form: str, primary_metric: Optional[str] = None) -> Dict[str, List[Dict[str, Any]]]:

    allowed_metrics = FORM_METRICS.get(reporting_form, set())

    primary_options = [
        opt for opt in METRICS_OPTIONS 
        if opt['value'] in PRIMARY_TO_SECONDARY_METRICS_MAP.keys() and opt['value'] in allowed_metrics
    ]

    secondary_options = []

    if primary_metric:
        # Get the intersection of secondary metrics allowed by both:
        # 1. The primary metric relationships
        # 2. The reporting form's allowed metrics
        potential_secondary = PRIMARY_TO_SECONDARY_METRICS_MAP.get(primary_metric, set())
        allowed_secondary = potential_secondary.intersection(allowed_metrics)
        secondary_options = [opt for opt in METRICS_OPTIONS if opt['value'] in allowed_secondary]

    logger.debug(f"primary_metric {primary_metric}")
    logger.debug(f"secondary_options {secondary_options}")

    return {
        'primary_y_metric_options': primary_options,
        'secondary_y_metric_options': secondary_options
    }


def setup_filter_update_callbacks(app: Dash, quarter_options_162, quarter_options_158) -> None:
    @app.callback(
        [Output('primary-y-metric', 'options'),
         Output('secondary-y-metric', 'options'),
         Output('end-quarter', 'options'),
         Output('insurance-line-dropdown', 'options'),
         Output('premium-loss-checklist-container', 'children')],
        [Input('reporting-form', 'value'),
         Input('primary-y-metric', 'value')],
         State('premium-loss-checklist', 'value'),
        prevent_initial_call=True
    )
    def update_options(reporting_form, primary_metric, current_values):
        ctx = dash.callback_context
        start_time = track_callback('app.callbacks.filter_update_callbacks', 'update_options', ctx)

        if not ctx.triggered:
            track_callback_end('app.callbacks.filter_update_callbacks', 'update_options', start_time)
            raise PreventUpdate

        try:
            metric_options = get_metric_options(reporting_form, primary_metric)

            end_quarter_options = quarter_options_162 if reporting_form == '0420162' else quarter_options_158

            insurance_line_dropdown_options = get_categories_by_level(category_structure_162 if reporting_form == '0420162' else category_structure_158, level=2, indent_char="--")

            metric = primary_metric[0] if isinstance(primary_metric, list) and primary_metric else primary_metric
            readonly, enforced_values = get_premium_loss_state(metric, reporting_form)
            values = enforced_values if enforced_values is not None else current_values
            component = create_component('checklist', id='premium-loss-checklist', readonly=readonly, value=values)

            output = metric_options['primary_y_metric_options'], metric_options['secondary_y_metric_options'], end_quarter_options, insurance_line_dropdown_options, [component]

            track_callback_end('app.callbacks.filter_update_callbacks', 'update_options', start_time, result=output)
            logger.debug(f"secondary_y_metric_options {metric_options['secondary_y_metric_options']}")

            return output

        except Exception as e:
            logger.error(f"Error in update_options: {str(e)}", exc_info=True)
            track_callback_end('app.callbacks.filter_update_callbacks', 'update_options', start_time, error=str(e))
            raise

    @app.callback(
        [Output('filter-state-store', 'data'),
         Output('primary-y-metric', 'value'),
         Output('secondary-y-metric', 'value')],
        [Input('clear-filters-button', 'n_clicks'),
         Input('primary-y-metric', 'value'),
         Input('secondary-y-metric', 'value'),
         Input('reporting-form', 'value'),
         Input('insurance-lines-state', 'data'),
         Input('premium-loss-checklist', 'value')],
        [State('show-data-table', 'data')]
    )
    def update_values(clear_btn, primary_metric, secondary_metric, 
                     reporting_form, lines, premium_loss, show_table):
        ctx = dash.callback_context
        start_time = track_callback('app.callbacks.filter_update_callbacks', 'update_values', ctx)

        if not ctx.triggered:
            track_callback_end('app.callbacks.filter_update_callbacks', 'update_values', start_time)
            raise PreventUpdate

        try:
            trigger_id = ctx.triggered[0]['prop_id'].split('.')[0]

            # Initialize state
            primary = FilterState.normalize(primary_metric)
            secondary = FilterState.normalize(secondary_metric)
            selected_metrics = primary.copy()
            if secondary:
                selected_metrics.extend(secondary)

            state = FilterState(
                primary_y_metric=primary,
                secondary_y_metric=secondary,
                selected_metrics=selected_metrics,
                premium_loss_checklist=FilterState.normalize(premium_loss),
                selected_lines=FilterState.normalize(lines),
                show_data_table=bool(show_table),
                clear_filters_btn=clear_btn or 0,
                reporting_form=reporting_form
            )

            # Update state based on trigger
            if trigger_id == 'clear-filters-button':
                state.primary_y_metric = DEFAULT_PRIMARY_METRICS
                state.secondary_y_metric = []
                state.premium_loss_checklist = DEFAULT_PREMIUM_LOSS_TYPES

            if state.secondary_y_metric:
                allowed_secondary = PRIMARY_TO_SECONDARY_METRICS_MAP.get(state.primary_y_metric[0], set())
                if state.secondary_y_metric[0] not in allowed_secondary:
                    state.secondary_y_metric = []
                    state.selected_metrics = state.primary_y_metric

            # Validate form 158 metrics
            if reporting_form == '0420158' and state.primary_y_metric:
                if state.primary_y_metric[0] not in FORM_METRICS['0420158']:
                    state.primary_y_metric = DEFAULT_PRIMARY_METRICS
                    state.secondary_y_metric = []
                    state.selected_metrics = DEFAULT_PRIMARY_METRICS

            result = (
                vars(state),
                state.primary_y_metric[0] if state.primary_y_metric else None,
                state.secondary_y_metric[0] if state.secondary_y_metric else None
            )

            track_callback_end('app.callbacks.filter_update_callbacks', 'update_values',
                             start_time, result=result)
            return result

        except Exception as e:
            logger.exception("Error in update_values")
            track_callback_end('app.callbacks.filter_update_callbacks', 'update_values', 
                             start_time, error=str(e))
            raise

================
File: app/callbacks/insurance_lines_callbacks.py
================
import dash
from dash import Input, Output, State, ALL
from dash.exceptions import PreventUpdate
import json
from typing import Dict, Any
from config.logging_config import get_logger, track_callback, track_callback_end
from config.default_values import DEFAULT_CHECKED_LINES
logger = get_logger(__name__)


def setup_insurance_lines_callbacks(app: dash.Dash, insurance_lines_tree):
    """Initialize all callbacks for the checklist component"""

    @app.callback(
        [
            Output('insurance-lines-state', 'data'),
            Output('insurance-line-dropdown', 'value')
        ],
        [
            Input({'type': 'insurance-line-checkbox', 'index': ALL}, 'value'),
            Input('insurance-line-dropdown', 'value'),
            Input('detailize-button', 'n_clicks'),
            Input('clear-filters-button', 'n_clicks'),
        ],
        [
            State({'type': 'insurance-line-checkbox', 'index': ALL}, 'id'),
            State('insurance-lines-state', 'data')
        ],
        prevent_initial_call=True
    )
    def update_selection(
            checkbox_values,
            dropdown_value,
            detailize_clicks,
            clear_filters_btn,
            checkbox_ids,
            current_state):
        ctx = dash.callback_context
        start_time = track_callback('app.checklist', 'update_selection', ctx)
        if not ctx.triggered:
            track_callback_end(
                'app.checklist',
                'update_selection',
                start_time,
                message_no_update="not ctx.triggered")
            raise PreventUpdate

        logger.debug(f"current_state state {current_state}")
        trigger = ctx.triggered[0]
        trigger_id = trigger['prop_id'].rsplit('.', 1)[0]
        # trigger_id = trigger['prop_id'].split('.')[0]

        logger.debug(f"current_state selected {current_state}")
        logger.debug(f"Trigger component: {trigger['prop_id']}")
        logger.debug(f"Trigger value: {trigger['value']}")

        # Initialize state if None with default lines
        if current_state is None:
            current_state = DEFAULT_CHECKED_LINES

        if trigger_id == 'clear-filters-button':
            final_selected = DEFAULT_CHECKED_LINES

        elif 'detailize-button' in trigger['prop_id']:  # detailize button
            logger.debug("Source: Detailize button")
            new_selected = current_state if current_state else DEFAULT_CHECKED_LINES
            final_selected = insurance_lines_tree.handle_parent_child_selections(
                new_selected, detailize=True)
        else:

            # trigger_line = json.loads(trigger['prop_id'].rsplit('.', 1)[0]).get('index')  # gets "3.1.1.1"
            # trigger_line = json.loads(trigger['prop_id'].split('.')[0]).get('index')
            if 'insurance-line-dropdown' in trigger['prop_id']:
                logger.debug(f"trigger_id {trigger_id}")
                logger.debug(f"trigger_id {trigger}")
                new_selected = [dropdown_value] if isinstance(dropdown_value, str) else (dropdown_value or DEFAULT_CHECKED_LINES)
                trigger_line = list(set(new_selected) - set(current_state))

                logger.debug(f"selecnew_selectedted {new_selected}")
                logger.debug(f"trigger_line {trigger_line}")

                final_selected = insurance_lines_tree.handle_parent_child_selections(
                    new_selected, trigger_line, detailize=False)
                # final_selected = new_selected

            elif 'insurance-line-checkbox' in trigger['prop_id']:
                logger.debug("Source: Checkbox click")
                if not any(checkbox_values):  # If all checkboxes are unchecked
                    logger.debug(f"selected {checkbox_values}")
                    new_selected = DEFAULT_CHECKED_LINES  # Keep default lines
                else:
                    new_selected = [
                        id_dict['index']
                        for value, id_dict in zip(checkbox_values, checkbox_ids)
                        if value
                    ]

                logger.debug(f"selected {new_selected}")
                logger.debug(f"trigger {trigger}")
                # prop_id = json.loads(trigger['prop_id'].split('.')[0]).get('type') if '{' in trigger['prop_id'] else trigger['prop_id'].split('.')[0]
                # prop_id = trigger['prop_id'].rsplit('.', 1)[0]# using rsplit to split from right side
                # logger.debug(f"prop_id {prop_id}")
                trigger_line = json.loads(
                    trigger['prop_id'].rsplit(
                        '.', 1)[0]).get('index')  # gets "3.1.1.1"
                logger.debug(f"trigger_line {trigger_line}")

                logger.debug(f"new_selected {new_selected}")
                logger.debug(f"current_state {current_state}")

                if trigger_line not in new_selected and trigger_line not in current_state:
                    logger.debug(f"PreventUpdate ")
                    track_callback_end(
                        'app.checklist',
                        'update_selection',
                        start_time,
                        message_no_update="trigger_line not in selected")
                    raise PreventUpdate
                trigger_line = [trigger_line]

                logger.debug(f"new_selected {new_selected}")
                logger.debug(f"trigger_line {trigger_line}")
                # logger.debug(f"detailize {detailize}")
                final_selected = insurance_lines_tree.handle_parent_child_selections(
                    new_selected, trigger_line, detailize=False)

        if not final_selected:  # If somehow we end up with empty selection
            final_selected = DEFAULT_CHECKED_LINES

        '''is_user_interaction = ('insurance-line-dropdown' in trigger['prop_id'] or 
                             'insurance-line-checkbox' in trigger['prop_id'])
        
        if not is_user_interaction and current_state == final_selected:
            track_callback_end('app.checklist', 'update_selection', start_time,
                             message_no_update="no change in selection")
            raise PreventUpdate'''
        
        # logger.debug(f"trigger {trigger}")

        # prop_id = trigger['prop_id'].split('.')[0]
        # logger.debug(f"prop_id {prop_id}")
        # trigger_line = json.loads(prop_id).get('index') if '{' in prop_id else prop_id
        # logger.debug(f"selected {selected}")

        # logger.debug(f"Pre-processed selection: {selected}")

        logger.debug(f"Final selection: {final_selected}")
        logger.debug("=" * 50 + "\n")

        # state.selected = set(final_selected)

        # result = state.to_store(), final_selected
        dropdown_value = final_selected[0] if isinstance(final_selected, list) and final_selected else final_selected

        result = final_selected, dropdown_value  # first for store, second for dropdown

        track_callback_end(
            'app.checklist',
            'update_selection',
            start_time,
            result=result)

        return result

    @app.callback(
        [
            Output('expansion-state', 'data'),
            Output('expand-all-button', 'children')
        ],
        [
            Input('expand-all-button', 'n_clicks'),
            Input({'type': 'insurance-line-expand', 'index': ALL}, 'n_clicks'),

        ],
        [
            State({'type': 'insurance-line-expand', 'index': ALL}, 'id'),
            State('expansion-state', 'data')
        ]
    )
    def update_expansion_state(
            expand_all_clicks,
            expand_clicks,
            expand_ids,
            current_expansion_state):
        ctx = dash.callback_context
        start_time = track_callback(
            'app.checklist', 'update_expansion_state', ctx)

        if not ctx.triggered:
            track_callback_end(
                'app.checklist',
                'Update_expansion_state',
                start_time,
                message_no_update="not ctx.triggered")
            raise PreventUpdate

        logger.debug(f"current_expansion_state {current_expansion_state}")
        trigger = ctx.triggered[0]
        logger.debug(f"Trigger update_expansion_state")
        trigger_id = trigger['prop_id'].split('.')[0]

        logger.debug(f"n clicks {expand_all_clicks}")
        logger.debug(f"Trigger component: {trigger['prop_id']}")
        logger.debug(f"Trigger value: {trigger['value']}")

        new_state = dict(current_expansion_state)

        if 'states' not in new_state:
            new_state['states'] = {}

        if 'expand-all-button' in trigger['prop_id']:
            logger.debug("Source: Expand/Collapse All button")
            new_state['all_expanded'] = not new_state.get(
                'all_expanded', False)
            logger.debug(
                f"Setting all_expanded to: {new_state['all_expanded']}")

            for code in insurance_lines_tree.insurance_line_structure:
                if insurance_lines_tree.get_children(code):
                    new_state['states'][code] = new_state['all_expanded']
            button_text = "Collapse All" if new_state['all_expanded'] else "Expand All"
        else:
            logger.debug("Source: Individual expand/collapse button")
            for i, n_clicks in enumerate(expand_clicks):
                if n_clicks:
                    code = expand_ids[i]['index']
                    new_state['states'][code] = not new_state['states'].get(
                        code, False)
                    logger.debug(
                        f"Toggled state for {code} to: {new_state['states'][code]}")
            button_text = "Collapse All" if new_state.get(
                'all_expanded', False) else "Expand All"

            new_lines_states = new_state['states'].keys()
            trigger_line = json.loads(
                trigger['prop_id'].rsplit(
                    '.', 1)[0]).get('index')  # gets "3.1.1.1"
            logger.debug(f"new_lines_states {list(new_lines_states)}")
            logger.debug(f"trigger_line {trigger_line}")

            if trigger_line not in new_lines_states:
                track_callback_end(
                    'app.checklist',
                    'Update_expansion_state',
                    start_time,
                    message_no_update="trigger_line not in new_lines_states")
                raise PreventUpdate

        result = new_state, button_text

        track_callback_end(
            'app.checklist',
            'update_expansion_state',
            start_time,
            result=result)

        return result

    @app.callback(
        Output("tree-container", "style"),
        [Input("collapse-button", "n_clicks")],
        [State("tree-container", "style")],
    )
    def toggle_collapse(
            n_clicks: int, current_style: Dict[str, Any]) -> Dict[str, str]:
        ctx = dash.callback_context
        start_time = track_callback(
            'app.app_layout', 'toggle_chart_config', ctx)

        try:
            if not n_clicks:
                result = {"display": "none"}
                track_callback_end(
                    'app.checklist',
                    'toggle_collapse',
                    start_time,
                    result=result)
                return result

            if current_style is None:
                current_style = {"display": "none"}

            current_display = current_style.get("display", "none")
            result = {
                **current_style,
                "display": "block" if current_display == "none" else "none"
            }

            track_callback_end(
                'app.checklist',
                'toggle_collapse',
                start_time,
                result=result)
            return result

        except Exception as e:
            track_callback_end(
                'app.checklist',
                'toggle_collapse',
                start_time,
                error=e)
            logger.exception("Error in toggle_collapse")
            raise

    @app.callback(
        Output('tree-container', 'children'),
        [
            Input('insurance-lines-state', 'data'),
            Input('expansion-state', 'data')
        ],
        [

            # State('tree-state', 'data')  # Add state to access previous tree
        ],

        prevent_initial_call=False
    )
    def update_tree(line_state, expansion_state):
        ctx = dash.callback_context
        start_time = track_callback('app.checklist', 'update_tree', ctx)

        logger.debug("\n" + "=" * 50)
        logger.debug("TREE UPDATE CALLBACK")

        # Handle initial render
        if not ctx.triggered:
            logger.debug("Initial render")
            initial_tree = insurance_lines_tree.create_tree(
                {}, set(DEFAULT_CHECKED_LINES))
            track_callback_end(
                'app.checklist',
                'update_tree',
                start_time,
                message_no_update="not ctx.triggered")
            return initial_tree

        logger.debug(f"Trigger: {ctx.triggered[0]['prop_id']}")

        # Handle missing states
        if line_state is None:
            line_state = DEFAULT_CHECKED_LINES
        if expansion_state is None:
            expansion_state = {'states': {}, 'all_expanded': False}

        selected = set(line_state)  # line_state is now just a lis

        logger.debug(f"expansion_state {expansion_state.get('states', {})}")
        logger.debug(f"line_state {line_state}")
        logger.debug(f"selected {selected}")
        tree = insurance_lines_tree.create_tree(
            expansion_state.get('states', {}), selected)
        logger.debug("Tree creation complete")
        logger.debug("=" * 50 + "\n")

        track_callback_end(
            'app.checklist',
            'update_tree',
            start_time,
            result=tree)
        logger.debug(f"tree update {tree}")

        return tree

================
File: app/callbacks/period_filter.py
================
# app/components/period_filter.py

import dash
from dash import Input, Output, State
from constants.translations import translate
from config.default_values import (
    DEFAULT_PERIOD_TYPE,
    button_period_main, button_period_main_active
)
from config.logging_config import get_logger, track_callback, track_callback_end
logger = get_logger(__name__)


def setup_period_type_callbacks(app: dash.Dash) -> None:
    """Setup callbacks for period filter"""

    @app.callback(
        [
            Output("btn-ytd", "className"),
            Output("btn-yoy-q", "className"),
            Output("btn-yoy-y", "className"),
            Output("btn-qoq", "className"),
            Output("btn-mat", "className"),
            Output('period-type', 'data'),
            Output('period-type-text', 'children')
        ],
        [
            Input("btn-ytd", "n_clicks"),
            Input("btn-yoy-q", "n_clicks"),
            Input("btn-yoy-y", "n_clicks"),
            Input("btn-qoq", "n_clicks"),
            Input("btn-mat", "n_clicks")
        ],
        [
            State('period-type', 'data')
        ]
    )
    def update_period_type(
        ytd_clicks, yoy_q_clicks, yoy_y_clicks, qoq_clicks, mat_clicks,
        current_state
    ):
        ctx = dash.callback_context
        start_time = track_callback('app.period_filter', 'update_period', ctx)

        button_map = {
            'btn-ytd': 'ytd',
            'btn-yoy-q': 'yoy_q',
            'btn-yoy-y': 'yoy_y',
            'btn-qoq': 'qoq',
            'btn-mat': 'mat'
        }

        # Initialize current state and button classes
        current_state = current_state if current_state else DEFAULT_PERIOD_TYPE
        button_classes = [button_period_main for _ in range(5)]
        active_main = current_state

        # Set initial active button class
        for i, (btn_id, btn_val) in enumerate(button_map.items()):
            if btn_val == active_main:
                button_classes[i] = button_period_main_active
                break

        # If no button clicked, return initial state
        if not ctx.triggered:
            period_type_text = translate(current_state)
            output = (*button_classes, current_state, period_type_text)
            track_callback_end('app.period_filter', 'update_period_type', start_time, result=output)
            return output

        try:
            triggered = ctx.triggered[0]["prop_id"].split(".")[0]
            logger.debug(f"update_period triggered by {triggered}")

            if triggered in button_map:
                # Reset button classes
                button_classes = [button_period_main for _ in range(5)]

                # Update state based on clicked button
                new_state = button_map[triggered]

                # Set active class for clicked button
                button_index = list(button_map.keys()).index(triggered)
                button_classes[button_index] = button_period_main_active

                period_type_text = translate(new_state)

                logger.debug(f"'date_type_state: {new_state}")

                output = (*button_classes, new_state, period_type_text)
                track_callback_end('app.period_filter', 'update_period_type', start_time, result=output)
                return output

        except Exception as e:
            logger.error(f"Error in update_period: {e}")

================
File: app/components/dash_table.py
================
# app.components.dash_table.py

from dataclasses import dataclass
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
from dash.dash_table.Format import Format, Scheme, Group
from constants.translations import translate
from constants.filter_options import METRICS
from data_process.data_utils import map_insurer
from config.logging_config import get_logger

logger = get_logger(__name__)

@dataclass(frozen=True)
class TableColors:
    PRIMARY: str = '#3C5A99'
    SECONDARY: str = '#6C757D'
    BACKGROUND: str = '#F8F9FA'
    TEXT: str = '#212529'
    SUCCESS: str = '#28a745'
    DANGER: str = '#dc3545'
    HIGHLIGHT: str = '#FFFFE0'
    SOHAGS: str = '#D4EDDA'
    QTOQ_BG: str = '#E9ECEF'
    INSURER_BG: str = '#F1F3F5'

@dataclass(frozen=True)
class TableStyles:
    FONT_FAMILY: str = 'var(--font-family-base)'
    BASE_FONT_SIZE: str = '0.8rem'
    HEADER_FONT_SIZE: str = '0.8rem'
    CELL_FONT_SIZE: str = '0.8rem'
    
    NUMBER_COL_WIDTH: str = '2rem'
    INSURER_COL_WIDTH: str = '10rem'
    DATA_COL_WIDTH: str = '5rem'
    
    CELL_PADDING: str = 'var(--spacing-md)'
    HEADER_PADDING: str = 'var(--spacing-sm) var(--spacing-lg) var(--spacing-sm) var(--spacing-sm)'

class TableStyler:
    def __init__(self):
        self.colors = TableColors()
        self.styles = TableStyles()

    def get_base_styles(self) -> Dict[str, Any]:
        return {
            'style_table': {
                'overflowX': 'auto',
                'backgroundColor': 'var(--color-bg)',
                'boxShadow': '0 0.25rem 0.375rem rgba(0, 0, 0, 0.1)',
                'borderRadius': 'var(--border-radius)',
            },
            'style_cell': {
                'textAlign': 'left',
                'padding': self.styles.CELL_PADDING,
                'border': '1px solid var(--color-border)',
                'fontSize': self.styles.CELL_FONT_SIZE,
                'fontFamily': self.styles.FONT_FAMILY,
                'color': 'var(--color-text)',
                'whiteSpace': 'normal',
                'height': 'auto',
            },
            'style_header': {
                'backgroundColor': self.colors.PRIMARY,
                'color': 'white',
                'fontWeight': 'bold',
                'textTransform': 'none',
                'borderBottom': '2px solid var(--color-border)',
                'textAlign': 'center',
                'fontSize': self.styles.HEADER_FONT_SIZE,
                'height': 'auto',
                'minHeight': '3.333rem',
                'whiteSpace': 'normal',
                'position': 'relative',
                'padding': self.styles.HEADER_PADDING,
            },
            'style_data': {
                'backgroundColor': 'var(--color-bg)',
            },
        }

    def create_conditional_styles(self, df: pd.DataFrame) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        special_insurers = {
            'Топ': {'backgroundColor': self.colors.HIGHLIGHT, 'fontWeight': 'normal'},
            'Весь рынок': {'backgroundColor': self.colors.HIGHLIGHT, 'fontWeight': 'bold'},
            'СОГАЗ': {'backgroundColor': self.colors.SOHAGS, 'fontWeight': 'normal'},
            'Газпром': {'backgroundColor': self.colors.SOHAGS, 'fontWeight': 'normal'},
            'КПСК': {'backgroundColor': self.colors.SOHAGS, 'fontWeight': 'normal'},
        }

        row_styles = [
            {'if': {'filter_query': f'{{insurer}} contains "{insurer}"'}, **style}
            for insurer, style in special_insurers.items()
        ]

        column_styles = []
        for col in df.columns:
            if any(pattern in col for pattern in ('_q_to_q_change', '_market_share_q_to_q_change')):
                column_styles.extend(self._get_change_column_styles(col))
            elif col.lower() in ['место', 'insurer']:
                column_styles.append(self._get_identifier_column_style(col))

        return column_styles, row_styles

    def _get_change_column_styles(self, col: str) -> List[Dict[str, Any]]:
        return [
            {
                'if': {'column_id': col, 'filter_query': f'{{{col}}} > 0'},
                'color': self.colors.SUCCESS,
                'fontWeight': 'bold'
            },
            {
                'if': {'column_id': col, 'filter_query': f'{{{col}}} < 0'},
                'color': self.colors.DANGER,
                'fontWeight': 'bold'
            },
            {
                'if': {'column_id': col},
                'backgroundColor': self.colors.QTOQ_BG,
            }
        ]

    def _get_identifier_column_style(self, col: str) -> Dict[str, Any]:
        return {
            'if': {'column_id': col},
            'backgroundColor': self.colors.INSURER_BG,
            'textAlign': 'left' if col.lower() == 'insurer' else 'center',
        }

    def get_column_width_styles(self) -> List[Dict[str, Any]]:
        return [
            {
                'if': {'column_id': 'Место'},
                'width': self.styles.NUMBER_COL_WIDTH,
                'minWidth': self.styles.NUMBER_COL_WIDTH,
                'maxWidth': self.styles.NUMBER_COL_WIDTH,
                'textAlign': 'center'
            },
            {
                'if': {'column_id': 'insurer'},
                'width': self.styles.INSURER_COL_WIDTH,
                'minWidth': self.styles.INSURER_COL_WIDTH,
                'maxWidth': self.styles.INSURER_COL_WIDTH,
                'textAlign': 'left'
            }
        ]

    def get_data_column_style(self, column_id: str) -> Dict[str, Any]:
        return {
            'if': {'column_id': column_id},
            'width': self.styles.DATA_COL_WIDTH,
            'minWidth': self.styles.DATA_COL_WIDTH,
            'maxWidth': self.styles.DATA_COL_WIDTH,
            'textAlign': 'center'
        }

    def get_header_styles(self) -> List[Dict[str, Any]]:
        return [
            {
                'if': {'column_id': col, 'header_index': 0},
                'borderBottom': 'none',
                'paddingBottom': '0',
            } for col in ['Место', 'insurer']
        ] + [
            {
                'if': {'column_id': col, 'header_index': 1},
                'borderTop': 'none',
                'paddingTop': '0',
                'color': 'transparent'
            } for col in ['Место', 'insurer']
        ]


class ColumnFormatter:
    @staticmethod
    def get_format(col_type: str) -> Format:
        if 'market_share' in col_type or 'q_to_q_change' in col_type:
            return Format(precision=2, scheme=Scheme.percentage)
        return Format(
            precision=3,
            scheme=Scheme.fixed,
            group=Group.yes,
            groups=3,
            group_delimiter=','
        )

    @staticmethod
    def parse_quarter(quarter_str: str, period_type: str) -> str:
        if not quarter_str or 'Q' not in quarter_str:
            return quarter_str
        try:
            year, q = quarter_str.split('Q')
            period_map = {
                'ytd': {'1': '3 мес.', '2': '6 мес.', '3': '9 мес.', '4': '12 мес.'},
                'default': {'1': '1 кв.', '2': '2 кв.', '3': '3 кв.', '4': '4 кв.'}
            }
            quarter_map = period_map.get(period_type, period_map['default'])
            return f"{quarter_map.get(q, q)} {year}"
        except Exception:
            return quarter_str

    def format_columns(self, columns: List[Dict[str, Any]], metrics: Dict[str, Dict[str, str]], period_type: str) -> List[Dict[str, Any]]:
        formatted_columns = []
        for col in columns:
            col_id = col['id']
            if col_id in ['Место', 'insurer']:
                formatted_columns.append(self._format_identifier_column(col_id))
                continue

            metric, quarter, additional_info = self._parse_column_components(col_id, metrics)
            column_name = self._get_column_name(metric, quarter, additional_info, metrics, period_type)

            formatted_columns.append({
                'id': col_id,
                'type': 'numeric',
                'format': self.get_format(additional_info),
                'name': column_name
            })
        return formatted_columns

    def _parse_column_components(self, col_id: str, metrics: Dict[str, Dict[str, str]]) -> Tuple[str, str, str]:
        metric = next((m for m in sorted(metrics.keys(), key=len, reverse=True) 
                       if col_id.startswith(m)), None)
        if metric:
            remaining = col_id[len(metric)+1:].split('_')
            quarter = remaining[0] if remaining else ""
            additional_info = '_'.join(remaining[1:]) if len(remaining) > 1 else ""
        else:
            parts = col_id.split('_')
            metric, quarter, *additional_parts = parts + ["", ""]
            additional_info = '_'.join(additional_parts)
        return metric, quarter, additional_info

    def _format_identifier_column(self, col_id: str) -> Dict[str, Any]:
        translated_col = translate(col_id)
        return {
            'name': [translated_col, translated_col],
            'id': col_id
        }

    def _get_column_name(self, metric: str, quarter: str, additional_info: str, 
                        metrics: Dict[str, Dict[str, str]], period_type: str) -> List[str]:
        translated_metric = translate(metrics.get(metric, {}).get('label', metric))
        if 'market_share_q_to_q_change' in additional_info:
            return [f"{translated_metric}, {translate('market_share')}", translate('q_to_q_change')]
        elif 'market_share' in additional_info:
            return [f"{translated_metric}, {translate('market_share')}", self.parse_quarter(quarter, period_type)]
        elif 'q_to_q_change' in additional_info:
            return [f"{translated_metric}, {translate('млрд руб.')}", translate('q_to_q_change')]
        else:
            return [f"{translated_metric}, {translate('млрд руб.')}", self.parse_quarter(quarter, period_type)]

def prepare_dash_table_data(df: pd.DataFrame, period_type: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    table_columns = [{"name": col, "id": col} for col in df.columns]
    table_data = df.assign(insurer=lambda x: x['insurer'].map(map_insurer)).to_dict('records')

    formatter = ColumnFormatter()
    styler = TableStyler()

    table_columns = formatter.format_columns(table_columns, METRICS, period_type)
    column_styles, row_styles = styler.create_conditional_styles(df)

    return table_columns, table_data, (column_styles + row_styles)

def generate_dash_table_config(
    df: pd.DataFrame,
    period_type: str,
    columns_config: Optional[Dict[str, str]] = None,
    toggle_selected_market_share: Optional[List[str]] = None,
    toggle_selected_qtoq: Optional[List[str]] = None,
) -> Dict[str, Any]:
    show_selected_market_share = toggle_selected_market_share and "show" in toggle_selected_market_share
    show_selected_qtoq = toggle_selected_qtoq and "show" in toggle_selected_qtoq

    # Prepare columns & data
    table_columns, table_data, style_data_conditional = prepare_dash_table_data(df, period_type)
    styler = TableStyler()

    # Use hidden_columns for visibility control
    visible_columns = ['Место', 'insurer']
    hidden_columns = [
        col['id'] for col in table_columns
        if col['id'] not in visible_columns and (
            ('_market_share' in col['id'] and not show_selected_market_share) or
            ('_q_to_q_change' in col['id'] and not show_selected_qtoq)
        )
    ]

    # Make all columns non-hideable and non-toggleable
    enforced_columns = [
        {
            **col_def,
            "hideable": False,
            "selectable": False,
            "deletable": False,
            "renamable": False
        } for col_def in table_columns
    ]

    # Get base styles from the styler instance
    base_styles = styler.get_base_styles()
    
    # Updated header styles with specific alignments for N and insurer
    style_header_conditional = [
        # N column - bottom center alignment
        {
            'if': {'column_id': 'Место', 'header_index': 0},
            'textAlign': 'center',
            'verticalAlign': 'bottom',
            'borderBottom': 'none',
            'paddingBottom': 'var(--spacing-sm)',
        },
        {
            'if': {'column_id': 'Место', 'header_index': 1},
            'borderTop': 'none',
            'paddingTop': '0',
            'color': 'transparent',
            'textAlign': 'center',
        },
        # Insurer column - bottom left alignment
        {
            'if': {'column_id': 'insurer', 'header_index': 0},
            'textAlign': 'left',
            'verticalAlign': 'bottom',
            'borderBottom': 'none',
            'paddingBottom': 'var(--spacing-sm)',
        },
        {
            'if': {'column_id': 'insurer', 'header_index': 1},
            'borderTop': 'none',
            'paddingTop': '0',
            'color': 'transparent',
            'textAlign': 'left',
        }
    ]
    
    # Style conditionals without column visibility CSS
    style_cell_conditional = styler.get_column_width_styles() + [
        styler.get_data_column_style(col['id'])
        for col in enforced_columns if col['id'] not in visible_columns
    ]

    # Enhanced CSS rules with more specificity and !important flags
    css_rules = [
        # Target the toggle button container and all its variations
        {
            'selector': '''
                .dash-table-container .dash-spreadsheet-container .dash-spreadsheet-inner 
                    .dash-fixed-content .dash-fixed-row .dash-fixed-row-info .dash-table-menu__toggle,
                .dash-spreadsheet-inner th .column-header--toggle,
                th .column-header--toggle,
                .dash-table-menu__toggle,
                .dash-table-menu__toggle--open,
                .dash-table-menu__toggle--closed,
                .dash-spreadsheet-menu,
                .dash-spreadsheet-menu *
            ''',
            'rule': '''
                display: none !important;
                width: 0 !important;
                height: 0 !important;
                opacity: 0 !important;
                pointer-events: none !important;
                position: absolute !important;
                visibility: hidden !important;
                margin: 0 !important;
                padding: 0 !important;
                border: none !important;
                clip: rect(0 0 0 0) !important;
                clip-path: inset(50%) !important;
            '''
        },
        # Target menu items and dropdowns
        {
            'selector': '''
                .dash-table-menu,
                .dash-menu-item,
                .dash-menu-item--show-hide-toggle-columns,
                .dash-menu-item--hide-columns,
                .show-hide-toggle-columns,
                .dash-table-menu__dropdown,
                .dash-table-menu__dropdown--content,
                [class*="dash-menu-item"]
            ''',
            'rule': '''
                display: none !important;
                width: 0 !important;
                height: 0 !important;
                opacity: 0 !important;
                pointer-events: none !important;
                position: absolute !important;
                visibility: hidden !important;
            '''
        },
        # Hide sort icons
        {
            'selector': '''
                .dash-header-cell::after,
                .dash-header-cell--sort-asc::after,
                .dash-header-cell--sort-desc::after
            ''',
            'rule': '''
                display: none !important;
                content: none !important;
            '''
        },
        # Header alignment specificity
        {
            'selector': '.dash-header[data-dash-column="N"]',
            'rule': '''
                text-align: center !important;
                vertical-align: bottom !important;
            '''
        },
        {
            'selector': '.dash-header[data-dash-column="insurer"]',
            'rule': '''
                text-align: left !important;
                vertical-align: bottom !important;
            '''
        },
        # Remove any menu-related margins/padding
        {
            'selector': '.dash-spreadsheet th',
            'rule': '''
                padding-right: var(--spacing-md) !important;
            '''
        }
    ]

    # Return table configuration with modified settings
    return {
        **base_styles,
        'columns': enforced_columns,
        'data': table_data,
        'hidden_columns': hidden_columns,
        'style_header_conditional': style_header_conditional,
        'style_cell_conditional': style_cell_conditional,
        'style_data_conditional': style_data_conditional,
        'sort_action': 'none',
        'sort_mode': None,
        'filter_action': 'none',
        'merge_duplicate_headers': True,
        'sort_as_null': ['', 'No answer', 'No Answer', 'N/A', 'NA'],
        'column_selectable': False,
        'row_selectable': False,
        'cell_selectable': False,
        'page_action': 'none',
        'css': css_rules,
        'style_table': {
            'overflowX': 'auto',
            'minWidth': '100%'
        },
        'editable': False,
        'dropdown': {},
        'tooltip_conditional': [],
        'tooltip_data': [],
        'tooltip_header': {}
    }

================
File: app/components/insurance_lines_tree.py
================
import functools
import dash_bootstrap_components as dbc
import dash
from dash import dcc, html
from typing import Dict, List, Set, Optional, TypedDict
from config.logging_config import get_logger
from config.default_values import DEFAULT_CHECKED_LINES
from data_process.data_utils import category_structure_162, get_categories_by_level

logger = get_logger(__name__)

insurance_line_structure = category_structure_162

class InsuranceLineDetails(TypedDict):
    label: str
    children: Optional[List[str]]

DEFAULT_SINGLE_LINE = DEFAULT_CHECKED_LINES[0] if isinstance(DEFAULT_CHECKED_LINES, list) else DEFAULT_CHECKED_LINES

InsuranceLineStructure = Dict[str, InsuranceLineDetails]

initial_state = list(DEFAULT_CHECKED_LINES)


class TreeItem(html.Div):
    def __init__(
        self,
        code: str,
        label: str,
        level: int,
        is_selected: bool,
        is_expanded: bool,
        has_children: bool,
        has_selected_descendants: bool
    ):
        logger.debug(f"Creating TreeItem: {code}, level: {level}")

        container_children = []

        # Always add either an expand button or a placeholder div
        if has_children:
            expand_button = html.Button(
                "▾" if is_expanded else "▸",
                id={'type': 'category-expand', 'index': code},
                className="expand-button me-2",
                n_clicks=None
            )
            container_children.append(expand_button)
        else:
            # Add placeholder div with same width as expand button
            container_children.append(
                html.Div(className="expand-button-placeholder me-2")
            )

        checkbox_classes = ["insurance-line-checkbox"]
        if has_selected_descendants:
            checkbox_classes.append("parent-of-selected")
        if is_selected:
            checkbox_classes.append("selected")

        label_classes = ["ms-2", "insurance-line-label"]

        checkbox = html.Div([
            dbc.Checkbox(
                id={'type': 'insurance-line-checkbox', 'index': code},
                value=is_selected,
                className=" ".join(checkbox_classes)
            ),
            html.Span(
                label,
                className=" ".join(label_classes)
            )
        ], className="d-flex align-items-center")

        container_children.append(checkbox)

        super().__init__(
            container_children,
            className=f"tree-item level-{level} {'has-selected' if has_selected_descendants else ''}",
            style={'margin-left': f'{level * 20}px'},
            id={'type': 'tree-item', 'index': code}
        )


class InsuranceLinesTree:
    def __init__(
        self,
        insurance_line_structure: InsuranceLineStructure,
        initial_state: Optional[Dict] = None
    ):
        logger.debug("Initializing InsuranceLinesTree")
        self.insurance_line_structure = insurance_line_structure
        self.state = initial_state or []
        self._initialize_cache()

    def _initialize_cache(self) -> None:
        self._descendants_cache = {}
        self._ancestors_cache = {}
        self._children_cache = {}

    @functools.lru_cache(maxsize=128)
    def get_children(self, parent: str) -> List[str]:
        details = self.insurance_line_structure.get(parent, {})
        return details.get('children', []) or []

    def get_descendants(self, category: str) -> Set[str]:
        if category not in self._descendants_cache:
            descendants = set()
            children = self.get_children(category)

            for child in children:
                descendants.add(child)
                descendants.update(self.get_descendants(child))

            self._descendants_cache[category] = descendants
        return self._descendants_cache[category]

    def get_ancestors(self, category: str) -> Set[str]:
        if category not in self._ancestors_cache:
            ancestors = set()
            for code, details in self.insurance_line_structure.items():
                children = details.get('children', [])
                if children and category in children:
                    ancestors.add(code)
                    ancestors.update(self.get_ancestors(code))
            self._ancestors_cache[category] = ancestors
        return self._ancestors_cache[category]

    def handle_parent_child_selections(
        self, selected_lines: List[str], 
        trigger_line: List[str] = None, 
        detailize: bool = False
    ) -> List[str]:
        # logger.error(f"trigger_line {trigger_line}")
        if not detailize:
            new_selected = set(selected_lines)
            for category in trigger_line:
                descendants = self.get_descendants(category)
                logger.info(f"descendants {descendants}")
                new_selected.difference_update(descendants)
                ancestors = self.get_ancestors(category)
                logger.info(f"ancestors {ancestors}")
                new_selected.difference_update(ancestors)

        else:
            new_selected = set()
            for category in selected_lines:
                children = self.get_children(category)
                if children:
                    new_selected.update(children)
                else:
                    new_selected.add(category)
        return list(new_selected)

    def create_tree(self, expansion_state: Dict[str, bool], selected: Set[str]) -> html.Div:
        logger.debug("Creating tree")
        logger.debug(f"Expansion state: {expansion_state}")
        logger.debug(f"Selected: {selected}")

        # Get all ancestors of selected items firs
        ancestors_of_selected = set()
        for item in selected:
            ancestors_of_selected.update(self.get_ancestors(item))

        def create_subtree(code: str, level: int = 0) -> Optional[List[html.Div]]:
            if code not in self.insurance_line_structure:
                return None

            details = self.insurance_line_structure[code]
            children = self.get_children(code)

            # Check if this node or any descendants are selected
            descendants = self.get_descendants(code)
            has_selected_descendants = bool(descendants & selected)
            is_ancestor_of_selected = code in ancestors_of_selected

            # Determine expansion state - expand if it's in expansion_state OR if it's an ancestor of selected
            is_expanded = expansion_state.get(code, False) if code in expansion_state else is_ancestor_of_selected

            tree_item = TreeItem(
                code=code,
                label=details['label'],
                level=level,
                is_selected=code in selected,
                is_expanded=is_expanded,
                has_children=bool(children),
                has_selected_descendants=has_selected_descendants or is_ancestor_of_selected
            )

            items = [tree_item]

            # If has children and is expanded, create child items
            if children and is_expanded:
                child_items = []
                for child in children:
                    child_subtree = create_subtree(child, level + 1)
                    if child_subtree:
                        child_items.extend(child_subtree)

                if child_items:
                    collapse = dbc.Collapse(
                        html.Div(child_items, className="tree-children"),
                        id={'type': 'category-collapse', 'index': code},
                        is_open=True  # Always open for ancestors of selected
                    )
                    items.append(collapse)

            return items

        # Get top-level categories
        top_level = [
            code for code in self.insurance_line_structure.keys()
            if not self.get_ancestors(code)
        ]

        all_items = []
        for code in top_level:
            items = create_subtree(code)
            if items:
                all_items.extend(items)

        return html.Div(all_items, className="category-tree")


insurance_lines_tree = InsuranceLinesTree(insurance_line_structure, initial_state)


def get_insurance_lines_tree_components(container_class=None, label_class=None, button_class=None, dropdown_col_class=None, dropdown_class=None):
    """Create insurance lines tree components"""
    dropdown = html.Div([
        dbc.Row([
            dbc.Col([
                html.Label("Линия:", className=label_class),
            ], width=3, className=dropdown_col_class),
            dbc.Col([
                dcc.Dropdown(
                    id='insurance-line-dropdown',
                    options=get_categories_by_level(category_structure_162, level=2),
                    value=DEFAULT_SINGLE_LINE,
                    multi=False,
                    clearable=False
                ),
            ], width=9),            
        ], className="mb-0 g-0")
    ])

    toggle_button = dbc.Row([
        dbc.Col(
            dbc.Button(
                "Показать все",
                id='expand-all-button',
                size="sm",
                className=button_class,
                style={"display": "none"}
            ),
            className="pe-1"
        ),
    ], className="g-0")

    components = [
        toggle_button,
        dcc.Store(id='insurance-lines-state', data=initial_state),
        dcc.Store(id='expansion-state', data={'states': {}, 'all_expanded': False}),
        dcc.Store(id='tree-state', data={'states': {}, 'all_expanded': False}),
        html.Div([
            dropdown,
            html.Div(id="tree-container")
        ], className=container_class)
    ]

    return components


def create_tree_control_buttons():
    return html.Div([
        dbc.Row([
            dbc.ButtonGroup([
                dbc.Button(
                    "Показать иерархию",
                    id="collapse-button",
                    size="sm",
                    className="py-0 btn-secondary-custom",
                    style={"height": "26px"},
                    n_clicks=0
                ),
                dbc.Button(
                    "Drill down",
                    id="detailize-button",
                    size="sm",
                    className="py-0 ms-1 btn-secondary-custom",
                    style={"height": "26px"},

                )
            ])
        ], className="mb-2")
    ])


def create_insurance_lines_tree_app(
    insurance_line_structure: InsuranceLineStructure,
    default_categories: Optional[List[str]] = None
) -> dash.Dash:

    app = dash.Dash(
        __name__,  # Corrected from **name** to __name__
        external_stylesheets=[dbc.themes.BOOTSTRAP],
        suppress_callback_exceptions=True
    )

    app.layout = dbc.Container([
        *get_insurance_lines_tree_components(insurance_line_structure, initial_state),
    ], fluid=True)

    setup_insurance_lines_callbacks(app)
    return app


if __name__ == '__main__':
    # Configure logging at the start of the application
    logger.basicConfig(
        level=logger.debug,  # Use INFO for important events, can change to DEBUG for more detail
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )

    # Example category structure and default categories remain the same
    '''insurance_line_structure = {
        'A': {'label': 'Category A', 'children': ['A1', 'A2']},
        'A1': {'label': 'Subcategory A1', 'children': None},
        'A2': {'label': 'Subcategory A2', 'children': None},
        'B': {'label': 'Category B', 'children': ['B1', 'B2']},
        'B1': {'label': 'Subcategory B1', 'children': None},
        'B2': {'label': 'Subcategory B2', 'children': None},
    }

    DEFAULT_CHECKED_LINES = ['A1', 'B1']'''

    logger.debug("Starting application...")
    app = create_insurance_lines_tree_app(
        insurance_line_structure=insurance_line_structure,
        default_categories=DEFAULT_CHECKED_LINES
    )

    logger.debug("Running server...")
    app.run_server(debug=True)

================
File: app/__init__.py
================
# app.__init__

from config.default_values import (
    DEFAULT_PREMIUM_LOSS_TYPES,
    DEFAULT_SECONDARY_METRICS,
    DEFAULT_CHECKED_LINES,
    DEFAULT_PREMIUM_LOSS_TYPES, DEFAULT_PRIMARY_METRICS,
    DEFAULT_PERIOD_TYPE, DEFAULT_NUMBER_OF_PERIODS
)

from config.main_config import (
    APP_TITLE, DEBUG_MODE, PORT, DATA_FILE_162, DATA_FILE_158,
    DATA_FILE_REINSURANCE,
    LINES_162_DICTIONARY, INSURERS_DICTIONARY, LINES_158_DICTIONARY,
    DASH_CONFIG,
    DATA_FILE_158
)

from constants.translations import translate, translate_quarter
from constants.filter_options import (
    BASE_METRICS, CALCULATED_METRICS, CALCULATED_RATIOS,
    PREMIUM_LOSS_OPTIONS,
    METRICS_OPTIONS,
    METRICS,
    VALUE_METRICS_OPTIONS, AVERAGE_VALUE_METRICS_OPTIONS,
    RATIO_METRICS_OPTIONS
)

from .app_layout import create_app_layout
from app.components.insurance_lines_tree import InsuranceLinesTree
from app.components.dash_table import generate_dash_table_config
from app.callbacks.period_filter import setup_period_type_callbacks
from app.callbacks.app_layout_callbacks import setup_tab_state_callbacks
from app.callbacks.insurance_lines_callbacks import setup_insurance_lines_callbacks
from app.callbacks.filter_update_callbacks import setup_filter_update_callbacks

from data_process.data_utils import (
    create_year_quarter_options, load_and_preprocess_data,
    save_df_to_csv, log_dataframe_info, print_dataframe_info,
    load_json, log_chart_structure, process_inputs, map_line, map_insurer
)
from data_process.process_filters import MetricsProcessor
from data_process.table_data import get_data_table

================
File: app/app_layout.py
================
import dash_bootstrap_components as dbc
from dash import dcc, html
from app.components.insurance_lines_tree import initial_state
from constants.translations import translate
from data_process.data_utils import category_structure_162, category_structure_158, get_categories_by_level
from constants.filter_options import VALUE_METRICS_OPTIONS, REPORTING_FORM_OPTIONS, PREMIUM_LOSS_OPTIONS
from config.default_values import (
    DEFAULT_PRIMARY_METRICS, DEFAULT_CHECKED_LINES, DEFAULT_END_QUARTER,
    DEFAULT_REPORTING_FORM, DEFAULT_PREMIUM_LOSS_TYPES, DEFAULT_PERIOD_TYPE,
    button_period_main
)
from config.logging_config import get_logger

logger = get_logger(__name__)

###############################################################################
# 1) CONFIG & CONSTANTS
###############################################################################
APP_CONFIG = {
    'dropdowns': {
        'insurance-line-dropdown': {
            'options': get_categories_by_level(category_structure_162 if DEFAULT_REPORTING_FORM == '0420162' else category_structure_158, level=2, indent_char="--"),
            'value': DEFAULT_CHECKED_LINES[0] if isinstance(DEFAULT_CHECKED_LINES, list) else DEFAULT_CHECKED_LINES,
            'placeholder': "Select a category..."
        },
        'primary-y-metric': {
            'options': VALUE_METRICS_OPTIONS,
            'value': DEFAULT_PRIMARY_METRICS[0] if isinstance(DEFAULT_PRIMARY_METRICS, list) else DEFAULT_PRIMARY_METRICS,
            'placeholder': "Select primary metric"
        },
        'secondary-y-metric': {
            'options': [],
            'value': None,
            'placeholder': "Доп. показтель...",
            'multi': False
        },
        'end-quarter': {
            'options': [],
            'value': DEFAULT_END_QUARTER,
            'placeholder': "Select quarter...",
            'clearable': False
        },
        'reporting-form': {
            'options': REPORTING_FORM_OPTIONS,
            'value': DEFAULT_REPORTING_FORM,
            'placeholder': "Select reporting form...",
            'clearable': False
        }
    },
    'checklists': {
        'premium-loss-checklist': {
            'options': PREMIUM_LOSS_OPTIONS,
            'value': DEFAULT_PREMIUM_LOSS_TYPES,
            'switch': True,
            'inline': True
        },
        'toggle-selected-market-share': {
            'options': [{"label": "", "value": "show"}],
            'value': [],
            'switch': True,
            'inline': True
        },
        'toggle-selected-qtoq': {
            'options': [{"label": "", "value": "show"}],
            'value': [],
            'switch': True,
            'inline': True
        }
    },
    'inputs': {
        'number-of-insurers': {'type': 'number', 'min': 1, 'max': 999, 'step': 1, 'value': 10},
        'number-of-periods-data-table': {'type': 'number', 'min': 1, 'max': 999, 'step': 1, 'value': 2}
    }
}

###############################################################################
# 2) FACTORY FUNCTION: CREATE_COMPONENT
###############################################################################
def create_component(component_type, id=None, **kwargs):
    component_kwargs = {
        k: v for k, v in kwargs.items()
        if k not in ["label_width", "component_width"]
    }

    if component_type == "dropdown":
        config = APP_CONFIG["dropdowns"].get(id, {})
        return dcc.Dropdown(
            id=id,
            options=config.get("options", []),
            value=config.get("value"),
            multi=False,
            placeholder=translate(config.get("placeholder", "")),
            clearable=config.get("clearable", True),
            className="dash-dropdown"
        )

    elif component_type == "button":
        return dbc.Button(
            component_kwargs.get("text", ""),
            id=id,
            size=component_kwargs.get("size", "sm"),
            className=component_kwargs.get("className", ""),
            color=component_kwargs.get("color", "primary"),
            style={"fontSize": "0.85rem", "padding": "0.3rem 0.6rem"}
        )

    elif component_type == "checklist":
        config = APP_CONFIG["checklists"].get(id, {}).copy()
        config.update({
            k: v for k, v in component_kwargs.items()
            if k in ["options", "value", "switch", "inline", "readonly"]
        })
        style = {"margin": 0, "fontSize": "0.85rem"}
        if config.get("readonly"):
            style.update({"pointerEvents": "none", "opacity": 0.5})
        return dbc.Checklist(
            id=id,
            options=config.get("options", []),
            value=config.get("value", []),
            switch=config.get("switch", False),
            inline=config.get("inline", False),
            style=style
        )

    elif component_type == "input":
        config = APP_CONFIG["inputs"].get(id, {})
        return dcc.Input(
            id=id,
            type=config.get("type", "text"),
            min=config.get("min"),
            max=config.get("max"),
            step=config.get("step"),
            value=config.get("value"),
            className="form-control input-short",  # see CSS below
            style={"fontSize": "0.85rem"}
        )

    elif component_type == "label":
        return html.Label(
            component_kwargs.get("text", ""),
            className="filter-label"
        )

###############################################################################
# 3) HELPER FUNCTION: CREATE_FILTER_ROW
###############################################################################
def create_filter_row(label_text, component_id, component_type="dropdown", vertical=False, **kwargs):
    """
    Create a filter row with option for vertical layout
    
    Args:
        label_text (str): The label text
        component_id (str): The component identifier
        component_type (str): The type of component
        vertical (bool): If True, stack label above component
        **kwargs: Additional arguments passed to create_component
    """
    # Build the main component
    comp = create_component(component_type, id=component_id, **kwargs)

    # Wrap premium-loss-checklist if needed
    if component_id == "premium-loss-checklist":
        comp = html.Div(
            id="premium-loss-checklist-container",
            children=[comp]
        )

    if vertical:
        return html.Div([
            html.Label(label_text, className="filter-label d-block mb-2"),
            html.Div(comp, className="w-100")
        ], className="mb-3")
    else:
        # Original horizontal layout
        label_width = kwargs.get("label_width", 6)
        component_width = kwargs.get("component_width", 6)

        extra_class = "checklist-row" if component_type == "checklist" else ""
        
        return dbc.Row(
            [
                dbc.Col(
                    html.Label(label_text, className="filter-label"),
                    width=label_width
                ),
                dbc.Col(comp, width=component_width, className=f"{extra_class}-content")
            ],
            className="mb-1 filter-row"
        )
    
###############################################################################
# 4) MAIN LAYOUT: SIDEBAR + MAIN CONTENT (NO OVERLAP)
###############################################################################


def create_app_layout(initial_quarter_options=None):
    """
    Reorganized layout with all required components
    """
    try:

        APP_CONFIG['dropdowns']['end-quarter']['options'] = [
            {'label': q, 'value': q} for q in [q['value'] for q in initial_quarter_options]
        ]

        # Stores remain unchanged
        stores = [
            html.Div(id="_hidden-init-trigger", style={"display": "none"}),
            dcc.Store(id="show-data-table", data=True),
            dcc.Store(id="processed-data-store"),
            dcc.Store(id="filter-state-store"),
            dcc.Store(id='insurance-lines-state', data=initial_state),
            dcc.Store(id='expansion-state', data={'states': {}, 'all_expanded': False}),
            dcc.Store(id='tree-state', data={'states': {}, 'all_expanded': False}),
            dcc.Store(id='period-type', data=DEFAULT_PERIOD_TYPE)
        ]

        # Navbar definition
        navbar = dbc.Navbar(
            [
                dbc.Container(
                    fluid=True,
                    children=[
                        dbc.Button(
                            "Show Filters",
                            id="toggle-sidebar-button",
                            color="secondary",
                            className="ms-3 toggle-sidebar-btn"
                        ),                        
                        dbc.NavbarToggler(id="navbar-toggler", n_clicks=0),
                        dbc.Button("Data Table", id="data-table-tab", color="light", className="ms-3")
                    ]
                )
            ],
            color="dark",
            dark=True,
            className="main-navbar"
        )

        # Hierarchy buttons definition
        hierarchy_buttons = dbc.Row(
            [
                dbc.Col([
                    dbc.Button("Показать все", id="expand-all-button",
                               style={"display": "none"}, color="secondary"),
                    dbc.Button("Показать иерархию", id="collapse-button",
                               style={"display": "none"}, color="info", className="ms-1"),
                    dbc.Button("Drill down", id="detailize-button",
                               style={"display": "none"}, color="success", className="ms-1")
                ])
            ],
            className="mb-3"
        )

        period_type_buttonsgroup = html.Div([
            dbc.Row([
                dbc.ButtonGroup([
                    dbc.Button(
                        label,
                        id=f"btn-{value}",
                        size="sm",
                        className=button_period_main,
                        style={"height": "28px"}
                    )
                    for label, value in [
                        ("YTD", "ytd"),
                        ("YoY-Q", "yoy-q"),
                        ("YoY-Y", "yoy-y"),
                        ("QoQ", "qoq"),
                        ("MAT", "mat")
                    ]
                ]),
            ], className="mb-0"),
        ])


        # Sidebar content
        sidebar = dbc.CardBody(
            id="sidebar-filters",
            className="sidebar-filters p-3",
            children=[
                create_filter_row("Форма отчетности:", "reporting-form"),
                create_filter_row("Отчетный квартал:", "end-quarter"),
                html.Label("Тип данных:", className="filter-label mb-2"),
                period_type_buttonsgroup,
                html.Div(id="period-type-text", className="period-type-text mb-3"),
                create_filter_row("Кол-во периодов для сравнения:", "number-of-periods-data-table", component_type="input", label_width=9, component_width=3),
                create_filter_row("Линия:", "insurance-line-dropdown", vertical=True),
                create_filter_row("Основной показатель:", "primary-y-metric", vertical=True),
                create_filter_row("Бизнес:", "premium-loss-checklist", component_type="checklist", label_width=4, component_width=8),
                create_filter_row("Доп. показатель:", "secondary-y-metric", vertical=True),
                create_filter_row("Показать долю рынка:", "toggle-selected-market-share", component_type="checklist", label_width=10, component_width=2),
                create_filter_row("Показать динамику:", "toggle-selected-qtoq", component_type="checklist", label_width=10, component_width=2),
                create_filter_row("Кол-во страховщиков:", "number-of-insurers", component_type="input", label_width=9, component_width=3),                
                dbc.Button(
                    "Clear Filters",
                    id="clear-filters-button",
                    color="warning",
                    className="mt-3 mb-2 w-100",
                    style={"fontSize": "0.85rem", "padding": "0.3rem 0.6rem"}
                )
            ]
        )

        # Main content area
        main_content = html.Div(
            id="main-content",
            className="main-content p-3",
            children=[
                dbc.Card([
                    dbc.CardBody([
                        html.H4(id="table-title", className="table-title"),
                        html.H4(id="table-subtitle", className="table-subtitle mb-3"),
                        dcc.Loading(
                            id="loading-data-table",
                            type="default",
                            children=[
                                # Add data-table-wrapper class to ensure styles are properly scoped
                                html.Div(
                                    id="data-table",
                                    className="data-table-wrapper"
                                )
                            ]
                        )
                    ])
                ], className="mb-3"),
                html.Div(
                    "Chart(s) / Additional Visuals Go Here", 
                    className="placeholder-charts mb-3"
                ),
                html.Div(
                    id="tree-container", 
                    className="tree-container"
                ),
                hierarchy_buttons
            ]
        )

        # Debug footer
        debug_footer = html.Div(
            id="debug-footer",
            className="debug-footer p-3",
            children=[
                dbc.Button("Toggle Debug Logs", id="debug-toggle", color="secondary", className="btn-debug-toggle mb-2"),
                dbc.Collapse(
                    dbc.Card(
                        dbc.CardBody([
                            html.H4("Debug Logs", className="debug-title"),
                            html.Pre(id="debug-output", className="debug-output")
                        ]),
                        className="debug-card"
                    ),
                    id="debug-collapse",
                    is_open=False
                ),
            ]
        )

        # Final layout assembly
        layout = html.Div(
            id="app-container",
            className="app-container",
            children=[
                *stores,
                navbar,
                html.Div(
                    className="layout-wrapper p-3",
                    children=[
                        dbc.Row(
                            [
                                dbc.Col(
                                    sidebar,
                                    id="sidebar-col",
                                    className="sidebar-col",
                                    width="auto"
                                ),
                                dbc.Col(
                                    main_content,
                                    id="main-content-col",
                                    className="main-col",
                                    width=True
                                )
                            ],
                            className="gx-0 h-100"
                        ),
                    ]
                ),
                debug_footer
            ]
        )

        return layout

    except Exception as e:
        print(f"Error in create_app_layout: {str(e)}")
        raise

================
File: config/default_values.py
================
DEFAULT_PERIOD_TYPE = 'ytd'
DEFAULT_START_QUARTER = '2022Q1'
DEFAULT_END_QUARTER = '2024Q3'
DEFAULT_NUMBER_OF_PERIODS = 5
DEFAULT_REPORTING_FORM = '0420162'

# default_values.py
DEFAULT_CHECKED_LINES = ["все линии"]
DEFAULT_PRIMARY_METRICS = ['total_premiums']
DEFAULT_PRIMARY_METRICS_INWARD = ['inward_premiums']
DEFAULT_SECONDARY_METRICS = []

DEFAULT_PREMIUM_LOSS_TYPES = ['direct', 'inward']

# Main period selector buttons (YTD, YoY, QoQ, etc.)
button_period_main = "py-0 period-main-btn"
button_period_main_active = "py-0 period-main-btn-active"

# Sub period selector buttons (3M, 1H, etc.)
button_period_sub = "py-0 period-sub-btn"
button_period_sub_active = "py-0 period-sub-btn-active"

# Range selector buttons (1Y, 2Y, etc.)
button_range_main = "py-0 range-main-btn"
button_range_main_active = "py-0 range-main-btn-active"

# Custom range button
button_range_custom = "py-0 range-custom-btn"
button_range_custom_active = "py-0 range-custom-btn-active"


button_toggle_chart = 'py-0 w-100 toggle-btn'
button_toggle_chart_active = 'py-0 w-100 toggle-btn-active'

INSURANCE_TAB = "insurance-tab"
REINSURANCE_TAB = "reinsurance-tab"
DATA_TABLE_TAB = "data-table-tab"
ACTIVE_CLASS = "me-1 tab-like-button active"
INACTIVE_CLASS = "tab-like-button"

================
File: config/logging_config.py
================
from __future__ import annotations

import logging
import os
import time
from dataclasses import dataclass
from enum import IntEnum
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Dict, Optional, Union, List, Callable, Any

import psutil
from colorama import Fore, Back, Style, init
# from memory_profiler import memory_usage
from functools import wraps

# Initialize colorama
init(autoreset=True)

class LogLevel(IntEnum):
    """Enumeration for debug levels with backwards compatibility"""
    NONE = 0
    BASIC = 1
    VERBOSE = 2

# Maintain backwards compatibility
DebugLevels = type('DebugLevels', (), {level.name: level.value for level in LogLevel})
debug_level = LogLevel.NONE

class ColoredFormatter(logging.Formatter):
    """Custom formatter for colored console output with status-specific colors"""
    COLORS = {
        'DEBUG': Fore.BLUE,
        'INFO': Fore.WHITE,
        'WARNING': Fore.YELLOW,
        'ERROR': Fore.RED,
        'CRITICAL': Fore.RED + Back.WHITE
    }
    
    # Define colors for different callback statuses
    STATUS_COLORS = {
        'Completed': Fore.GREEN,  # Light green for completed
        'Prevented': Fore.LIGHTBLUE_EX,   # Light blue for prevented
        'Error': Fore.RED                 # Red for errors
    }

    def __init__(self, fmt: str = None, callback_color: bool = False):
        super().__init__(fmt)
        self.callback_color = callback_color

    def format(self, record: logging.LogRecord) -> str:
        formatted_message = super().format(record)
        
        if self.callback_color and record.name == 'callbacks':
            # Split the message into parts
            parts = formatted_message.split(' | ')
            colored_parts = []
            
            # Determine the status color
            status_color = Fore.WHITE  # Default color
            for status, color in self.STATUS_COLORS.items():
                if f"Status: {status}" in formatted_message:
                    status_color = color
                    break
            
            for part in parts:
                if part.startswith('Result:'):
                    # Use standard level color for result
                    colored_parts.append(f"{self.COLORS.get(record.levelname, '')}{part}{Style.RESET_ALL}")
                else:
                    # Color the entire part (both label and value) with status color
                    colored_parts.append(f"{status_color}{part}{Style.RESET_ALL}")
            
            return ' | '.join(colored_parts)
            
        return f"{self.COLORS.get(record.levelname, '')}{formatted_message}{Style.RESET_ALL}"




@dataclass
class MemoryStats:
    """Data class for memory statistics"""
    rss: float
    vms: float
    percent: float
    system_used: float

class MemoryMonitor:
    """Memory monitoring utility class"""
    def __init__(self):
        self.process = psutil.Process(os.getpid())
        self.start_time = self.last_check = time.time()
        self.check_interval = 60

    def get_memory_usage(self) -> MemoryStats:
        mem = self.process.memory_info()
        return MemoryStats(
            rss=mem.rss / (1024 * 1024),
            vms=mem.vms / (1024 * 1024),
            percent=self.process.memory_percent(),
            system_used=psutil.virtual_memory().percent
        )

    def log_memory(self, tag: str, logger: logging.Logger) -> None:
        try:
            stats = self.get_memory_usage()
            logger.info(
                f"Memory at {tag}: RSS={stats.rss:.1f}MB, "
                f"Process=%{stats.percent:.1f}, System=%{stats.system_used:.1f}"
            )
            self.last_check = time.time()
        except Exception as e:
            logger.error(f"Error monitoring memory: {str(e)}")

memory_monitor = MemoryMonitor()

class LoggerFactory:
    """Factory class for creating and configuring loggers"""
    @staticmethod
    def create_rotating_handler(filepath: str, formatter: logging.Formatter, 
                              level: int = logging.DEBUG) -> RotatingFileHandler:
        handler = RotatingFileHandler(filepath, maxBytes=10*1024*1024, backupCount=5)
        handler.setLevel(level)
        handler.setFormatter(formatter)
        return handler

    @staticmethod
    def configure_logger(name: str, handlers: List[logging.Handler], 
                        level: int, propagate: bool = True) -> logging.Logger:
        logger = logging.getLogger(name)
        logger.setLevel(level)
        for handler in handlers:
            logger.addHandler(handler)
        logger.propagate = propagate
        return logger



def setup_logging(console_level=logging.DEBUG, file_level=logging.DEBUG, 
                 log_file='app.log', fsevents_level=logging.INFO):
    """Configure logging with specified levels and handlers"""
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)

    # Create formatters
    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_formatter = ColoredFormatter('%(name)s - %(levelname)s - %(message)s')
    memory_formatter = logging.Formatter('%(asctime)s - %(message)s')
    callback_formatter = ColoredFormatter('%(name)s - %(levelname)s - %(message)s', callback_color=True)

    # Create common handlers
    file_handler = LoggerFactory.create_rotating_handler(
        str(log_dir / log_file), file_formatter, file_level
    )
    console_handler = logging.StreamHandler()
    console_handler.setLevel(console_level)
    console_handler.setFormatter(console_formatter)

    # Configure root logger first
    root_logger = LoggerFactory.configure_logger(
        '', [file_handler, console_handler], 
        min(console_level, file_level)
    )

    # Configure callback logger with its own handler and blue color formatter
    callback_console_handler = logging.StreamHandler()
    callback_console_handler.setFormatter(callback_formatter)
    callback_console_handler.setLevel(console_level)

    callback_file_handler = LoggerFactory.create_rotating_handler(
        str(log_dir / 'callbacks.log'), 
        file_formatter  # Use standard file formatter for file logs
    )

    # Important: Set propagate=False to prevent callback logs from going through root logger
    callback_logger = LoggerFactory.configure_logger(
        'callbacks',
        [callback_file_handler, callback_console_handler],
        logging.DEBUG,
        propagate=False  # This is crucial to prevent double-logging
    )

    # Configure memory logger
    memory_logger = LoggerFactory.configure_logger(
        'memory_profiler',
        [
            LoggerFactory.create_rotating_handler(
                str(log_dir / 'memory_profile.log'), 
                memory_formatter
            ),
            logging.StreamHandler()
        ],
        logging.DEBUG,
        False
    )

    # Configure module-specific loggers
    logger_levels = {
        'app.app_layout': logging.DEBUG,
        'app.callbacks': {
            'app_layout_callbacks': logging.WARNING,
            'insurance_lines_callbacks': logging.WARNING,
            'filter_update_callbacks': logging.DEBUG,
        },
        'app.components': {
            'insurance_lines_tree': logging.WARNING,
            'period_filter': logging.WARNING,
            'range_filter': logging.WARNING,
            'dropdown': logging.WARNING,
            'dash_table': logging.WARNING,
        },
        'charting': logging.WARNING,
        'constants': logging.WARNING,
        'data_process': logging.DEBUG,
        'fsevents': fsevents_level,
    }

    for base_name, config in logger_levels.items():
        if isinstance(config, dict):
            for sub_name, level in config.items():
                logger = logging.getLogger(f"{base_name}.{sub_name}")
                logger.setLevel(level)
        else:
            logger = logging.getLogger(base_name)
            logger.setLevel(config)

def get_logger(name: str) -> logging.Logger:
    """Get a logger instance"""
    return logging.getLogger(name)

def custom_profile(func: Callable) -> Callable:
    """Profile function execution with memory usage tracking"""
    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        if debug_level < LogLevel.VERBOSE:
            return func(*args, **kwargs)

        memory_logger = get_logger('memory_profiler')
        initial_memory = memory_usage(-1, interval=0.1, timeout=1)[0]
        start_time = time.time()

        mem_usage = list(memory_usage(
            (func, args, kwargs), 
            interval=0.1, 
            timeout=None, 
            max_iterations=1
        ))
        result = func(*args, **kwargs)

        execution_time = time.time() - start_time
        memory_stats = {
            'initial': initial_memory,
            'final': mem_usage[-1],
            'min': min(mem_usage),
            'max': max(mem_usage),
            'change': mem_usage[-1] - initial_memory
        }

        memory_logger.debug(
            f"\nFunction '{func.__name__}' memory profile:\n"
            f"├─ Time and Memory Overview:\n"
            f"│  ├─ Execution time: {execution_time:.2f}s\n"
            f"│  ├─ Initial memory: {memory_stats['initial']:.1f} MiB\n"
            f"│  ├─ Final memory: {memory_stats['final']:.1f} MiB\n"
            f"│  ├─ Net change: {memory_stats['change']:.1f} MiB\n"
            f"│  └─ Memory range: {memory_stats['min']:.1f} MiB - {memory_stats['max']:.1f} MiB"
        )

        memory_logger.info(
            f"\nFunction: {func.__name__}\n"
            f"Memory range: {memory_stats['min']:.1f} MiB - {memory_stats['max']:.1f} MiB\n"
            f"Execution time: {execution_time:.2f}s\n"
        )

        return result
    return wrapper

def monitor_memory(func: Callable) -> Callable:
    """Monitor memory usage of decorated functions"""
    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        start_time = time.time()
        start_mem = memory_monitor.get_memory_usage()
        try:
            return func(*args, **kwargs)
        finally:
            end_mem = memory_monitor.get_memory_usage()
            print(
                f"Memory {func.__name__}: Current={end_mem.rss:.1f}MB, "
                f"Change={end_mem.rss - start_mem.rss:+.1f}MB, "
                f"Time={time.time()-start_time:.3f}s"
            )
    return wrapper

def track_callback(logger_name: str, callback_name: str, ctx) -> tuple[float, dict]:
    """Track callback execution start with enhanced debugging"""
    trigger_info = ctx.triggered[0]
    # Add stack trace to help identify where the callback is being triggered from
    import traceback
    stack = traceback.extract_stack()
    caller_info = stack[-2]  # Get the caller's information

    logger = get_logger('callbacks')
    logger.debug(
        f"Callback '{callback_name}' triggered by '{trigger_info['prop_id']}' "
        f"from {caller_info.filename}:{caller_info.lineno}"
    )

    return time.time(), {
        'trigger': trigger_info['prop_id'].split('.')[0],
        'trigger_value': trigger_info['value'],
        'start_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'caller_info': f"{caller_info.filename}:{caller_info.lineno}"
    }

def format_value_for_logging(value: Any) -> str:
    """Format complex data structures for logging in a more readable way"""
    if isinstance(value, dict):
        # For dictionaries, summarize the keys and length
        return f"<Dict with {len(value)} keys: {', '.join(list(value.keys())[:5])}{'...' if len(value) > 5 else ''}>"
    elif isinstance(value, list):
        # For lists, show length and first few items
        return f"<List with {len(value)} items>"
    elif isinstance(value, str) and len(value) > 200:
        # Truncate long strings
        return f"{value[:200]}... (truncated)"
    return str(value)


def track_callback_end(logger_name: str, callback_name: str, 
                      start_info: tuple[float, dict], result=None, 
                      error=None, message_no_update=None) -> None:
    """Log comprehensive callback execution information without duplication"""
    logger = get_logger('callbacks')
    start_time, context = start_info
    execution_time = (time.time() - start_time) * 1000

    def format_data(data):
        """Format complex data for logging"""
        if isinstance(data, dict):
            if 'df' in data and 'prev_ranks' in data:
                df_length = len(data['df']) if isinstance(data['df'], list) else 'N/A'
                ranks_length = len(data['prev_ranks']) if isinstance(data['prev_ranks'], dict) else 'N/A'
                return f"<DataFrame with {df_length} rows, Rankings with {ranks_length} items>"
            return f"<Dict with {len(data)} keys: {', '.join(list(data.keys())[:3])}{'...' if len(data) > 3 else ''}>"
        elif isinstance(data, (tuple, list)):
            return f"<{type(data).__name__} with {len(data)} items>"
        elif isinstance(data, str) and len(data) > 100:
            return f"{data[:100]}..."
        return str(data)

    # Create base log message
    log_parts = [
        f"Callback: {callback_name}",
        f"Time: {context['start_timestamp']}",
        f"Trigger: {context['trigger']}",
        f"Value: {format_data(context['trigger_value'])}",
        f"Execution: {execution_time:.2f}ms"
    ]

    # Add status and any additional information
    if error:
        log_parts.extend([f"Status: Error", f"Error: {str(error)}"])
        logger.error(" | ".join(log_parts), exc_info=True)
    elif message_no_update:
        log_parts.extend([f"Status: Prevented", f"Reason: {message_no_update}"])
        logger.info(" | ".join(log_parts))
    else:
        log_parts.append(f"Status: Completed")
        if result:
            log_parts.append(f"Result: {format_data(result)}")
        logger.info(" | ".join(log_parts))

    # Log slow callback warning separately
    if execution_time > 1000:
        logger.warning(f"Slow callback detected: {callback_name} took {execution_time:.2f}ms")



# Export all required symbols
__all__ = [
    'setup_logging',
    'get_logger',
    'custom_profile',
    'set_debug_level',
    'DebugLevels',
    'track_callback',
    'track_callback_end',
    'monitor_memory',
    'memory_monitor'
]

================
File: config/main_config.py
================
# config.py
import dash_bootstrap_components as dbc
from typing import Dict, List, Any

# Application configuration
APP_TITLE: str = "Insurance Data Dashboard"
DEBUG_MODE: bool = True
PORT: int = 8051

DATA_FILE_REINSURANCE: str = './data_files/reinsurance_market.csv'
DATA_FILE_158: str = './data_files/3rd_158_net.csv'
DATA_FILE_162: str = './data_files/3rd_162_net.csv'

INSURERS_DICTIONARY: str = './constants/mappings/insurer_map.json'
LINES_162_DICTIONARY: str = './constants/mappings/insurance_lines_162_rev.json'
LINES_158_DICTIONARY: str = './constants/mappings/insurance_lines_158_rev.json'

# Dash configuration to prevent reloads
# Dash configuration
DASH_CONFIG = {
    'debug': DEBUG_MODE,
    'dev_tools_hot_reload': False,
    'jupyter_mode': 'tab',
    'port': PORT
}

================
File: constants/filter_options.py
================
# constants/filter_options.py

from typing import Dict, Any, Union, Callable, List
from constants.translations import translate

BASE_OPTIONS = {
    'REPORTING_FORM': {
        '0420162': {'label': '0420162', 'type': 'value'}, # 0420162 «Сведения о деятельности страховщика»
        '0420158': {'label': '0420158', 'type': 'value'} # 0420158 «Отчет о структуре финансового результата по учетным группам»'
    },
    'PREMIUM_LOSS': {
        'direct': {'label': 'Direct', 'type': 'value'},
        'inward': {'label': 'Inward', 'type': 'value'}
    }
}


# Metrics Configuration
BASE_METRICS = {
    'ceded_losses', 'ceded_premiums', 'claims_reported', 'claims_settled',
    'contracts_end', 'direct_losses', 'direct_premiums', 'inward_losses',
    'inward_premiums', 'new_contracts', 'new_sums', 'sums_end',
    'premiums_interm', 'commissions_interm',
    'net_balance', 'total_premiums', 'net_premiums', 'total_losses',
    'net_losses', 'gross_result', 'net_result'
}

# Keeping original CALCULATED_METRICS and CALCULATED_RATIOS for backwards compatibility
CALCULATED_METRICS = {
    'net_balance': ['ceded_losses', 'ceded_premiums'],
    'total_premiums': ['direct_premiums', 'inward_premiums'],
    'net_premiums': ['direct_premiums', 'inward_premiums', 'ceded_premiums'],
    'total_losses': ['direct_losses', 'inward_losses'],
    'net_losses': ['direct_losses', 'inward_losses', 'ceded_losses'],
    'gross_result': ['direct_premiums', 'inward_premiums', 'direct_losses', 'inward_losses'],
    'net_result': ['direct_premiums', 'inward_premiums', 'direct_losses', 'inward_losses', 'ceded_premiums', 'ceded_losses']
}

CALCULATED_RATIOS = {
    'average_sum_insured': ['sums_end', 'contracts_end'],
    'average_new_sum_insured': ['new_sums', 'new_contracts'],
    'average_new_premium': ['direct_premiums', 'new_contracts'],
    'average_loss': ['direct_losses', 'claims_settled'],
    'average_rate': ['new_sums', 'direct_premiums'],
    'commissions_rate': ['premiums_interm', 'commissions_interm'],
    'ceded_premiums_ratio': ['ceded_premiums', 'total_premiums'],
    'ceded_losses_ratio': ['ceded_losses', 'total_losses'],
    'ceded_losses_to_ceded_premiums_ratio': ['ceded_losses', 'ceded_premiums'],
    'direct_loss_ratio': ['direct_losses', 'direct_premiums'],
    'inward_loss_ratio': ['inward_losses', 'inward_premiums'],
    'gross_loss_ratio': ['direct_losses', 'inward_losses', 'direct_premiums', 'inward_premiums'],
    'net_loss_ratio': ['direct_losses', 'inward_losses', 'ceded_losses', 'direct_premiums', 'inward_premiums', 'ceded_premiums'],
    'effect_on_loss_ratio': ['direct_losses', 'inward_losses', 'ceded_losses', 'direct_premiums', 'inward_premiums', 'ceded_premiums'],
    'ceded_ratio_diff': ['ceded_losses', 'direct_losses', 'inward_losses', 'ceded_premiums', 'direct_premiums', 'inward_premiums', 'total_losses', 'total_premiums', 'ceded_losses_ratio', 'ceded_premiums_ratio'],
    'premiums_interm_ratio': ['direct_premiums', 'premiums_interm']
}

def create_metric_definition(label: str, metric_type: str) -> Dict[str, str]:
    """Create a standardized metric definition dictionary."""
    return {'label': label, 'type': metric_type}

# Define base metrics with their types
METRICS = {
    # Value metrics
    **{metric: create_metric_definition(f"{metric.replace('_', ' ').title()}", 'value')
       for metric in ['direct_premiums', 'direct_losses', 'inward_premiums', 'inward_losses',
                     'total_premiums', 'total_losses', 'ceded_premiums', 'ceded_losses',
                     'net_premiums', 'net_losses', 'premiums_interm',
                     'commissions_interm', 'new_sums', 'sums_end']},

    # Quantity metrics
    **{metric: create_metric_definition(f"{metric.replace('_', ' ').title()}", 'quantity')
       for metric in ['new_contracts', 'contracts_end', 'claims_reported', 'claims_settled']},

    # Average metrics
    **{metric: create_metric_definition(f"{metric.replace('_', ' ').title()}", 'average_value')
       for metric in ['average_sum_insured', 'average_new_sum_insured', 'average_new_premium', 
                     'average_loss', 'average_rate']},

    # Percentage metrics
    **{metric: create_metric_definition(f"{metric.replace('_', ' ').title()}", 'percentage')
       for metric in ['net_loss_ratio', 'ceded_premiums_ratio', 'ceded_losses_ratio',
                     'ceded_losses_to_ceded_premiums_ratio', 'direct_loss_ratio',
                     'inward_loss_ratio', 'gross_loss_ratio', 'premiums_interm_ratio',
                     'commissions_rate']},  # Added the missing ratio metrics

    # Market share metrics
    **{f"{base}_market_share": create_metric_definition(f"{base.replace('_', ' ').title()} Market Share", 'market_share')
       for base in ['direct_premiums', 'direct_losses', 'inward_premiums',
                   'inward_losses', 'ceded_premiums', 'ceded_losses']},

    # Quarter-to-quarter change metrics
    **{f"{base}_q_to_q_change": create_metric_definition(f"{base.replace('_', ' ').title()} Growth", 'q_to_q_change')
       for base in ['direct_premiums', 'direct_losses', 'inward_premiums', 'inward_losses',
                   'ceded_premiums', 'ceded_losses', 'total_premiums', 'total_losses',
                   'net_loss_ratio', 'ceded_premiums_ratio', 'ceded_losses_to_ceded_premiums_ratio',
                   'average_new_sum_insured', 'average_loss', 'average_new_premium',
                   'direct_premiums_market_share', 'direct_losses_market_share',
                   'ceded_premiums_market_share', 'ceded_losses_market_share']}
}

# Specific metrics for form 158
METRICS_158 = {k: v for k, v in METRICS.items() if k in {
    'total_premiums', 'total_losses', 'ceded_premiums', 'ceded_losses',
    'net_premiums', 'net_losses', 'ceded_premiums_ratio',
    'ceded_losses_to_ceded_premiums_ratio'
}}

def create_dropdown_options(
    items: Union[Dict[str, Dict[str, str]], List[str]],
    translation_func: Callable = translate
) -> List[Dict[str, Any]]:
    """Create dropdown options with optional translation."""
    if isinstance(items, dict):
        return [{'label': translation_func(info['label']), 'value': key} 
                for key, info in items.items()]
    return [{'label': translation_func(item), 'value': item} for item in items]

# Generate dropdown options
LINEMAIN_OPTIONS = create_dropdown_options({})  # Empty dict as per original
REPORTING_FORM_OPTIONS = create_dropdown_options(BASE_OPTIONS['REPORTING_FORM'])
PREMIUM_LOSS_OPTIONS = create_dropdown_options(BASE_OPTIONS['PREMIUM_LOSS'])

# Filtered metric options
VALUE_METRICS = {k: v for k, v in METRICS.items() if v['type'] == 'value'}
AVERAGE_VALUE_METRICS = {k: v for k, v in METRICS.items() if v['type'] == 'average_value'}
RATIO_METRICS = {k: v for k, v in METRICS.items() if v['type'] == 'percentage'}

# Generate metric dropdown options
VALUE_METRICS_OPTIONS = create_dropdown_options(VALUE_METRICS)
AVERAGE_VALUE_METRICS_OPTIONS = create_dropdown_options(AVERAGE_VALUE_METRICS)
RATIO_METRICS_OPTIONS = create_dropdown_options(RATIO_METRICS)
VALUE_METRICS_OPTIONS_158 = create_dropdown_options(METRICS_158)
METRICS_OPTIONS = create_dropdown_options(METRICS)

================
File: constants/translations.py
================
# constants/translations.py

from typing import Dict


TRANSLATIONS: Dict[str, str] = {

    # Period Types
    
    "ytd": "Нарастающим итогом с начала года (Year-to-Date)", 
    "qoq": "К предыдущему кварталу",
    "yoy_q": "К аналогичному кварталу прошлого года", 
    "yoy_y": "Данные за последовательные 12 месяцев, год к году", 
    "mat": "Moving Annual Total",

    # YTD
    '''это сумма данных за все завершённые периоды текущего года, начиная с января".  
    - Пример:  
      - Если сейчас февраль 2024:  
        - Период расчёта: январь 2024 + февраль 2024 (например, 1 000 + 1 200 = 2 200).  
        - Сравнивается с: январь 2023 + февраль 2023 (например, 900 + 1 100 = 2 000).'''
    # (Quarter-over-Quarter)
    '''сравнение показателей за текущий квартал с предыдущим кварталом - то есть последовательные кварталы".  
    - Пример:  
      - 1 квартал 2024:  
        - Период расчёта: январь-март 2024 (например, 3 200).  
        - Сравнивается с: октябрь-декабрь 2023 (например, 3 000).'''

    # (Year-over-Year Quarter)   
    '''сравнение показателей за текущий квартал с тем же кварталом предыдущего года".  
        - Пример:  
          - 1 квартал 2024 года:  
            - Период расчёта: январь-март 2024 (например, 3 000).  
            - Сравнивается с: январь-март 2023 (например, 2 800).  '''

    # (Year-over-Year Year)
    '''сравнение суммы показателей за последние 12 месяцев с аналогичным периодом предыдущего года".  
        - Пример:  
          - На март 2024:  
            - Период расчёта: апрель 2023 — март 2024 (например, 12 000).  
            - Сравнивается с: апрель 2022 — март 2023 (например, 11 500).'''

    # Скользящие годовые данные с квартальным интервалом между периодами (Moving Annual Total)
    '''сравнение скользящих годовых данных с квартальным интервалом между периодами".  
        - Пример:  
          - На март 2024:  
            - Период расчёта: апрель 2023 — март 2024 (например, 12 500).  
            - Сравнивается с: январь 2023 — декабрь 2023 (например, 12 300). '''


    # Line Types
    "Direct": "Прямое",
    "Inward": "Входящее",
    "Sums End": "Страховые суммы по действующим договорам",  # Made parallel with other sums
    "Average Sum Insured": "Средняя страховая сумма по действующим договорам",
    # Premium and Loss Types
    "direct_premiums": "Премии по прямому страхованию",
    "direct_losses": "Выплаты по прямому страхованию",
    "inward_premiums": "Премии по входящему перестрахованию",
    "inward_losses": "Выплаты по входящему перестрахованию",
    "ceded_premiums": "Премии по исходящему перестрахованию",
    "ceded_losses": "Выплаты по исходящему перестрахованию",
    "net_premiums": "Премии-нетто перестрахования",
    "net_losses": "Выплаты-нетто перестрахования",

    # Contract and Claims Metrics
    "new_contracts": "Количество новых договоров",
    "new_sums": "Страховые суммы по новым договорам",
    "contracts_end": "Количество действующих договоров",
    "sums_end": "Страховые суммы по действующим договорам",
    "claims_reported": "Количество заявленных страховых случаев",
    "claims_settled": "Количество урегулированных страховых случаев",

    # Intermediary Metrics
    "premiums_interm": "Премии от посредников",
    "commissions_interm": "Вознаграждение посредникам",
    "New Sums": "Страховые суммы по заключенным договорам",

    # Market Metrics
    "Total Premiums": "Премии",
    "Total Losses": "Выплаты",
    "Ceded Premiums": "Премии по исходящему перестрахованию",
    "Ceded Losses": "Выплаты по исходящему перестрахованию",
    "New Contracts": "Кол-во заключенных договоров",
    "New Sums": "Страховые суммы по заключенным договорам",
    "Claims Settled": "Кол-во урегулированных случаев",
    "Average New Sum Insured": "Средняя страховая сумма по новым договорам",
    "Average New Premium": "Средняя премия по новым договорам",
    "average_loss": "Средняя сумма выплаты",
    "Direct Premiums": "Премии по прямому страхованию",
    "Direct Losses": "Выплаты по прямому страхованию",
    "Inward Premiums": "Премии по входящему перестрахованию",
    "Inward Losses": "Выплаты по входящему перестрахованию",
    "Net Premiums": "Премии нетто-перестрахование",
    "Net Losses": "Выплаты нетто-перестрахование",
    "Premiums Interm": "Премии от посредников",
    "Commissions Interm": "Вознаграждение посредникам",
    "Premiums Interm Ratio": "Доля премий от посредников",
    "Commissions Rate": "Вознаграждение к премии",
    "Gross Loss Ratio": "Убыточность",
    "Direct Loss Ratio": "Убыточность",
    "Contracts End": "Кол-во действующих договоров",
    "Average Rate": "Средняя ставка",

    # Market Metrics
    "total_premiums": "Премии",
    "total_losses": "Выплаты",
    "ceded_premiums": "Премии по исходящему перестрахованию",
    "ceded_losses": "Выплаты по исходящему перестрахованию",
    "new_contracts": "Кол-во заключенных договоров",
    "new_sums": "Страховые суммы по заключенным договорам",
    "claims_settled": "Кол-во урегулированных случаев",
    "direct_premiums": "Премии по прямому страхованию",
    "direct_losses": "Выплаты по прямому страхованию",
    "inward_premiums": "Премии по входящему перестрахованию",
    "inward_losses": "Выплаты по входящему перестрахованию",
    "net_premiums": "Премии нетто-перестрахование",
    "net_losses": "Выплаты нетто-перестрахование",
    "premiums_interm": "Премии от посредников",
    "commissions_interm": "Вознаграждение посредникам",
    "contracts_end": "Кол-во действующих договоров",

    
    # Market Share Metrics
    "direct_premiums_market_share": "Доля рынка по прямому страхованию (премии)",
    "direct_losses_market_share": "Доля рынка по прямому страхованию (выплаты)",
    "inward_premiums_market_share": "Доля рынка по входящему перестрахованию (премии)",
    "inward_losses_market_share": "Доля рынка по входящему перестрахованию (выплаты)",
    "ceded_premiums_market_share": "Доля рынка по переданным в перестрахование премиям",
    "ceded_losses_market_share": "Доля рынка по переданным в перестрахование выплатам",
    "total_losses_market_share": "Доля рынка по общей сумме выплат",
    "total_premiums_market_share": "Доля рынка по общей сумме премий",
    "market_share": "доля рынка, %",
    "q_to_q_change": "Динамика, %",


    # Ratio Metrics and Changes
    "ceded_premiums_ratio": "Доля премий, переданных в перестрахование",
    "ceded_losses_ratio": "Доля выплат, переданных в перестрахование",
    "ceded_ratio_diff": "Разница между долями премий и выплат в перестраховании",
    "ceded_losses_to_ceded_premiums_ratio": "Коэффициент выплат по исходящему перестрахованию",
    "gross_loss_ratio": "Коэффициент убыточности (брутто)",
    "net_loss_ratio": "Коэффициент убыточности (нетто)",
    "effect_on_loss_ratio": "Влияние перестрахования на коэффициент убыточности",
    "Net Loss Ratio Growth": "Динамика коэффициента убыточности (нетто)",
    "Ceded Premiums Ratio Growth": "Динамика доли премий в перестраховании",
    "Ceded Losses to Premiums Ratio Growth": "Динамика коэффициента выплат в перестраховании",
    "Ceded Premiums Ratio": "Доля премий, переданных в перестрахование",
    "Ceded Losses Ratio": "Доля выплат, переданных в перестрахование",
    "Net Loss Ratio": "Коэффициент убыточности (нетто)",
    "Ceded Losses to Ceded Premiums Ratio": "Коэффициент выплат по исходящему перестрахованию",

    # Average Values
    "Average Sum Insured": "Средняя страховая сумма",
    "Average Loss": "Средняя сумма выплаты",

    "Total Market": "Весь рынок",

    # General Labels
    "Value": "Значение",
    "Percentage / Ratio": "Процент / Коэффициент",
    "Market Share by Line of Business": "Доля рынка по видам страхования",
    "Market Share": "Доля рынка",
    "Line of Business": "Вид страхования",
    "insurer": "Страховщик",

    # Reinsurance Categories
    "within_russia": "на территории Российской Федерации",
    "outside_russia": "за пределы территории Российской Федерации",
    "facultative": "факультативное",
    "obligatory": "облигаторное",
    "ob_fac": "облигаторно-факультативное",
    "fac_ob": "факультативно-облигаторное",
    "proportional": "пропорция",
    "non_proportional": "непропорция",
    "reinsurance_geography": "По географии перестрахования",
    "reinsurance_form": "По форме перестрахования",
    "reinsurance_type": "По виду перестрахования",
    "Hide": "Скрыть все категории",
}

def translate(text: str) -> str:
    """
    Translate the given text using the TRANSLATIONS dictionary.

    Args:
        text (str): The text to translate.

    Returns:
        str: The translated text, or the original text if no translation is found.
    """
    return TRANSLATIONS.get(text, text)

def translate_quarter_column(column_name: str, quarter: str) -> str:
    """
    Translate a quarter-specific column name.

    Args:
        column_name (str): The base column name to translate.
        quarter (str): The quarter identifier (e.g., "2023Q1").

    Returns:
        str: The translated column name with the quarter.
    """
    base_translation = translate(column_name.split('_')[0])
    return f"{base_translation} ({quarter})"

def cached_translate(text: str) -> str:
    """Cache translations to improve performance."""
    return translate(text)

def translate_quarter(quarter: str) -> str:
    """Translate quarter string to Russian format."""
    year, q = quarter.split('Q')
    months = {
        '1': '3 месяца',
        '2': '6 месяцев',
        '3': '9 месяцев',
        '4': '12 месяцев'
    }
    return f"{year} год, {months[q]}"

================
File: data_process/data_utils.py
================
# data_process.data_utils.py

from dash import dcc
import logging
import pandas as pd
from typing import List, Dict, Any, Tuple, Set, Union, Optional
import json
import re
import os
import plotly.graph_objects as go

from config.logging_config import get_logger
from config.main_config import LINES_162_DICTIONARY, INSURERS_DICTIONARY, LINES_158_DICTIONARY


logger = get_logger(__name__)


def load_json(file_path: str) -> Dict:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.debug(f"Successfully loaded {file_path}")
        return data
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        raise
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in file: {file_path}")
        raise


def strip_metric_suffixes(metric: str) -> str:
    suffixes_to_remove = [
        '_market_share_q_to_q_change',
        '_q_to_q_change',
        '_market_share'
    ]

    for suffix in suffixes_to_remove:
        if metric.endswith(suffix):
            logger.debug(f"Stripping suffix {suffix} from {metric}")
            return metric[:-len(suffix)]
    return metric


def get_categories_by_level(category_structure, level=1, indent_char="  "):
    def clean_label(label):
        """Remove 'Добровольное' from the label and handle extra spaces"""
        return ' '.join(label.replace('Добровольное', '').split())

    def traverse_categories(code, current_level=0, max_level=None):
        # Check if category exists in structure
        if code not in category_structure:
            return []
            
        if max_level is not None and current_level > max_level:
            return []
            
        result = []
        # Add current category
        label = category_structure[code].get('label', f"Category {code}")
        cleaned_label = clean_label(label)
        indentation = indent_char * current_level
        result.append({
            'label': f"{indentation}{cleaned_label}",
            'value': code
        })
        
        # Add children if within max_level
        if max_level is None or current_level < max_level:
            # Safely get children, defaulting to empty list if none exist
            children = category_structure[code].get('children', [])
            # Only traverse existing children
            for child in children:
                if child in category_structure:  # Extra safety check
                    result.extend(traverse_categories(child, current_level + 1, max_level))
                
        return result

    # Verify root category exists
    root = "все линии"
    if root not in category_structure:
        return []  # Return empty list if root category doesn't exist
        
    # Start with root category and traverse
    return traverse_categories(root, 0, level)


def get_top_level_and_children(category_structure):
    # First, identify top-level categories
    top_level = [code for code, category in category_structure.items()
                 if not any(code in cat.get('children', []) for cat in category_structure.values())]

    # Then, get these categories and their direct children
    options = []

    for code, category in category_structure.items():
        if (code in top_level or
            any(code in category_structure[parent].get('children', []) for parent in top_level) or
            any(code in category_structure[child].get('children', [])
                for parent in top_level
                for child in category_structure[parent].get('children', []))):
            options.append({'label': category.get('label', f"Category {code}"), 'value': code})

    return options


def get_all_descendants(category):
    descendants = set()
    to_process = get_immediate_children(category)

    while to_process:
        current = to_process.pop()
        descendants.add(current)
        # Add children of current category to processing queue
        if current in category_structure:
            to_process.update(get_immediate_children(current))

    return descendants


def get_immediate_children(category):
    return set(category_structure[category].get('children', []))


def handle_parent_child_selections(selected_categories, category_structure, detailize=False):
    logger.info(f"Starting handle_parent_child_selections with selected categories: {selected_categories}")
    logger.info(f"Detailize flag: {detailize}")
    if not detailize:
        new_selected = set(selected_categories)
        # Remove all descendants if their ancestor is selected
        for category in selected_categories:
            if category in category_structure and category_structure[category].get('children'):
                descendants = get_all_descendants(category)
                removed_descendants = new_selected.intersection(descendants)
                new_selected.difference_update(removed_descendants)
                if removed_descendants:
                    logger.debug(f"Removed descendants of {category}: {removed_descendants}")
    else:
        new_selected = set()
        # Add children for categories that have them, or keep the category if it's a leaf
        for category in selected_categories:
            if category in category_structure and category_structure[category].get('children'):
                children = get_immediate_children(category)
                new_selected.update(children)
                logger.info(f"Detailized {category}. Added children: {children}")
            else:
                new_selected.add(category)
                logger.info(f"Category {category} has no children, keeping it as is")

    final_selected = list(new_selected)
    logger.info(f"Final selected categories: {final_selected}")
    return final_selected


def get_required_metrics(
    selected_metrics: List[str],
    calculated_metrics: Dict[str, List[str]],
    premium_loss_selection: Optional[List[str]] = None,
    base_metrics: Optional[Set[str]] = None
) -> Set[str]:
    logger.debug("Starting get_required_metrics function")

    def strip_metric_suffixes(metric: str) -> str:
        suffixes_to_remove = [
            '_market_share_q_to_q_change',
            '_q_to_q_change',
            '_market_share'
        ]

        for suffix in suffixes_to_remove:
            if metric.endswith(suffix):
                logger.debug(f"Stripping suffix {suffix} from {metric}")
                return metric[:-len(suffix)]
        return metric

    def update_required_metrics(selected_metrics, calculated_metrics, base_metrics):
        # First, strip suffixes from selected metrics
        cleaned_metrics = [strip_metric_suffixes(metric) for metric in selected_metrics]
        logger.debug(f"Cleaned metrics after stripping suffixes: {cleaned_metrics}")

        required_metrics = set(cleaned_metrics)
        metrics_to_process = list(cleaned_metrics)
        processed_metrics = set()

        while metrics_to_process:
            current_metric = metrics_to_process.pop(0)
            if current_metric in processed_metrics:
                continue

            processed_metrics.add(current_metric)
            logger.debug(f"Processing metric: {current_metric}")

            for calc_metric, base_metrics_for_calc in calculated_metrics.items():
                if calc_metric == current_metric:  # Simplified check
                    logger.debug(f"Found calc_metric: {calc_metric}")
                    new_metrics = set(base_metrics_for_calc) - required_metrics
                    logger.debug(f"new_metrics: {new_metrics}")

                    required_metrics.update(new_metrics)
                    metrics_to_process.extend(new_metrics)

            logger.debug(f"Current required_metrics: {required_metrics}")
            logger.debug(f"Metrics left to process: {metrics_to_process}")

        return required_metrics

    required_metrics = update_required_metrics(selected_metrics, calculated_metrics, base_metrics)

    if premium_loss_selection:
        if 'direct' not in premium_loss_selection:
            required_metrics.difference_update(['direct_premiums', 'direct_losses'])
        if 'inward' not in premium_loss_selection:
            required_metrics.difference_update(['inward_premiums', 'inward_losses'])

    logger.debug(f"List of required metrics: {required_metrics}")
    logger.debug("Finished get_required_metrics function")
    required_metrics = list(required_metrics)
    logger.debug(f"List of required metrics: {required_metrics}")

    return required_metrics


def process_inputs(
    primary_y_metric,
    secondary_y_metric,
    main_insurer,
    compare_insurers,
    x_column, series_column, group_column
):
    def ensure_list(value):
        if value is None:
            return []
        return [value] if isinstance(value, str) else list(value) if value else []
    primary_y_metric = ensure_list(primary_y_metric)
    secondary_y_metric = ensure_list(secondary_y_metric)

    chart_selected_metric = [m for m in (primary_y_metric + secondary_y_metric) if m]
    table_selected_metric = ensure_list(primary_y_metric)
    main_insurer = ensure_list(main_insurer)
    compare_insurers = ensure_list(compare_insurers)
    selected_insurers = ensure_list(main_insurer) + ensure_list(compare_insurers)
    chart_columns=[x_column, series_column, group_column]

    return primary_y_metric, secondary_y_metric, chart_selected_metric, table_selected_metric, main_insurer, compare_insurers, selected_insurers, chart_columns


def save_df_to_csv(df, filename, max_rows=500):
    """
    Save a DataFrame to two CSV files - one with random rows and one with the first N rows.

    Args:
        df (pd.DataFrame): The DataFrame to save
        filename (str): The base name of the file to save to
        max_rows (int): The maximum number of rows to save (default: 1000)
    """
    output_dir = './outputs/intermediate'
    os.makedirs(output_dir, exist_ok=True)

    n_rows = min(max_rows, len(df))

    # Create both samples
    df_to_save = df.sample(n=n_rows)
    df_to_save_ordered = df.head(n=n_rows)

    # Generate filenames
    base_name = os.path.splitext(filename)[0]  # Remove extension if presen
    full_path_sample = os.path.join(output_dir, f"{base_name}_random.csv")
    full_path_ordered = os.path.join(output_dir, f"{base_name}_first.csv")

    # Save both files
    df_to_save.to_csv(full_path_sample, index=False)
    df_to_save_ordered.to_csv(full_path_ordered, index=False)

    logger.debug(f"Saved {n_rows} random rows to {full_path_sample}")
    logger.debug(f"Saved {n_rows} ordered rows to {full_path_ordered}")


def print_dataframe_info(df: pd.DataFrame, stage: str = ""):
    logger.debug(f"DataFrame Info - {stage}")
    logger.debug("Column Names:")
    for col in df.columns:
        logger.debug(f"- {col}")
    logger.debug("=" * 50)

    logger.debug("Data Types:")
    logger.debug(df.dtypes)
    logger.debug("=" * 50)

    for col in df.columns:
        unique_values = df[col].unique()
        logger.debug(f"{col}: {len(unique_values)} unique values")
        logger.debug(f"  Values: {unique_values}")
    logger.debug("=" * 50)


def log_dataframe_info(df: pd.DataFrame, step_name: str):
    logger.debug(f"--- {step_name} ---")
    logger.debug(f"DataFrame shape: {df.shape}")
    logger.debug(f"Columns: {df.columns.tolist()}")
    for column in df.columns:
        unique_values = df[column].nunique()
        logger.debug(f"Unique values in '{column}': {unique_values}")
        if unique_values < 10:
            logger.debug(f"Unique values: {df[column].unique().tolist()}")
    logger.debug(f"First 5 rows:\n{df.head().to_string()}")
    logger.debug("-------------------")


def load_and_preprocess_data(file_path):

    logger.debug(f"Starting to load and preprocess data from {file_path}")

    dtype_dict = {
    'datatypec': 'object',
    'linemain': 'object',
    'insurer': 'object',
    'value': 'float64'
    }

    try:
        df = pd.read_csv(file_path, dtype=dtype_dict, parse_dates=['year_quarter'])
        print(f"Data loaded: {len(df)} rows x {len(df.columns)} columns")

        # Add prints before each preprocessing step
        df['metric'] = df['metric'].fillna(0)

        df = df.sort_values('year_quarter', ascending=True)

        return df

    except Exception as e:
        print(f"Error loading data: {str(e)}")
        raise


insurer_data = load_json(INSURERS_DICTIONARY)
default_insurer_options = [{'label': item['short_name'], 'value': item['reg_number']} for item in insurer_data]
default_insurer_options.append({'label': 'Всего по рынку', 'value': 'total'})
default_insurer_options.append({'label': 'Остальные', 'value': 'others'})
insurer_name_map = {item['reg_number']: item['short_name'] for item in insurer_data}
default_benchmark_options = [{'label': 'Остальные', 'value': 'others'}, {'label': 'Топ-5', 'value': 'top-5'}]


def create_year_quarter_options(df: pd.DataFrame) -> List[Dict[str, str]]:
    """Create year-quarter options efficiently."""
    # Extract unique periods once
    unique_periods = pd.PeriodIndex(df['year_quarter'].dt.to_period('Q')).unique()

    # Create options in a single list comprehension
    options = [
        {
            'label': period.strftime('%YQ%q'),
            'value': period.strftime('%YQ%q')
        }
        for period in unique_periods
    ]

    return options


category_structure_162 = load_json(LINES_162_DICTIONARY)

category_structure_158 = load_json(LINES_158_DICTIONARY)


def map_insurer(insurer_code: str) -> str:
    # Load the insurer mapping from the JSON file
    with open(INSURERS_DICTIONARY, 'r', encoding='utf-8') as f:
        INSURER_MAPPING = {item['reg_number']: item['short_name'] for item in json.load(f)}

    INSURER_MAPPING['total'] = 'Весь рынок'
    INSURER_MAPPING['others'] = 'Остальные'

    # Regular expressions to match 'top-X' and 'top-Y-benchmark' patterns
    top_pattern = re.compile(r'^top-(\d+)$')
    benchmark_pattern = re.compile(r'^top-(\d+)-benchmark$')

    # Check for 'top-X' pattern
    top_match = top_pattern.match(insurer_code)
    if top_match:
        n = top_match.group(1)
        return f'Топ {n}'

    # Check for 'top-Y-benchmark' pattern
    benchmark_match = benchmark_pattern.match(insurer_code)
    if benchmark_match:
        n = benchmark_match.group(1)
        return f'Топ {n} Бенчмарк'

    # Fallback to the existing mapping or return the original insurer_code
    return INSURER_MAPPING.get(insurer_code, insurer_code)


def map_line(line_code):
    # Load first JSON file
    with open(LINES_162_DICTIONARY, 'r', encoding='utf-8') as f:
        insurance_data1 = json.load(f)

    # Load second JSON file
    with open(LINES_158_DICTIONARY, 'r', encoding='utf-8') as f:
        insurance_data2 = json.load(f)

    # Combine the two dictionaries
    # If there are duplicate keys, the second file will override the firs
    combined_data = {**insurance_data1, **insurance_data2}

    # Create the mapping dictionary
    LINE_MAPPING = {category: data['label'] for category, data in combined_data.items()}

    # Handle both list and single inputs
    if isinstance(line_code, list):
        return [LINE_MAPPING.get(code, code) for code in line_code]

    return LINE_MAPPING.get(line_code, line_code)

def parse_title_text(title_text: str) -> Tuple[str, List[str]]:
    """
    Parse title text to separate main title and subtitles.

    Args:
        title_text: Original title text with HTML spans

    Returns:
        Tuple of (main_title, list_of_subtitles)
    """
    if not title_text:
        return "", []

    # Split by <br> or <br/> tags
    parts = re.split(r'<br\s*/?>', title_text)

    # First part is the main title
    main_title = parts[0] if parts else ""

    # Extract subtitles from spans
    subtitles = []
    span_pattern = re.compile(r"<span[^>]*>(.*?)</span>")

    for part in parts[1:]:
        match = span_pattern.search(part)
        if match:
            subtitle = match.group(1)
            subtitles.append(subtitle)

    return main_title, subtitles


def extract_chart_structure(figure: Union[dict, go.Figure]) -> Dict[str, Any]:
    """
    Extract and analyze the hierarchical structure of the chart.
    """
    # Convert figure to dict if it's a Figure objec
    fig_dict = figure if isinstance(figure, dict) else figure.to_dict()

    # Initialize structure containers
    groups_structure = defaultdict(lambda: defaultdict(list))

    if 'data' in fig_dict:
        for trace in fig_dict['data']:
            name = trace.get('name', '')
            group = trace.get('legendgroup')

            if group:
                groups_structure[group]['traces'].append(name)
            else:
                groups_structure['ungrouped']['traces'].append(name)

    # Extract and parse title
    title = ""
    subtitles = []
    if 'layout' in fig_dict and 'title' in fig_dict['layout']:
        title_data = fig_dict['layout']['title']
        if isinstance(title_data, dict):
            title_text = title_data.get('text', '')
            title, subtitles = parse_title_text(title_text)
        else:
            title = str(title_data)

    # Analyze structure
    structure_analysis = {
        'groups': {},
        'total_traces': 0,
        'title': title,
        'subtitles': subtitles
    }

    for group, data in groups_structure.items():
        traces = data['traces']
        unique_traces = sorted(set(traces))

        structure_analysis['groups'][group] = {
            'traces': traces,
            'unique_traces': unique_traces,
            'trace_count': len(traces),
            'unique_count': len(unique_traces),
            'instances_per_trace': {
                trace: traces.count(trace)
                for trace in unique_traces
            }
        }
        structure_analysis['total_traces'] += len(traces)

    return structure_analysis


def format_log_message(
    structure: Dict[str, Any],
    config: Dict[str, str]
) -> str:
    """Format the log message to show the hierarchical structure"""
    lines = [
        "Chart Generation Structure:",
        f"  Series Column: {config.get('series_column', 'N/A')}",
        f"  Group Column: {config.get('group_column', 'N/A')}",
        f"  X Column: {config.get('x_column', 'N/A')}",
        "",
        "Trace Structure:"
    ]

    for group, data in structure['groups'].items():
        if group != 'ungrouped':
            lines.extend([
                f"  Legend Group: {group}",
                "    Traces:"
            ])
        else:
            lines.extend([
                "  Ungrouped Traces:"
            ])

        for trace in data['unique_traces']:
            instances = data['instances_per_trace'][trace]
            lines.append(f"    - {trace} (× {instances} instances)")

        lines.append("")

    lines.extend([
        f"Total Traces: {structure['total_traces']}",
        "",
        "Title:",
        f"  {structure['title']}"
    ])

    if structure['subtitles']:
        lines.extend([
            "",
            "Subtitles:"
        ])
        for subtitle in structure['subtitles']:
            lines.append(f"  {subtitle}")

    return "\n".join(lines)


def log_chart_structure(
    figure: Union[dict, go.Figure, dcc.Graph],
    logger: logging.Logger,
    series_column: str = None,
    group_column: str = None,
    x_column: str = None
) -> None:
    """
    Log the hierarchical structure of the chart.
    """
    if isinstance(figure, dcc.Graph):
        figure = figure.figure

    config = {
        'series_column': series_column,
        'group_column': group_column,
        'x_column': x_column
    }

    structure = extract_chart_structure(figure)
    formatted_message = format_log_message(structure, config)
    logger.warning("\n" + formatted_message)


__all__ = ['load_csv', 'print_dataframe_info', 'create_insurer_options', 'create_additional_table_metrics', 'create_year_quarter_options', 'load_and_preprocess_data', 'load_json', 'map_insurer', 'map_line', 'map_insurer_code_to_name', 'map_line_code_to_name', 'map_metric_code_to_name', 'category_structure', 'default_insurer_options']

================
File: data_process/process_filters.py
================
# data_process.process_filters

import pandas as pd
import numpy as np
import gc
import logging
import time
from typing import List, Tuple, Dict
from dataclasses import dataclass
# from memory_profiler import profile
from data_process.data_utils import save_df_to_csv, get_required_metrics, map_insurer
from constants.filter_options import (
    BASE_METRICS, CALCULATED_METRICS, CALCULATED_RATIOS
)
from config.logging_config import get_logger
logger = get_logger(__name__)


@dataclass
class MetricsProcessor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    #@profile
    def process_general_filters(
        self,
        df: pd.DataFrame,
        show_data_table: bool,
        premium_loss_selection: List[str],
        selected_metrics: List[str],
        selected_linemains: List[str],
        period_type: str,
        start_quarter: str,
        end_quarter: str,
        num_periods: int,
        line_type: List[str] = None,
        chart_columns: List[str] = None,
        selected_insurers: List[str] = None,
        reinsurance_form: List[str] = None,
        reinsurance_geography: List[str] = None,
        reinsurance_type: List[str] = None,
        number_of_insurers: int = 150,
        top_n_list: List[int] = None,
        main_insurer: str = None
    ) -> Tuple[pd.DataFrame, List[Dict], List[Dict], List[str]]:

        #self.logger.debug(f"Initial DataFrame shape: {df.shape}")
        current_time = time.time()

        logger.debug(f"process_general_filters called, Time: {current_time}")
        start_time = time.perf_counter_ns()  # Nanosecond precision
        try:
            gc.enable()
            # Build mask incrementally without intermediate Series objects
            mask = pd.Series(True, index=df.index)
            # Get required metrics and convert to set once
            required_metrics_set = set(get_required_metrics(
                selected_metrics,
                {**CALCULATED_METRICS, **CALCULATED_RATIOS},
                premium_loss_selection,
                BASE_METRICS
            ))
            logger.debug(f"required_metrics_set: {required_metrics_set}")
            logger.debug(f"metrics unique: {df['metric'].unique()}")
            logger.debug(f"linemain unique: {df['linemain'].unique()}")
            # Apply linemain filter using isin() directly
            mask &= df['linemain'].isin(selected_linemains)
            mask &= df['metric'].isin(required_metrics_set)

            # Handle datetime filtering
            if start_quarter and end_quarter:
                if not pd.api.types.is_datetime64_any_dtype(df['year_quarter']):
                    df.loc[:, 'year_quarter'] = pd.to_datetime(df['year_quarter'])

                if period_type == 'ytd':
                    # Convert start_quarter to beginning of the year
                    start_quarter_interm = pd.Timestamp(start_quarter).replace(month=1)
                elif period_type in ['mat', 'yoy_y']:
                    # Subtract 3 quarters from start_quarter
                    start_quarter_interm = pd.Timestamp(start_quarter) - pd.DateOffset(months=9)
                else:
                    start_quarter_interm = start_quarter

                mask &= (df['year_quarter'] >= start_quarter_interm) & (df['year_quarter'] <= end_quarter)

            logger.debug(f"df shape before remaining filters: {df.shape}")

            # Apply remaining filters efficiently
            for col, values in {
                'reinsurance_form': reinsurance_form,
                'reinsurance_geography': reinsurance_geography,
                'reinsurance_type': reinsurance_type,
                'line_type': line_type
            }.items():
                if values and col in df.columns:
                    mask &= df[col].isin(values)

            # Apply the combined mask once
            df = df.loc[mask]
            del mask
            #gc.collect()
            # Get all columns except 'line_type' and 'value' for groupby
            group_cols = [col for col in df.columns if col not in ['line_type', 'value']]
            
            # Aggregate by summing values after dropping line_type
            df = df.groupby(group_cols, observed=True)['value'].sum().reset_index()

            
            logger.debug(f"df shape after remaining filters: {df.shape}")
            logger.debug(f"metrics unique: {df['metric'].unique()}")
            # Process data in chunks

            df = self.filter_by_date_range_and_period_type(
                df,
                period_type=period_type,
                num_periods=num_periods
            )

            number_of_periods_options = len(df['year_quarter'].unique())

            years_to_keep = sorted(df['year'].unique(), reverse=True)[:min(num_periods, number_of_periods_options) + 1]
            df = df[df['year'].isin(years_to_keep)]
            df = df.drop(columns=['year'])
            
            # Initialize options
            insurer_options = []
            compare_options = []
            prev_ranks = []

            # Add calculated metrics
            required_metrics = required_metrics_set - set(df['metric'].unique())
            
            df = self.add_calculated_metrics(df, list(required_metrics))


            logger.debug(f"metrics unique after add_calculated_metrics: {df['metric'].unique()}")

            # Filter by required ratio metrics
            required_ratio_metrics = set(get_required_metrics(
                selected_metrics,
                CALCULATED_RATIOS,
                premium_loss_selection,
                BASE_METRICS
            ))
            logger.debug(f"required_ratio_metrics: {required_ratio_metrics}")
            df = df[df['metric'].isin(required_ratio_metrics)]

            # Handle aggregation
            if (chart_columns and 'linemain' not in chart_columns) or show_data_table:
                group_cols = [col for col in df.columns if col not in {'linemain', 'value'}]
                df = (df.groupby(group_cols, observed=True)['value']
                       .sum()
                       .reset_index()
                       .assign(linemain='all_lines'))

            # Add market share calculations
            if show_data_table or any(metric.endswith(("market_share", "market_share_q_to_q_change"))
                                     for metric in selected_metrics):
                df = self.add_market_share_rows(df, selected_insurers, selected_metrics, show_data_table)

            logger.debug(f"metrics uniqye: {df['metric'].unique() }")
            logger.debug(f"required_metrics : {required_ratio_metrics}")
            # Add averages and ratios if needed
            required_metrics = get_required_metrics(selected_metrics, CALCULATED_RATIOS)
            if set(required_metrics) - BASE_METRICS:
                df = self.add_averages_and_ratios(df, required_metrics)


            end_quarter_df = df[df['year_quarter'] == sorted(df['year_quarter'].unique())[-1]]
            number_of_insurer_options = len(end_quarter_df['insurer'].unique())
            logger.debug(f"number of insurers options: {number_of_insurer_options}")
            logger.debug(f"number of insurers: {number_of_insurers}")
            logger.debug(f"min of insurers: {min(number_of_insurers, number_of_insurer_options)}")

            # Process insurers once
            if show_data_table or selected_insurers:
                df, insurer_options, compare_options, selected_insurers, prev_ranks = self.process_insurers_data(
                    df, selected_insurers, top_n_list, show_data_table,
                    selected_metrics, min(number_of_insurers, number_of_insurer_options), main_insurer
                )
            
            logger.debug(f"metrics unique: {df['metric'].unique() }")
            # Add growth calculations if needed
            if show_data_table or any(metric.endswith("q_to_q_change") for metric in selected_metrics):
                df = self.add_growth_rows_long(df, selected_insurers, show_data_table, num_periods, period_type)

            df = df[df['year_quarter'] >= start_quarter]

            if chart_columns and not show_data_table:
                if 'metric' in chart_columns:
                    df = df[df['metric'].isin(selected_metrics)].copy()
                else:
                    df[df['metric'] == selected_metrics[0] if selected_metrics[0] in df['metric'].unique() else df['metric'].iloc[0]]

            save_df_to_csv(df, "process_dataframe.csv")

            logger.debug(f"process_general_filters returns, Time: {current_time}")
            logger.debug(f"metrics unique: {df['metric'].unique() }")
            return df, insurer_options, compare_options, selected_insurers, prev_ranks, number_of_periods_options, number_of_insurer_options

        finally:
            end_time = time.perf_counter_ns()
            duration_ms = (end_time - start_time) / 1_000_000
            logger.debug(f"process_general_filters actual duration: {duration_ms:.3f}ms")

    #@profile
    def filter_by_date_range_and_period_type(
        self,
        df: pd.DataFrame,
        period_type: str,
        num_periods: int
    ) -> pd.DataFrame:

        """Filter data by date range and period type with optimized performance."""
        df = df.copy()
        df['year'] = df['year_quarter'].dt.year.to_numpy()
        grouping_cols = [col for col in df.columns if col not in {'year_quarter', 'value', 'year', 'quarter'}]
        end_quarter_num = df['year_quarter'].max().quarter
        
        if period_type == 'yoy_q':
            df.loc[:, 'quarter'] = df['year_quarter'].dt.quarter
            df = df[df['year_quarter'].dt.quarter == end_quarter_num]
            df = df.drop(columns=['quarter'])

        elif period_type == 'ytd':
            df.loc[:, 'quarter'] = df['year_quarter'].dt.quarter
            df = (df[df['year_quarter'].dt.quarter <= end_quarter_num]
                  .assign(year=lambda x: x['year_quarter'].dt.year,
                         ytd_value=lambda x: x.groupby(['year'] + grouping_cols)['value'].cumsum())
                  .assign(value=lambda x: x['ytd_value'])
                  .drop(columns=['ytd_value', 'quarter'])
                  .loc[lambda x: x['year_quarter'].dt.quarter == end_quarter_num]
                  .reset_index(drop=True))

        elif period_type in ['mat', 'yoy_y']:
            df = df.sort_values(grouping_cols + ['year_quarter'])
            df.set_index('year_quarter', inplace=True)
            df['value'] = df.groupby(grouping_cols)['value'].transform(
                lambda x: x.rolling(window='365D', min_periods=1).sum()
            )
            df.reset_index(inplace=True)

            if period_type == 'yoy_y':
                df.loc[:, 'quarter'] = df['year_quarter'].dt.quarter
                df = df[df['quarter'] == end_quarter_num]
                df = df.drop(columns=['quarter'])

        elif period_type == 'cumulative_sum':
            df['value'] = df.groupby(grouping_cols)['value'].cumsum()

        return df

    def process_insurers_data(
        self,
        df: pd.DataFrame,
        selected_insurers: List[str],
        top_n_list: List[int],
        show_data_table: bool,
        selected_metrics: List[str],
        number_of_insurers = 200,
        main_insurer: str = None
    ) -> Tuple[pd.DataFrame, List[Dict], List[Dict], List[str], Dict[str, int]]:
        """Process insurers data with improved memory efficiency and previous ranks."""
        benchmark_insurers = {f"top-{n}-benchmark" for n in top_n_list}
        top_n_rows = {f"top-{n}" for n in top_n_list}
        total_rows = {'total'}
        others_rows = {'others'}

        ranking_metric = (selected_metrics[0] if selected_metrics and
                        selected_metrics[0] in df['metric'].unique()
                        else df['metric'].unique()[0])

        benchmark_metric = 'direct_premiums'

        end_quarter_dt = df['year_quarter'].max()

        group_columns = [col for col in df.columns if col not in ['insurer', 'value']]
        dataframes_to_concat = []
        prev_ranks = {}

        if len(df['insurer'].unique()) > 1:
            insurers_to_exclude = top_n_rows | benchmark_insurers | others_rows | total_rows
            ranking_df = df[
                (~df['insurer'].isin(insurers_to_exclude)) &
                (df['metric'] == ranking_metric)
            ]

            # Get current and previous quarter rankings
            quarters = sorted(ranking_df['year_quarter'].unique())
            if len(quarters) >= 2:
                end_quarter_dt = quarters[-1]
                prev_quarter_dt = quarters[-2]
                logger.debug(f"end_quarter_dt: {end_quarter_dt}")
                logger.debug(f"prev_quarter_dt: {prev_quarter_dt}")
                
                # Current quarter rankings
                end_quarter_ranking_df = ranking_df[ranking_df['year_quarter'] == end_quarter_dt]
                end_quarter_ranking_df = end_quarter_ranking_df.sort_values(
                    ['value'], ascending=[False]
                ).reset_index(drop=True)
                logger.debug(f"end_quarter_ranking_df head: {end_quarter_ranking_df.head()}")
                # Previous quarter rankings
                prev_quarter_ranking_df = ranking_df[ranking_df['year_quarter'] == prev_quarter_dt]
                prev_quarter_ranking_df = prev_quarter_ranking_df.sort_values(
                    ['value'], ascending=[False]
                ).reset_index(drop=True)
                logger.debug(f"prev_quarter_ranking_df metric unique: {prev_quarter_ranking_df['metric'].unique()}")
                logger.debug(f"prev_quarter_ranking_df year_quarter unique: {prev_quarter_ranking_df['year_quarter'].unique()}")
                logger.debug(f"prev_quarter_ranking_df linemain unique: {prev_quarter_ranking_df['linemain'].unique()}")
                
                logger.debug(f"prev_quarter_ranking_df head: {prev_quarter_ranking_df.head()}")
                # Store previous ranks for selected insurers
                prev_ranks = {
                    row['insurer']: idx + 1
                    for idx, row in prev_quarter_ranking_df.iterrows()
                }
                logger.debug(f"prev_ranks: {prev_ranks}")
            
            else:
                end_quarter_ranking_df = ranking_df[ranking_df['year_quarter'] == end_quarter_dt]
                end_quarter_ranking_df = end_quarter_ranking_df.sort_values(
                    ['value'], ascending=[False]
                ).reset_index(drop=True)

            top_insurers_options = list(dict.fromkeys(end_quarter_ranking_df['insurer']))
            top5_insurers_options = (end_quarter_ranking_df.groupby('insurer', observed=True)['value']
                                   .sum()
                                   .nlargest(5)
                                   .index
                                   .tolist())

            if show_data_table:
                selected_insurers = (end_quarter_ranking_df.groupby('insurer', observed=True)['value']
                                   .sum()
                                   .nlargest(number_of_insurers)
                                   .index
                                   .tolist())

            # Process top-n rows
            if (selected_insurers and any(insurer in top_n_rows for insurer in selected_insurers)) or show_data_table:
                for n in top_n_list:
                    ranking_df_top_rows = df[~df['insurer'].isin(insurers_to_exclude)]
                    top_n_rows_df = (ranking_df_top_rows.groupby(group_columns)
                                   .apply(lambda x: x.nlargest(n, 'value'))
                                   .reset_index(drop=True)
                                   .groupby(group_columns, observed=True)['value']
                                   .sum()
                                   .reset_index())
                    top_n_rows_df['insurer'] = f'top-{n}'
                    dataframes_to_concat.append(top_n_rows_df)

            # Process benchmark rows
            if any(insurer in benchmark_insurers for insurer in selected_insurers):
                insurers_to_exclude = (set(selected_insurers[0]) | benchmark_insurers |
                                     top_n_rows | total_rows | others_rows)

                end_quarter_benchmark_df = df[
                    (~df['insurer'].isin(insurers_to_exclude)) &
                    (df['year_quarter'] == end_quarter_dt) &
                    (df['metric'] == benchmark_metric)
                ]

                for n in top_n_list:
                    benchmark_insurers_list = (end_quarter_benchmark_df
                                            .groupby('insurer', observed=True)['value']
                                            .sum()
                                            .nlargest(n)
                                            .index
                                            .tolist())
                    benchmark_insurers_df = (df[df['insurer'].isin(benchmark_insurers_list)]
                                          .groupby(group_columns, observed=True)['value']
                                          .sum()
                                          .reset_index())
                    benchmark_insurers_df['insurer'] = f'top-{n}-benchmark'
                    dataframes_to_concat.append(benchmark_insurers_df)

            # Process others rows
            if others_rows.intersection(set(selected_insurers)):
                insurers_to_exclude = (set(selected_insurers) | benchmark_insurers |
                                     top_n_rows | total_rows | others_rows)

                others_df = (df[~df['insurer'].isin(insurers_to_exclude)]
                           .groupby(group_columns, observed=True, sort=False)['value']
                           .sum()
                           .reset_index())
                others_df['insurer'] = 'others'
                dataframes_to_concat.append(others_df)
        else:
            selected_insurers = [df['insurer'].unique()[0]]

        insurers_to_exclude_original_df = benchmark_insurers | top_n_rows | others_rows
        original_df = df[~df['insurer'].isin(insurers_to_exclude_original_df)]
        dataframes_to_concat.insert(0, original_df)

        concat_df = pd.concat(dataframes_to_concat, ignore_index=True)

        if not show_data_table:
            if not any(metric.endswith(("market_share", "market_share_q_to_q_change"))
                      for metric in selected_metrics):
                insurers_to_keep = selected_insurers
            else:
                insurers_to_keep = (selected_insurers or []) + list(total_rows)
        else:
            insurers_to_keep = (selected_insurers or []) + list(top_n_rows) + list(total_rows)

        result_df = concat_df[concat_df['insurer'].isin(insurers_to_keep)]

        # Generate options
        all_insurers_original = list(dict.fromkeys(df['insurer']))
        insurers_not_in_top_insurers_options = [
            insurer for insurer in all_insurers_original
            if insurer not in top_insurers_options
        ]
        insurers_not_in_top5 = [
            insurer for insurer in top_insurers_options
            if insurer not in top5_insurers_options
        ]

        insurer_options = [
            {'label': map_insurer(insurer), 'value': insurer}
            for insurer in (top5_insurers_options + list(total_rows) +
                          list(top_n_rows) + insurers_not_in_top5 +
                          insurers_not_in_top_insurers_options)
        ]

        compare_options = [
            {'label': map_insurer(insurer), 'value': insurer}
            for insurer in (top5_insurers_options + list(total_rows) +
                          list(others_rows) + list(benchmark_insurers) +
                          list(top_n_rows) + insurers_not_in_top5 +
                          insurers_not_in_top_insurers_options)
            if not main_insurer or insurer not in main_insurer
        ]

        # Return previous ranks as additional outpu
        return result_df, insurer_options, compare_options, selected_insurers, prev_ranks

    def add_market_share_rows(
            self,
            df: pd.DataFrame,
            selected_insurers: List[str],
            selected_metrics: List[str],
            show_data_table: bool,
            total_insurer: str = 'total',
            suffix: str = '_market_share'
        ) -> pd.DataFrame:
            """Calculate market share metrics."""
            if df.empty:
                return df

            # Get grouping columns (all columns except 'insurer' and 'value')
            group_cols = [col for col in df.columns if col not in {'insurer', 'value'}]

            # Calculate totals for each group
            totals = (df[df['insurer'] == total_insurer]
                     .groupby(group_cols)['value']
                     .first()
                     .to_dict())

            if not totals:
                return df

            # Calculate market shares
            market_shares = []
            for group_key, group in df.groupby(group_cols):
                if group_key not in totals or totals[group_key] == 0:
                    continue
                group = group.copy()
                group['value'] = (group['value'] / totals[group_key]).fillna(0)
                group['metric'] = group['metric'] + suffix
                market_shares.append(group)

            if not market_shares:
                return df

            result = pd.concat([df] + market_shares, ignore_index=True)

            if not show_data_table:
                result = result[result['insurer'].isin(selected_insurers)]

            return result

    def add_averages_and_ratios(
        self,
        df: pd.DataFrame,
        required_metrics: List[str]
    ) -> pd.DataFrame:
        """Calculate averages and ratios with optimized performance."""
        logger.debug(f"Unique metrics df before averages_and_ratio: {df['metric'].unique().tolist()}")
        logger.debug(f"required_metrics: {required_metrics}")
            
        if 'ceded_premiums_ratio' in required_metrics:
            logger.debug("Checking presence of required metrics:")
            logger.debug(f"'ceded_premiums' in df: {'ceded_premiums' in df['metric'].unique()}")
            logger.debug(f"'total_premiums' in df: {'total_premiums' in df['metric'].unique()}")
            
            # Sample data for both metrics
            logger.debug("\nSample ceded_premiums data:")
            logger.debug(df[df['metric'] == 'ceded_premiums'].head())
            logger.debug("\nSample total_premiums data:")
            logger.debug(df[df['metric'] == 'total_premiums'].head())        
        
        
        METRIC_GROUPS = {
            'transform': {
                'sums_end': lambda x: x / 10,
                'new_sums': lambda x: x / 10,
                'new_contracts': lambda x: x * 1000,
                'contracts_end': lambda x: x * 1000,
                'claims_settled': lambda x: x * 1000,
                'claims_reported': lambda x: x * 1000
            },
            'averages': {
                'average_sum_insured': (
                    ['sums_end', 'contracts_end'],
                    lambda df: df['sums_end'] / df['contracts_end'] / 1_000
                ),
                'average_new_sum_insured': (
                    ['new_sums', 'new_contracts'],
                    lambda df: df['new_sums'] / df['new_contracts'] / 1_000
                ),
                'average_new_premium': (
                    ['new_sums', 'direct_premiums'],
                    lambda df: df['direct_premiums'] / df['new_sums']
                ),

                'average_new_premium': (
                    ['direct_premiums', 'new_contracts'],
                    lambda df: df['direct_premiums'] / df['new_contracts']
                ),
                'average_loss': (
                    ['direct_losses', 'claims_settled'],
                    lambda df: df['direct_losses'] / df['claims_settled']
                )
            },
            'ratios': {
                'ceded_premiums_ratio': (
                    ['ceded_premiums', 'total_premiums'],
                    lambda df: df['ceded_premiums'].fillna(0) / df['total_premiums']
                ),
                'ceded_losses_ratio': (
                    ['ceded_losses', 'total_losses'],
                    lambda df: df['ceded_losses'].fillna(0) / df['total_losses']
                ),
                'ceded_losses_to_ceded_premiums_ratio': (
                    ['ceded_losses', 'ceded_premiums'],
                    lambda df: df['ceded_losses'].fillna(0) / df['ceded_premiums'].fillna(1)
                ),
                'gross_loss_ratio': (
                    ['direct_losses', 'inward_losses', 'direct_premiums', 'inward_premiums'],
                    lambda df: ((df['direct_losses'].fillna(0) + df['inward_losses'].fillna(0)) /
                              (df['direct_premiums'].fillna(0) + df['inward_premiums'].fillna(0)))
                ),
                'direct_loss_ratio': (
                    ['direct_losses', 'direct_premiums'],
                    lambda df: df['direct_losses'].fillna(0) / df['direct_premiums'].fillna(1)
                ),
                'inward_loss_ratio': (
                    ['inward_losses', 'inward_premiums'],
                    lambda df: df['inward_losses'].fillna(0) / df['inward_premiums'].fillna(1)
                ),

                'premiums_interm_ratio': (
                    ['direct_premiums', 'premiums_interm'],
                    lambda df: df['premiums_interm'].fillna(0) / df['direct_premiums'].fillna(1)
                ),

                'commissions_rate': (
                    ['premiums_interm', 'commissions_interm'],
                    lambda df: df['commissions_interm'].fillna(0) / df['premiums_interm'].fillna(1)
                ),

                'net_loss_ratio': (
                    ['direct_losses', 'inward_losses', 'ceded_losses',
                     'direct_premiums', 'inward_premiums', 'ceded_premiums'],
                    lambda df: ((df['direct_losses'].fillna(0) + df['inward_losses'].fillna(0) -
                               df['ceded_losses'].fillna(0)) /
                              (df['direct_premiums'].fillna(0) + df['inward_premiums'].fillna(0) -
                               df['ceded_premiums'].fillna(0)))
                ),
                'effect_on_loss_ratio': (
                    ['direct_losses', 'inward_losses', 'ceded_losses',
                     'direct_premiums', 'inward_premiums', 'ceded_premiums'],
                    lambda df: ((df['direct_losses'].fillna(0) + df['inward_losses'].fillna(0)) /
                              (df['direct_premiums'].fillna(0) + df['inward_premiums'].fillna(0))) -
                             ((df['direct_losses'].fillna(0) + df['inward_losses'].fillna(0) -
                               df['ceded_losses'].fillna(0)) /
                              (df['direct_premiums'].fillna(0) + df['inward_premiums'].fillna(0) -
                               df['ceded_premiums'].fillna(0)))
                ),
                'ceded_ratio_diff': (
                    ['ceded_losses', 'total_losses', 'ceded_premiums', 'total_premiums'],
                    lambda df: (df['ceded_losses'].fillna(0) / df['total_losses']) -
                             (df['ceded_premiums'].fillna(0) / df['total_premiums'])
                )
            }
        }

        def transform_metrics(data: pd.DataFrame) -> pd.DataFrame:
            """Transform metrics using vectorized operations."""
            for metric, transform_func in METRIC_GROUPS['transform'].items():
                if metric in required_metrics:
                    mask = data['metric'] == metric
                    data.loc[mask, 'value'] = transform_func(data.loc[mask, 'value'])
            return data

        def calculate_ratios(pivot_df: pd.DataFrame) -> Dict[str, pd.Series]:
            """Calculate required ratios efficiently."""
            results = {}
            for group in ['averages', 'ratios']:
                for metric, (deps, formula) in METRIC_GROUPS[group].items():
                    if metric in required_metrics:
                        try:
                            if all(dep in pivot_df.columns for dep in deps):
                                results[metric] = formula(pivot_df)
                        except Exception as e:
                            self.logger.error(f"Error calculating {metric}: {str(e)}")
            return results

        try:
            result_df = transform_metrics(df.copy())

            calc_metrics = [m for m in required_metrics if any(
                m in group for group in [METRIC_GROUPS['averages'], METRIC_GROUPS['ratios']]
            )]

            if calc_metrics:
                index_cols = [col for col in df.columns if col not in ['metric', 'value']]
                pivot_df = result_df.pivot_table(
                    values='value',
                    index=index_cols,
                    columns='metric',
                    aggfunc='first'
                ).reset_index()

                if 'ceded_premiums_ratio' in required_metrics:
                    logger.debug("Checking ceded_premiums_ratio components:")
                    logger.debug(f"ceded_premiums values: {pivot_df['ceded_premiums'].head()}")
                    logger.debug(f"total_premiums values: {pivot_df['total_premiums'].head()}")
                    

                
                new_metrics = calculate_ratios(pivot_df)

                # After calculation
                if 'ceded_premiums_ratio' in new_metrics:
                    logger.debug("Resulting ceded_premiums_ratio:")
                    logger.debug(new_metrics['ceded_premiums_ratio'].head())
                
                for metric, values in new_metrics.items():
                    new_rows = pd.DataFrame({
                        **{col: pivot_df[col] for col in index_cols},
                        'metric': metric,
                        'value': values
                    })
                    result_df = pd.concat([result_df, new_rows], ignore_index=True)
            logger.debug(f"Unique metrics df after averages_and_ratio: {result_df['metric'].unique().tolist()}")
            return result_df

        except Exception as e:
            self.logger.error(f"Fatal error in add_averages_and_ratios: {str(e)}")
            raise

    def add_growth_rows_long(
        self,
        df: pd.DataFrame,
        selected_insurers: List[str],
        show_data_table: bool,
        num_periods: int = 2,
        period_type: str = 'qoq'
    ) -> pd.DataFrame:
        """Calculate growth metrics with improved performance."""
        try:
            if df.empty:
                return df

            # Ensure datetime type
            if not pd.api.types.is_datetime64_any_dtype(df['year_quarter']):
                df['year_quarter'] = pd.to_datetime(df['year_quarter'], errors='coerce')

            group_cols = [col for col in df.columns if col not in ['year_quarter', 'metric', 'value']]
            df_sorted = df.sort_values(by=group_cols + ['year_quarter']).copy()

            # Split processing by metric type
            market_share_mask = df_sorted['metric'].str.endswith('market_share')
            regular_metrics = df_sorted[~market_share_mask].copy()
            market_share_metrics = df_sorted[market_share_mask].copy()

            processed_dfs = []

            # Process regular metrics
            if len(regular_metrics) > 0:
                grouped = regular_metrics.groupby(group_cols + ['metric'], observed=True)
                regular_metrics['previous'] = grouped['value'].shift(1)

                mask = regular_metrics['previous'] > 1e-9
                regular_metrics['growth'] = np.where(
                    mask,
                    (regular_metrics['value'] - regular_metrics['previous']) / regular_metrics['previous'],
                    np.nan
                )

                growth_regular = regular_metrics.copy()
                growth_regular['metric'] += '_q_to_q_change'
                growth_regular['value'] = growth_regular['growth']
                processed_dfs.append(growth_regular.drop(columns=['growth', 'previous']))

            # Process market share metrics
            if len(market_share_metrics) > 0:
                grouped = market_share_metrics.groupby(group_cols + ['metric'], observed=True)
                market_share_metrics['growth'] = grouped['value'].diff().fillna(0)

                growth_market = market_share_metrics.copy()
                growth_market['metric'] += '_q_to_q_change'
                growth_market['value'] = growth_market['growth']
                processed_dfs.append(growth_market.drop(columns=['growth']))

            growth_df = pd.concat(processed_dfs, ignore_index=True) if processed_dfs else pd.DataFrame(columns=df.columns)

            # Filter periods if needed
            if show_data_table:
                num_periods_growth = num_periods - 1

                recent_periods = (df_sorted['year_quarter']
                                .drop_duplicates()
                                .sort_values(ascending=False)
                                .iloc[:num_periods])

                recent_growth_periods = (df_sorted['year_quarter']
                                       .drop_duplicates()
                                       .sort_values(ascending=False)
                                       .iloc[:max(num_periods_growth, 1)])

                df_filtered = df_sorted[df_sorted['year_quarter'].isin(recent_periods)].copy()
                growth_filtered = growth_df[growth_df['year_quarter'].isin(recent_growth_periods)].copy()

                result = pd.concat([df_filtered, growth_filtered], ignore_index=True)
            else:
                result = pd.concat([df_sorted, growth_df], ignore_index=True)

            result.sort_values(by=group_cols + ['year_quarter', 'metric'], inplace=True)
            result.reset_index(drop=True, inplace=True)

            return result

        except Exception as e:
            self.logger.error(f"Error in growth calculation: {str(e)}")
            raise

    #@staticmethod
    def add_calculated_metrics(self, df: pd.DataFrame, required_metrics: List[str]):
        """Add calculated metrics to the DataFrame in-place."""
        grouping_cols = [col for col in df.columns if col not in ['metric', 'value']]
        metrics_to_calculate = {
            'net_balance': lambda d: d.get('ceded_losses', 0) - d.get('ceded_premiums', 0),

            'total_premiums': lambda d: d.get('direct_premiums', 0) + d.get('inward_premiums', 0),
            'net_premiums': lambda d: (d.get('direct_premiums', 0) + d.get('inward_premiums', 0) -
                                     d.get('ceded_premiums', 0)),
            'total_losses': lambda d: d.get('direct_losses', 0) + d.get('inward_losses', 0),
            'net_losses': lambda d: (d.get('direct_losses', 0) + d.get('inward_losses', 0) -
                                   d.get('ceded_losses', 0)),
            'gross_result': lambda d: ((d.get('direct_premiums', 0) + d.get('inward_premiums', 0)) -
                                     (d.get('direct_losses', 0) + d.get('inward_losses', 0))),
            'net_result': lambda d: ((d.get('direct_premiums', 0) + d.get('inward_premiums', 0) -
                                    d.get('ceded_premiums', 0)) -
                                   (d.get('direct_losses', 0) + d.get('inward_losses', 0) -
                                    d.get('ceded_losses', 0)))
        }

        # Filter for only required calculations
        active_calculations = {
            metric: calc for metric, calc in metrics_to_calculate.items()
            if metric in required_metrics
        }

        if not active_calculations:
            return df
        logger.debug(f"active_calculations {active_calculations}")

        def calculate_for_group(group):
            try:
                first_row = next(group.itertuples())
                base_dict = {col: getattr(first_row, col) for col in grouping_cols}
                metrics_dict = dict(zip(group['metric'], group['value']))

                result_data = []
                result_data.extend(group.to_dict('records'))

                for metric, calculation in active_calculations.items():
                    result_data.append({
                        'metric': metric,
                        'value': calculation(metrics_dict),
                        **base_dict
                    })

                return pd.DataFrame(result_data)

            except Exception as e:
                logger.error(f"Error in calculate_for_group: {str(e)}")
                return group

        # Process in chunks for memory efficiency
        chunks = []
        for _, group in df.groupby(grouping_cols):
            chunks.append(calculate_for_group(group))

        result_df = pd.concat(chunks, ignore_index=True)

        # Update the input DataFrame in-place
        df.drop(df.index, inplace=True)

        df = pd.concat([df, result_df], ignore_index=True)

        return df

================
File: data_process/table_data.py
================
# data_process.table_data.py
import pandas as pd
import numpy as np
from dash import dash_table
from typing import List, Tuple, Optional, Dict, OrderedDict
from app.components.dash_table import generate_dash_table_config
from data_process.data_utils import map_line
from constants.translations import translate
from config.logging_config import get_logger

logger = get_logger(__name__)


def get_data_table(
    df: pd.DataFrame,
    table_selected_metric: List[str],
    selected_linemains: List[str],
    period_type: str,
    number_of_insurers: int,
    toggle_selected_market_share: Optional[List[str]],
    toggle_selected_qtoq: Optional[List[str]],
    prev_ranks: Optional[Dict[str, int]] = None
) -> Tuple[dash_table.DataTable, str, str]:

    logger.debug(f"Updating data table. table_selected_metric: {table_selected_metric}")

    table_data = table_data_pivot(df, table_selected_metric, prev_ranks)

    table_config = generate_dash_table_config(
        df=table_data,
        period_type=period_type,
        toggle_selected_market_share=toggle_selected_market_share,
        toggle_selected_qtoq=toggle_selected_qtoq
    )

    data_table = dash_table.DataTable(**table_config)

    mapped_lines = map_line(selected_linemains)

    lines_str = ', '.join(mapped_lines) if isinstance(mapped_lines, list) else mapped_lines
    table_title = f"Топ-{number_of_insurers} страховщиков"
    table_subtitle = f"{translate(table_selected_metric[0])}: {lines_str}"

    return data_table, table_title, table_subtitle


def table_data_pivot(
    df: pd.DataFrame,
    table_selected_metric: List[str], 
    prev_ranks: Optional[Dict[str, int]] = None
) -> pd.DataFrame:

    logger.debug("Starting table data pivot")

    try:
        # 1. Prepare metrics and initial filtering
        def _prepare_metrics(base_metrics: List[str]) -> List[str]:
            """Generate complete list of metrics including derived ones."""
            derived_suffixes = ['q_to_q_change', 'market_share', 'market_share_q_to_q_change']
            return (base_metrics +
                    [f"{m}_{suffix}" for m in base_metrics for suffix in derived_suffixes])

        metrics_to_keep = _prepare_metrics(table_selected_metric)
        logger.debug(f"metrics_to_keep : {metrics_to_keep}")
        filtered_df = df[df['metric'].isin(metrics_to_keep)].copy()

        # 2. Format time periods and extract metric components
        def _format_time_periods(df: pd.DataFrame) -> pd.DataFrame:
            """Convert year_quarter to standard YYYYQX format."""
            df['year_quarter'] = pd.to_datetime(df['year_quarter']).dt.to_period('Q').astype(str)
            return df

        def _extract_metric_components(df: pd.DataFrame, base_metrics: List[str]) -> pd.DataFrame:
            """Split metrics into base metric and attribute components."""
            def extract_metric_attribute(metric, base_metrics):
                # Sort base_metrics by length, longest first
                sorted_bases = sorted(base_metrics, key=len, reverse=True)
                for base in sorted_bases:
                    if metric.startswith(base):
                        suffix = metric[len(base):].lstrip('_')
                        return base, suffix or ''
                return metric, ''

            df[['base_metric', 'attribute']] = df.apply(
                lambda row: pd.Series(extract_metric_attribute(row['metric'], base_metrics)),
                axis=1
            )
            return df

        processed_df = _format_time_periods(filtered_df)
        processed_df = _extract_metric_components(processed_df, table_selected_metric)
        logger.debug(f"metrics uniqye processed_df after _extract_metric_components: {processed_df['metric'].unique() }")

        # 3. Create and format pivot table
        def _create_pivot_columns(df: pd.DataFrame) -> pd.DataFrame:
            """Create unified column names for pivot operation."""
            df['column_name'] = df.apply(
                lambda row: f"{row['base_metric']}_{row['year_quarter']}"
                           f"{'_' + row['attribute'] if row['attribute'] else ''}",
                axis=1
            )
            return df

        def _pivot_data(df: pd.DataFrame) -> pd.DataFrame:
            """Create pivot table with insurers as index."""
            return df.pivot_table(
                index='insurer',
                columns='column_name',
                values='value',
                aggfunc='first'
            ).reset_index()

        processed_df = _create_pivot_columns(processed_df)
        logger.debug(f"processed_df first row:\n{processed_df.head(1).to_string()}")
        logger.debug(f"metrics uniqye processed_df: {processed_df['column_name'].unique() }")
        pivot_df = _pivot_data(processed_df)
        logger.debug(f"pivot_df: {pivot_df.head() }")
        logger.debug(f"pivot_df first row:\n{pivot_df.head(1).to_string()}")

        # 4. Organize columns and handle missing values
        def _organize_columns(df: pd.DataFrame, base_metrics: List[str],
                             time_periods: List[str]) -> pd.DataFrame:
            """Organize columns in desired order and format."""
            attributes = ['', 'q_to_q_change', 'market_share', 'market_share_q_to_q_change']
            desired_columns = ['insurer'] + [
                f"{metric}_{year}{'_' + attr if attr else ''}"
                for metric in base_metrics
                for attr in attributes
                for year in sorted(time_periods, reverse=True)
            ]
            # Remove duplicates while preserving order
            desired_columns = list(OrderedDict.fromkeys(desired_columns))

            # Add missing columns and reorder
            for col in desired_columns:
                if col not in df.columns:
                    df[col] = pd.NA
            return df[desired_columns]

        organized_df = _organize_columns(
            pivot_df,
            table_selected_metric,
            processed_df['year_quarter'].unique()
        )

        logger.debug(f"organized_df first row:\n{organized_df.head(1).to_string()}")

        # 5. Clean and sort the final table
        def _clean_and_sort_table(df: pd.DataFrame) -> pd.DataFrame:
            # Remove empty columns except insurer
            value_cols = df.columns[df.columns != 'insurer']
            mask = ~((df[value_cols] == 0) | df[value_cols].isna()).all()
            keep_cols = ['insurer'] + list(value_cols[mask])
            df = df[keep_cols]

            # Separate regular and summary rows
            summary_mask = df['insurer'].str.lower().str.startswith(('top', 'total'))
            main_df = df[~summary_mask].copy()
            summary_df = df[summary_mask].copy()

            # Sort by first metric column
            sort_col = value_cols[0]
            main_df[sort_col] = pd.to_numeric(main_df[sort_col], errors='coerce')
            main_df = main_df.sort_values(by=sort_col, ascending=False)

            # Add numbering with previous rank
            main_df.insert(0, 'Место', range(1, len(main_df) + 1))
            if prev_ranks:
                main_df['Место'] = main_df.apply(
                    lambda row: f"{int(row['Место'])} ({prev_ranks.get(row['insurer'], 'n/a')})"
                    if row['insurer'] in prev_ranks else str(int(row['Место'])),
                    axis=1
                )
            else:
                main_df['Место'] = main_df['Место'].astype(str)

            # Combine and finalize
            summary_df.insert(0, 'Место', np.nan)
            final_df = pd.concat([main_df, summary_df], ignore_index=True)

            return final_df.fillna('n/a')

        final_df = _clean_and_sort_table(organized_df)
        logger.debug(f"final_df first row:\n{final_df.head(1).to_string()}")

        logger.debug(f"Completed table pivot. Output shape: {final_df.shape}")
        return final_df

    except Exception as e:
        logger.error(f"Error in table_data_pivot: {e}", exc_info=True)
        raise

    final_df = _clean_and_sort_table(organized_df)

    return final_df

================
File: app.py
================
import logging
import pandas as pd
import dash_bootstrap_components as dbc
import dash
from dash import Input, Output, State
from typing import Dict, List, Tuple, Any
from dash.exceptions import PreventUpdate
from pathlib import Path
from app.components.insurance_lines_tree import insurance_lines_tree
from app.callbacks.app_layout_callbacks import setup_tab_state_callbacks, setup_sidebar_callbacks
from app.callbacks.insurance_lines_callbacks import setup_insurance_lines_callbacks
from app.callbacks.filter_update_callbacks import setup_filter_update_callbacks
from config.main_config import APP_TITLE, DEBUG_MODE, PORT, DATA_FILE_162, DATA_FILE_158
from config.logging_config import (
        monitor_memory, memory_monitor,
        setup_logging, DebugLevels, get_logger,
        track_callback, track_callback_end
)
from data_process.data_utils import create_year_quarter_options, load_and_preprocess_data
from data_process.process_filters import MetricsProcessor
from data_process.table_data import get_data_table
from app.app_layout import create_app_layout
from app.callbacks.period_filter import setup_period_type_callbacks
logger = get_logger(__name__)
from config.default_values import DEFAULT_REPORTING_FORM

def create_dash_app() -> dash.Dash:
    """Create and configure the Dash application."""
    app = dash.Dash(
        __name__,
        assets_folder=Path(__file__).parent / "assets",
        external_stylesheets=[dbc.themes.BOOTSTRAP],
        suppress_callback_exceptions=True
    )
    app.title = APP_TITLE
    app.index_string = '''
    <!DOCTYPE html>
    <html>
        <head>
            {%metas%}
            <title>{%title%}</title>
            {%favicon%}
            {%css%}
        </head>
        <body>
            {%app_entry%}
            <footer>
                {%config%}
                {%scripts%}
                {%renderer%}
            </footer>
        </body>
    </html>
    '''
    return app


def main() -> dash.Dash:
    """Initialize and configure the main application."""
    print("Starting main()")

    # Initialize logging and debug level
    setup_logging(console_level=logging.INFO, file_level=logging.DEBUG)
    logger.info("Starting the Insurance Data Analysis Dashboard")

    try:
        insurance_df_162 = load_and_preprocess_data(DATA_FILE_162)
        insurance_df_158 = load_and_preprocess_data(DATA_FILE_158)
    except Exception as e:
        logger.error(f"Failed to load data: {str(e)}")
        raise

    # Create application and initialize filters
    app = create_dash_app()
    quarter_options_162 = create_year_quarter_options(insurance_df_162)
    quarter_options_158 = create_year_quarter_options(insurance_df_158)
    initial_quarter_options = quarter_options_162 if DEFAULT_REPORTING_FORM == '0420162' else quarter_options_158

    layout = create_app_layout(initial_quarter_options)
    app.layout = layout

    # Setup basic callbacks
    setup_period_type_callbacks(app)
    setup_tab_state_callbacks(app)
    setup_insurance_lines_callbacks(app, insurance_lines_tree)
    setup_filter_update_callbacks(app, quarter_options_162, quarter_options_158)
    setup_sidebar_callbacks(app)

    logger.debug("Dashboard layout created")

    @app.callback(
        [Output('processed-data-store', 'data'),
         Output('number-of-periods-data-table', 'value'),
         Output('number-of-insurers', 'value')],
        [Input('filter-state-store', 'data'),
         Input('period-type', 'data'),
         Input('end-quarter', 'value'),
         Input('number-of-periods-data-table', 'value'),
         Input('number-of-insurers', 'value')],
        [State('show-data-table', 'data')],
        prevent_initial_call=True
    )
    @monitor_memory
    def process_data(
        filter_state: Dict[str, Any],
        period_type: str,
        end_quarter: str,
        num_periods_table: int,
        number_of_insurers: int,
        show_data_table: bool
    ) -> Tuple:
        """Process data based on filter state."""
        ctx = dash.callback_context
        start_time = track_callback('app.main', 'process_data', dash.callback_context)
        if not filter_state:
            track_callback_end('app.main', 'process_data', start_time, message_no_update="not filter_state")
            raise PreventUpdate
        memory_monitor.log_memory("before_process_data", logger)
        try:
            processor = MetricsProcessor()
            logger.debug(f"Process data - processing with parameters: {filter_state}, period_type: {period_type}, end_quarter: {end_quarter}, num_periods_table: {num_periods_table}, number_of_insurers: {number_of_insurers} ")
            logger.debug(f"reporting_form: {filter_state['reporting_form']}")

            df, insurer_options, compare_options, selected_insurers, prev_ranks, number_of_periods_options, number_of_insurer_options = (
                processor.process_general_filters(
                    df=insurance_df_162 if filter_state['reporting_form'] == '0420162' else insurance_df_158,
                    show_data_table=show_data_table,
                    premium_loss_selection=filter_state['premium_loss_checklist'],
                    selected_metrics=filter_state['selected_metrics'] or ['direct_premiums'],
                    selected_linemains=filter_state['selected_lines'],
                    period_type=period_type,
                    start_quarter='2018Q1' if filter_state['reporting_form'] == '0420162' else '2022Q1',
                    end_quarter=end_quarter,
                    num_periods=num_periods_table,
                    chart_columns=[],
                    selected_insurers=None,
                    number_of_insurers=number_of_insurers,
                    top_n_list=[5, 10, 20]
                )
            )

            df['year_quarter'] = df['year_quarter'].dt.strftime('%Y-%m-%d')
            output = ({'df': df.to_dict('records'), 'prev_ranks': prev_ranks}, min(num_periods_table, number_of_periods_options), min(number_of_insurers, number_of_insurer_options))
            logger.debug(f"metrics unique: {df['metric'].unique() }")

            return output

        except Exception as e:
            logger.error(f"Error in process_data: {str(e)}", exc_info=True)
            track_callback_end('app.main', 'process_data', start_time, error=e)
            raise

        finally:
            track_callback_end('app.main', 'process_data', start_time, result=output)

    @app.callback(
        [Output('data-table', 'children'),
         Output('table-title', 'children'),
         Output('table-subtitle', 'children')],
        [Input('processed-data-store', 'data'),
         Input('toggle-selected-market-share', 'value'),
         Input('toggle-selected-qtoq', 'value')],
        [State('filter-state-store', 'data'),
         State('period-type', 'data'),
         State('end-quarter', 'value'),
         State('number-of-insurers', 'value')],
        prevent_initial_call=True
    )
    @monitor_memory
    def process_ui(
        processed_data: Dict,
        toggle_selected_market_share: bool,
        toggle_selected_qtoq: bool,
        filter_state: Dict,
        period_type: str,
        end_quarter: str,
        number_of_insurers: int
    ) -> List:
        """Update table based on processed data."""
        memory_monitor.log_memory("before_process_ui", logger)
        start_time = track_callback('app.main', 'process_ui', dash.callback_context)

        try:
            df = pd.DataFrame.from_records(processed_data['df'])
            df['year_quarter'] = pd.to_datetime(df['year_quarter'])

            table_data = get_data_table(
                df=df,
                table_selected_metric=filter_state['selected_metrics'],
                selected_linemains=filter_state['selected_lines'],
                period_type=period_type,
                number_of_insurers=number_of_insurers,
                toggle_selected_market_share=toggle_selected_market_share,
                toggle_selected_qtoq=toggle_selected_qtoq,
                prev_ranks=processed_data['prev_ranks']
            )
            logger.debug(f"table_data {table_data}")
            logger.debug("Returning table data")
            return table_data[0], table_data[1], table_data[2]

        except Exception as e:
            logger.error(f"Error in process_ui: {str(e)}", exc_info=True)
            raise

        finally:
            track_callback_end('app.main', 'process_ui', start_time)

    return app


if __name__ == '__main__':
    try:
        print("Starting application initialization...")
        app = main()
        print("Starting server...")
        app.run_server(debug=DEBUG_MODE, port=PORT)
    except Exception as e:
        print(f"Error during startup: {e}")
        raise
